# Sampled reviews (5 papers per year)
- Source: `_conference_collect/ICLR_review_data_with_keywords.parquet`
- Seed: `42`
- Papers per year: `5`

---

## Year 2016

### Paper 1 (paper_id: SkXIrV9le)

**Review 1:**

This paper presents an approach to modeling videos based on a decomposition into a background + 2d sprites with a latent hidden state. The exposition is OK, and I think the approach is sensible, but the main issue with this paper is that it is lacking experiments on non-synthetic datasets. As such, while I find the graphics inspired questions the paper is investigating interesting, I don't think it is clear that this work introduces useful machinery for modeling more general videos.

I think this paper is more appropriate as a workshop contribution in its current form.

---

**Review 2:**

This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.  The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.  This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.

Some results are presented on simple synthetic data (such as a moving rectangle on a black background or the “Moving MNIST” data.  However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.  On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.

The model itself is also not particularly novel and the work currently misses some relevant citations.  The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).  Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.

Finally, the exposition in this paper is short on many details and I don’t believe that the paper is reproducible from the text alone.  For example, it is not clear what the form of the recognition model is…  Low-level details (which are very important) are also not presented, such as initialization strategy.

---

**Review 3:**

This paper proposes a generative model of videos composed of a background and a set of 2D objects (sprites). Optimization is performed under a VAE framework.

The authors' proposal of an outer product of softmaxed vectors (resulting in a 2D map that is delta-like), composed with a convolution, is a very interesting way to achieve translation of an image with differentiable parameters. It seems to be an attractive alternative to more complicated differentiable resamplers (such as those used by STNs) when only translation is needed.

Below I have made some comments regarding parts of the text, especially the experiments, that are not clear. The experimental section in particular seems rushed, with some results only alluded to but not given, not even in the appendix.

For an extremely novel and exotic proposal, showing only synthetic experiments could be excused. However, though there is some novelty in the method, it is disappointing that there isn't even an attempt at trying to tackle a problem with real data.

I suggest as an example aerial videos (such as those taken from drone platforms), since the planar assumption that the authors make would most probably hold in that case.

I also suggest that the authors do another pass at proof-reading the paper. There are missing references ("Fig. ??"), unfinished sentences (caption of Fig. 5), and the aforementioned issues with the experimental exposition.

---


### Paper 2 (paper_id: BJm4T4Kgx)

**Review 1:**

This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.
The paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.

Pros:
For the first time in the literature, this paper proposed the concept of ‘label leaking’. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.
Using the ratio of 'clean accuracy' over ‘adversarial accuracy’ as the measure of robust is more reasonable compared to the existing works in the literature.

Cons:
Although the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main contribution of the paper. Note that Section 4.3 and Section 4.4 are not specific to large-scale dataset, thus emphasizing the ‘large-scale’ in the title and in the introduction seems improper.
Basically all the conclusions of the paper are made based on observing the experimental results. Further tests should have been performed to verify these hypotheses. Without that, the conclusions of the paper seems rushy. For example, one dataset of imageNet can not infer the conclusions for all large-scale datasets.

---

**Review 2:**

This paper is a well written paper. This paper can be divided into 2 parts:
1.Adversary training on ImageNet
2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity

For part [1], I don’t think training without clean example will not make reasonable ImageNet level model. Ian’s experiment in “Explaining and Harnessing Adversarial Examples” didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian’s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.

For part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation "FGSM examples are most transferable".

In this part the authors raise many interesting problems or guess, but lack theoretical explanations.

Overall I think these empirical observations are useful for future work.

---

**Review 3:**

This paper has two main contributions:
(1) Applying adversarial training to imagenet, a larger dataset than previously considered
(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution.

This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.

---


### Paper 3 (paper_id: B1KBHtcel)

**Review 1:**

This paper addresses the problem of argument mining, which consists of finding argument types and predicting the relationships between the arguments. The authors proposed a pointer network structure to recover the argument relations. They also propose modifications on pointer network to perform joint training on both type and link prediction tasks. Overall the model is reasonable, but I am not sure if ICLR is the best venue for this work.

My first concern of the paper is on the novelty of the model. Pointer network has been proposed before. The proposed multi-task learning method is interesting, but the authors only verified it on one task. This makes me feel that maybe the submission is more for a NLP conference rather than ICLR.

The authors stated that the pointer network is less restrictive compared to some of the existing tree predicting method. However, the datasets seem to only contain single trees or forests, and the stack-based method can be used for forest prediction by adding a virtual root node to each example (as done in the dependency parsing tasks). Therefore, I think the experiments right now cannot reflect the advantages of pointer network models unfortunately.

My second concern of the paper is on the target task. Given that the authors want to analyze the structures between sentences, is the argumentation mining the best dataset? For example, authors could verify their model by applying it to the other tasks that require tree structures such as dependency parsing. As for NLP applications, I found that the assumption that the boundaries of AC are given is a very strong constraint, and could potentially limit the usefulness of the proposed model.

Overall, in terms of ML, I also feel that baseline methods the authors compared to are probably strong for the argument mining task, but not necessary strong enough for the general tree/forest prediction tasks (as there are other tree/forest prediction methods). In terms of NLP applications, I think the assumption of having AC boundaries is too restrictive, and maybe ICLR is not the best venture for this submission.

---

**Review 2:**

This paper proposes a model for the task of argumentation mining (labeling the set of relationships between statements expressed as sentence-sized spans in a short text). The model combines a pointer network component that identifies links between statements and a classifier that predicts the roles of these statements. The resulting model works well: It outperforms strong baselines, even on datasets with fewer than 100 training examples.

I don't see any major technical issues with this paper, and the results are strong. I am concerned, though, that the paper doesn't make a substantial novel contribution to representation learning. It focuses on ways to adapt reasonably mature techniques to a novel NLP problem. I think that one of the ACL conferences would be a better fit for this work.

The choice of a pointer network for this problem seems reasonable, though (as noted by other commenters) the paper does not make any substantial comparison with other possible ways of producing trees. The paper does a solid job at breaking down the results quantitatively, but I would appreciate some examples of model output and some qualitative error analysis.

Detail notes:

- Figure 2 appears to have an error. You report that the decoder produces a distribution over input indices only, but you show an example of the network pointing to an output index in one case.
- I don't think "Wei12" is a name.

---

**Review 3:**

This paper addresses automated argumentation mining using pointer network. Although the task and the discussion is interesting, the contribution and the novelty is marginal because this is a single-task application of PN among many potential tasks.

---


### Paper 4 (paper_id: r1PRvK9el)

**Review 1:**

[Summary]
This paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned during training; 2) modeling a multi-step search process that can decide when to terminate.

The experimental results on WN18 and FB15k seem pretty good. The authors also perform an analysis on a shortest path synthetic task, and demonstrate that this model is better than standard seq2seq.

The paper is well-written and it is easy to follow.

[Major comments]
I actually do like the idea and am also impressed that this model can work well.
The main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper-parameters, besides that only reporting a final model on WN18 and FB15k.

One key hyper-parameter I believe is the size of shared memory (using 64 for the experiments). I don’t think that this number should be fixed for all tasks, at least it should depend on the KB scale. Could you verify this in your experiments? Would it be even possible to make a memory structure with dynamic size?

The RL setting (stochastic search process) is also one highlight of the paper, but could you demonstrate that how much it does really help? I think it is necessary to compare to the following: remove the termination gate and fix the number of inference steps and see how well the model does? Also show how the performance varies on # of steps?

I appreciate your attempts on the shortest path synthetic task. However, I think it would be much better if you can demonstrate that under a real KB setting. You can still perform the shortest path analysis, but using KB  (e.g., Freebase) entities and relations.

[Minor comments]
I am afraid that the output gate illustrated in Figure 1 is a bit confusing. There should be only one output, depending on when the search process is terminated.

---

**Review 2:**

This paper proposes a method for link prediction on Knowledge Bases. The method contains 2 main innovations: (1) an iterative inference process that allows the model to refine its predictions and (2) a shared memory component. Thanks to these 2 elements, the model introduced in the paper achieved remarkable results on two benchmarks.


The paper is fairly written. The model is interesting and the experimental results are strikingly good. Still, I only rate for a weak accept for the following reasons.

* The main problem with this paper is that there is little explanation of how and why the two new elements aforementioned are leading to such better results. For instance:
  - What are the performance without the shared memory? And when its size is grown?
  - How does the performance is impacted when one varies Tmax from 1 to 5 (which the chosen value for the experiments I assume)? This gives an indications of how often the termination gate works.
  - It would also be interesting to give the proportion of examples for which the inference is terminated before hitting Tmax.
  - What is the proportion of examples for which the prediction changed along several inference iterations?

* A value of \lambda set to 10 (Section 2) seems to indicate a low temperature for the softmax. Is the attention finally attending mostly at a single cell? How do the softmax activations change with the type of relationships? the entity type?

* FB15k and WN18 are quite old overused benchmarks now. It would be interesting to test on larger conditions.

---

**Review 3:**

In this paper, the authors proposed an implicit ResoNet model for knowledge base completion. The proposed model performs inference implicitly by a search controller and shared memory. The proposed approach demonstrates promising results on FB15k benchmark dataset.

Pros:

- The proposed approach demonstrates strong performance on FB15k dataset.

- The idea of using shared memory for knowledge base completion is new and interesting.

- The proposed approach is general and can be applied in various tasks.

Cons:

- There is no qualitative analysis on the results, and it is hard to see why the proposed approach works on the knowledge-base completion task.

- The introduction section can be improved. Specifically, the authors should motivate "shared memory" more in the introduction and how it different from existing methods that using "unshared memory" for knowledge base completion. Similarly, the function of search controller is unclear in the introduction section as it is unclear what does search mean in the content of knowledge base completion.  The concept of shared memory and search controller only make sense to me after reading through section 2.

---


### Paper 5 (paper_id: H1_QSDqxl)

**Review 1:**

The paper presents a nice idea of directly finding rules such as brother(father) => uncle in knowledge bases, by directly searching in embedding space. The idea is to interpret the successive application of relationships as the multiplication of the relation-dependent matrices in non-negative RESCAL.

The experimental section provides an evaluation of the rules that are found by the algorithm. Nonetheless, the work seems only at its first stages for now, and many questions are left open:

1) while the approach to find rules seems very general, the reason why it should work is unclear. What properties of the embedding space or of the initial algorithm are required for this approach to find meaningful rules? Can we apply the same principles to other algorithms than non-negative RESCAL?

2) there is no real evaluation in terms of link prediction. How can we use these rules in conjunction with the original algorithm to improve link prediction? What performance gains can be expected? Can these rules find links that would not be found be the original algorithm in the first place?

3) scaling: for now the number of parameters of the rule miner is (#relationships)^(max. path length + 1). How does this method scale on standard benchmarks such as FB15k where there is more than a 1000 of relationships?

---

**Review 2:**

This paper proposes a process to mine rules from vector space representations learned from KBs (using nonnegative RESCAL).

The paper is nicely written.
But its motivations are unclear: what is the underlying motivation to mine rules from embedding spaces?
- If it is for better performance on link prediction then the paper does not show this. The experiments do not compare FRM against the performance of the original vector space model.
- If it is for a better interpretability and debugging of the representations learned by vector space models, then there should have more elements on this in the paper.

Other remarks:
- The fact that the performance of the methods in Figure 1 and 2 are not compared to any baseline is problematic.
- The scalability of the rule miner is a big drawback that should be addressed.
- Figure 3 does not do a good job at convincing that rule based systems should be used for prediction or interpretation. The learned rules are bad for both cases.

---

**Review 3:**

This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem. Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets.

The solution part sounds plausible. However, it confuses me that why we need to mine rules from learned KB embeddings.

- It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them.

- If the goal is to extract useful rules (for other applications), it is necessary to compare it to “graph random walk” (http://rtw.ml.cmu.edu/papers/lao-emnlp11.pdf) which could learn rules from KB graph directly.

- As there is only one KNN baseline, the experimental results seem pretty weak. At the least, it is necessary to show the original precision / recall of RESCAL, together with the proposed rule mining approach (with different max length), so we know how much the current information the current rule miner could recover.

In addition, the four datasets are all very small. Would it be able to scale it to WordNet or Freebase?

[Minor comments]

“Relational embedding” and “relation embedding” are used mixedly throughout the paper. I am not sure if they are well-defined terms (it is better to cite relevant paper).

---



---

## Year 2017

### Paper 1 (paper_id: H1aIuk-RW)

**Review 1:**

After reading rebuttals from the authors: The authors have addressed all of my concerns. THe additional experiments are a good addition.

************************
The authors provide an algorithm-agnostic active learning algorithm for multi-class classification. The core technique is to construct a coreset of points whose labels inform the labels of other points.  The coreset construction requires one to construct a set of  points which can cover the entire dataset. While this is NP-hard problem in general, the greedy algorithm is 2-approximate. The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time.  This cover tells us which points are to be queried. The reason why choosing the cover is a good idea is because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space.  The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification.
The experimental results are convincing enough to show that it outperforms other active learning algorithms. However, I have a few major and minor comments.

Major comments:

1. The proof of Lemma 1 is incomplete. We need the Lipschitz constant of the loss function. The loss function is a function of the CNN function and the true label. The proof of lemma 1 only establishes the Lipschitz constant of the CNN function. Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function.

2. The statement of Prop 1 seems a bit confusing to me. the hypothsis says that the loss on the coreset = 0. But the equation in proposition 1 also includes the loss on the coreset. Why is this term included. Is this term not equal to 0?

3. Some important works are missing.  Especially works related to pool based active learning, and landmark results on labell complexity of agnostic active learning.
UPAL: Unbiased Pool based active learning by Ganti & Gray. http://proceedings.mlr.press/v22/ganti12/ganti12.pdf
Efficient active learning of half-spaces by Gonen et al. http://www.jmlr.org/papers/volume14/gonen13a/gonen13a.pdf
A bound on the label complexity of agnostic active learning. http://www.machinelearning.org/proceedings/icml2007/papers/375.pdf

4.  The authors use L_2 loss as their objective function. This is a bit of a weird choice given that they are dealing with multi-class classification and the output layer is a sigmoid layer, making it a natural fit to work with something like a cross-entropy loss function. I guess the theoretical results do not extend to cross-entropy loss, but the authors do not mention these points anywhere in the paper. For example, the ladder network, which is one of the networks used by the authors is a network that uses cross-entropy for training.

Minor-comment:
1. The feasibility program in (6) is an MILP. However, the way it is written it does not look like an MILP. It would have been great had the authors mentioned that u_j \in {0,1}.

2. The authors write on page 4, "Moreover, zero training error can be enforced by converting average loss into maximal loss". It is not clear to me what the authors mean here. For example, can I replace the average error in proposition 1, by maximal loss? Why can I do that? Why would that result in zero training error?

On the whole this is interesting work and the results are very nice. But, the proof for Lemma 1 seems incomplete to me, and some choices (such as choice of loss function) are unjustified. Also, important references in active learning literature are missing.

---

**Review 2:**

This paper studies active learning for convolutional neural networks. Authors formulate the active learning problem as core-set selection and present a novel strategy.

Experiments are performed on three datasets to validate the effectiveness of the proposed method comparing with some baselines.

Theoretical analysis is presented to show the performance of any selected subset using the geometry of the data points.

Authors are suggested to perform experiments on more datasets to make the results more convincing.

The initialization of the CNN model is not clearly introduced, which however, may affect the performance significantly.

---

**Review 3:**

Active learning for deep learning is an interesting topic and there is few useful tool available in the literature. It is happy to see such paper in the field. This paper proposes a batch mode active learning algorithm for CNN as a core-set problem. The authors provide an upper bound of the core-set loss, which is the gap between the training loss on the whole set and the core-set. By minimizing this upper bound, the problem becomes a K-center problem which can be solved by using a greedy approximation method, 2-OPT. The experiments are performed on image classification problem (CIFAR, CALTECH, SVHN datasets), under either supervised setting or weakly-supervised setting. Results show that the proposed method outperforms the random sampling and uncertainty sampling by a large margin. Moreover, the authors show that 2-OPT can save tractable amount of time in practice with a small accuracy drop.

The proposed algorithm is new and writing is clear. However, the paper is not flawless. The proposed active learning framework is under ERM and cover-set, which are currently not supported by deep learning. To validate such theoretical result, a non-deep-learning model should be adopted. The ERM for active learning has been investigated in the literature, such as "Querying discriminative and representative samples for batch mode active learning" in KDD 2013, which also provided an upper bound loss of the batch mode active learning and seems applicable for the problem in this paper. Another interesting question is most of the competing algorithm is myoptic active learning algorithms. The comparison is not fair enough. The authors should provide more competing algorithms in batch mode active learning.

---


### Paper 2 (paper_id: H1DkN7ZCZ)

**Review 1:**

Summary:

In this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free DNA (cfDNA). The idea is that in the sample being sequenced there would also be circulating tumor DNA (ctDNA) so such mutations could be captured in the sequencing reads. The issue is that the ctDNA are expected to be found with low abundance in such samples, and therefore are likely to be hit by few or even single reads. This makes the task of differentiating between sequencing errors and true variants due to ctDNA hard. The authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mutations. To this, they add channels based on low base quality, low mapping quality. The algorithm for learning the context of sequencing reads compared to true mutations is based on a multi layered CNN, with 2/3bp long filters to capture di and trinucleotide frequencies, and a fully connected layer to a softmax function at the top. The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region. One more sample is used for testing and an additional cancer control which is not lung cancer is also used to evaluate performance.

Pros:

The paper tackles what seems to be both an important and challenging problem. We also liked the thoughtful construction of the network and way the reference, the read, the CIGAR and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context. Using matched samples of tumor and normal from the patients is also a nice idea to mimic cfDNA data.

Cons:

While we liked both the challenge posed and the idea to solve it we found several major issues with the work.

First, the writing is far from clear. There are typos and errors all over at an unacceptable level. Many terms are not defined or defined after being introduced (e.g. CIGAR, MF, BQMQ). A more reasonable CS style of organization is to first introduce the methods/model and then the results, but somehow the authors flipped it and started with results first, lacking many definitions and experimental setup to make sense of those.  Yet Sec. 2 “Results” p. 3 is not really results but part of the methods. The “pipeline” is never well defined, only implicitly in p.7 top, and then it is hard to relate the various figures/tables to bottom line results (having the labels wrong does not help that).

The filters by themselves seem trivial and as such do not offer much novelty. Moreover, the authors filter the “normal” samples using those (p.7 top), which makes the entire exercise a possible circular argument.

If the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations (if more than a single read for that mutation is available) - but the authors do not discuss/try that.

The entire dataset is based on 4 patients. It is not clear what is the source of the other cancer control case. The authors claim the reduced performance show they are learning lung cancer-specific context. What evidence do they have for that? Can they show a context they learned and make sense of it? How does this relate to the original papers they cite to motivate this direction (Alexandrov 2013)? Since we know nothing about all these samples it may very well be that that are learning technical artifacts related to their specific batch of 4 patients. As such, this may have very little relevance for the actual problem of cfDNA.

Finally, performance itself did not seem to improve significantly compared to previous methods/simple filters, and the novelty in terms of ML and insights about learning representations seemed limited.

Albeit the above caveats, we iterate the paper offers a nice construction for an important problem. We believe the method and paper could potentially be improved and make a good fit for a future bioinformatics focused meeting such as ISMB/RECOMB.

---

**Review 2:**

In this paper the author propose a CNN based solution for somatic mutation calling at ultra low allele frequencies.
The tackled problem is a hard task in computational biology, and the proposed solution Kittyhawk, although designed with very standard ingredients (several layers of CNN inspired to the VGG structure), seems to be very effective on both the shown datasets.
The paper is well written (up to a few misprints), the introduction and the biological background very accurate (although a bit technical for the broader audience) and the bibliography reasonably complete. Maybe the manuscript part with the definition of the accuracy measures may be skipped. Moreover, the authors themselves suggest how to proceed along this line of research with further improvements.
I would only suggest to expand the experimental section with further (real) examples to strengthen the claim.
Overall, I rate this manuscript in the top 50% of the accepted papers.

---

**Review 3:**

his paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell-free DNA. They key innovation is a convolutional architecture that represents the invariance around the target base. The method is validated on simulations as well as in cfDNA and is s
hown to provide increased precision over competing methods.

While the method is of interest, there are more recent mutation callers that should be compared. For example, Snooper which uses a RandomForest  (https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-016-3281-2) and hence would be of interest as another machine learning framework. They also should compare to Strelka whic
h interestingly they included only to make final calls of mutations but not in the comparison.

Further, I  would also have liked to see the use of standard benchmark datasets for mutation calling ( https://www.nature.com/articles/ncomms10001).

It appears that the proposed method (Kittyhawk) has a steep decrease in PPV and enrichment for low tumor fraction which are presumably the parameter of greatest interest. The authors should explore this behavior in greater detail.

---


### Paper 3 (paper_id: Bki1Ct1AW)

**Review 1:**

In this contribution, the authors propose an improvement of a tensor decomposition method for decoding spike train. Relying on a non-negative matrix factorization, the authors tackle the influence of the baseline activity on the decomposition. The main consequence is that the retrieved components are not necessarily non-negative and the proposed decomposition rely on signed activation coefficients. An experimental validation shows that for high frequency baseline (> 0.7 Hz), the baseline corrected algorithm yields better classification results than non-corrected version (and other common factorization techniques).

The objective function is defined with a Frobenius norm, which has an important influence on the obtained solutions, as it could be seen on Figure 2. The proposed method seems to provide a more discriminant factorization than the NMF one, at the expense of the sparsity of spatial and temporal components, impeding the biological interpretability.  A possible solution is to add a regularization term to the objective function to ensure the sparsity of the factorization.

---

**Review 2:**

This study proposes the use of non-negative matrix factorization accounting for baseline by subtracting the pre-stimulus baseline from each trial and subsequently decompose the data using a 3-way factorization thereby identifying spatial and temporal modules as well as their signed activation. The method is used on data recorded from mouse and pig retinal ganglion cells of time binned spike trains providing improved performance over non-baseline corrected data.

Pros:
The paper is well written, the analysis interesting and the application of the Tucker2 framework sound. Removing baseline is a reasonable step and the paper includes analysis of several spike-train datasets. The analysis of the approaches in terms of their ability to decode is also sound and interesting.

Cons:
I find the novelty of the paper limited:
The authors extend the work by (Onken et al. 2016) to subtract baseline (a rather marginal innovation) of this approach. To use a semi-NMF type of update rule (as proposed by Ding et al .2010) and apply the approach to new spike-train datasets evaluating performance by their decoding ability (decoding also considered in Onken et al. 2016).

Multiplicative update-rules are known to suffer from slow-convergence and I would suspect this also to be an issue for the semi-NMF update rules. It would therefore be relevant and quite easy to consider other approaches such as active set or column wise updating also denoted HALS which admit negative values in the optimization, see also the review by N. Giles
https://arxiv.org/abs/1401.5226
as well as for instance:
Nielsen, Søren Føns Vind, and Morten Mørup. "Non-negative tensor factorization with missing data for the modeling of gene expressions in the human brain." Machine Learning for Signal Processing (MLSP), 2014 IEEE International Workshop on. IEEE, 2014.

It would improve the paper to also discuss that the non-negativity constrained Tucker2 model may be subject to local minima solutions and have issues of non-uniqueness (i.e. rotational ambiguity). At least local minima issues could be assessed using multiple random initializations.

The results are in general only marginally improved by the baseline corrected non-negativity constrained approach. For comparison the existing methods ICA, Tucker2 should also be evaluated for the baseline corrected data, to see if it is the constrained representation or the preprocessing influencing the performance. Finally, how performance is influenced by dimensionality P and L should also be clarified.

It seems that it would be naturally to model the baseline by including mean values in the model rather than treating the baseline as a preprocessing step. This would bridge the entire framework as one model and make it potentially possible to avoid structure well represented by the Tucker2 representation to be removed by the preprocessing.



Minor:
The approach corresponds to a Tucker2 decomposition with non-negativity constrained factor matrices and unconstrained core - please clarify this as you also compare to Tucker2 in the paper with orthogonal factor matrices.

Ding et al. in their semi-NMF work provide elaborate derivation with convergence guarantees.  In the present paper these details are omitted and it is unclear how the update rules are derived from the KKT conditions and the Lagrange multiplier and how they differ from standard semi-NMF, this should be better clarified.

---

**Review 3:**

In this paper, the authors present an adaptation of space-by-time non-negative matrix factorization (SbT-NMF) that can rigorously account for the pre-stimulus baseline activity. The authors go on to compare their baseline-corrected (BC) method with several established methods for dimensionality reduction of spike train data.

Overall, the results are a bit mixed. The BC method often performs similarly to or is outperformed by non-BC SbT-NMF. The authors provide a possible mechanism to explain these results, by analyzing classification performance as a function of baseline firing rate. The authors posit that their method can be useful when sensory responses are on the order of magnitude of baseline activity; however, this doesn't fully address why non-BC SbT-NMF can strongly outperform the BC method in certain tasks (e.g. the step of light, Fig. 3b). Finally, while this method introduces a principled way to remove mean baseline activity from the sensory-driven response, this may also discount the effect that baseline firing rate and fast temporal fluctuations can have on the response (Destexhe et al., Nature Reviews Neuroscience 4, 2003; Gutnisky DA et al., Cerebral Cortex 27, 2017).

---


### Paper 4 (paper_id: rJ7yZ2P6-)

**Review 1:**

The main contributions in this paper are:
1) New variants of a recent LSTM-based model ("ESIM") are applied to the task of response-selection in dialogue modeling -- ESIM was originally introduced and evaluated for natural language inference. In this new setting, the ESIM model (vanilla and extended) outperform previous models when trained and evaluated on two distinct conversational datasets.

2) A fairly trivial method is proposed to extend the coverage of pre-trained word embeddings to deal with the OOV problem that arises when applying them to these conversational datasets.
The method itself is to combine d1-dimensional word embeddings that were pretrained on a large unannotated corpus (vocabulary S) with distinct d2-dimensional word embeddings that are trained on the task-specific training data (vocabulary T). The enhanced (d1+d2)-dimensional representation for a word is constructed by concatenating its vectors from the two embeddings, setting either the d1- or d2-dimensional subvector to zeros when the word is absent from either S or T, respectively. This method is incorporated as an extension into ESIM and evaluated on the two conversation datasets.

The main results can be characterized as showing that this vocabulary extension method leads to performance gains on two datasets, on top of an ESIM-model extended with character-based word embeddings, which itself outperforms the vanilla ESIM model.

These empirical results are potentially meaningful and could justify reporting, but the paper's organization is very confusing, and too many details are too unclear, leading to low confidence in reproducibility.

There is basic novelty in applying the base model to a new task, and the analysis of the role of the special conversational boundary tokens is interesting and can help to inform future modeling choices. The embedding-enhancing method has low originality but is effective on this particular combination of model architecture, task and datasets. I am left wondering how well it might generalize to other models or tasks, since the problem it addresses shows up in many other places too...

Overall, the presentation switches back and forth between the Douban corpus and the Ubuntu corpus, and between word2vec and Glove embeddings, and this makes it very challenging to understand the details fully.

S3.1 - Word representation layer: This paragraph should probably mention that the character-composed embeddings are newly introduced here, and were not part of the original formulation of ESIM. That statement is currently hidden in the figure caption.

Algorithm 1:
- What set does P denote, and what is the set-theoretic relation between P and T?
- Under one possible interpretation, there may be items in P that are in neither T nor S, yet the algorithm does not define embeddings for those items even though its output is described as "a dictionary with word embeddings ... for P". This does not seem consistent? I think the sentence in S4.2 about initializing remaining OOV words as zeros is relevant and wonder if it should form part of the algorithm description?

S4.1 - What do the authors mean by the statement that response candidates for the Douban corpus were "collected by Lucene retrieval model"?

S4.2 - Paragraph two is very unclear. In particular, I don't understand the role of the Glove vectors here when Algorithm 1 is used, since the authors refer to word2vec vectors later in this paragraph and also in the Algorithm description.

S4.3 - It's insufficiently clear what the model definitions are for the Douban corpus. Is there still a character-based LSTM involved, or does FastText make it unnecessary?

S4.3 - "It can be seen from table 3 that the original ESIM did not perform well without character embedding." This is a curious way to describe the result, when, in fact, the ESIM model in table 3 already outperforms all the previous models listed.

S4.4 - gensim package -- for the benefit of readers unfamiliar with gensim, the text should ideally state explicitly that it is used to create the *word2vec* embeddings, instead of the ambiguous "word embeddings".

---

**Review 2:**

Summary:
This paper proposes an approach to improve the out-of-vocabulary embedding prediction for the task of modeling dialogue conversations. The proposed approach uses generic embeddings and combines them with the embeddings trained on the training dataset in a straightforward string-matching algorithm. In addition, the paper also makes a couple of improvements to Chen et. al's enhanced LSTM by adding character-level embeddings and replacing average pooling by LSTM last state summary vector. The results are shown on the standard Ubuntu dialogue dataset as well as a new Douban conversation dataset. The proposed approach gives sizable gains over the baselines.


Comments:

The paper is well written and puts itself nicely in context of previous work. Though, the proposed extension to handle out-of-vocabulary items is a simple and straightforward string matching algorithm, but nonetheless it gives noticeable increase in empirical performance on both the tasks. All in all, the methodological novelty of the paper is small but it has high practical relevance in terms of giving improved accuracy on an important task of dialogue conversation.

---

**Review 3:**

The paper considers a setting (Ubuntu Dialogue Corpus and Douban Conversation Corpus) where most word types in the data are not covered by pretrained representations. The proposed solution is to combine (1) external pretrained word embeddings and (2) pretrained word embeddings on the training data by keeping them as two views: use the view if it's available, otherwise use a zero vector. This scheme is shown to perform well compared to other methods, specifically combinations of pretraining vs not pretraining embeddings on the training data, updating vs not updating embeddings during training, and others.

Quality: Low. The research is not very well modularized: the addressed problem has nothing specifically to do with ESIM and dialogue response classification, but it's all tangled up. The proposed solution is reasonable but rather minor. Given that the model will learn task-specific word representations on the training set anyway, it's not clear how important it is to follow this procedure, though minor improvement is reported (Table 5).

Clarity: The writing is clear. But the point of the paper is not immediately obvious because of its failure to modularize its contributions (see above).

Originality: Low to minor.

Significance: It's not convincing that an incremental improvement in the pretraining phase is so significant, for instance compared to developing a novel better architecture actually tailored to the dialogue task.

---


### Paper 5 (paper_id: BJluxbWC-)

**Review 1:**

This paper concerns open-world classification.  The open-world related tasks have been defined in many previous works. This paper had made a good survey.
The only special point of the open-word classification task defined in this paper is to employ the constraints from the similarity/difference expected for examples from the same class or from different classes.  Unfortunately, this paper is lack of novelty.

Firstly, the problem context and setting is kinda synthesized. I cannot quite imagine in what kind of applications we can get “a set of pairs of intra-class (same class) examples, and the negative training data consists of a set of pairs of inter-class”.

Secondly, this model is just a direct combination of the recent powerful algorithms such as DOC and other simple traditional models. I do not really see enough novelty here.

Thirdly, the experiments are only on the MNIST and EMNIST; still not quite sure any real-world problems/datasets can be used to validate this approach.
I also cannot see the promising performance. The clustering results of rejected
examples are still far from the ground truth, and comparing the result with
a total unsupervised K-means is a kind of unreasonable.

---

**Review 2:**

This paper focuses on the sub-problem of discovering previously unseen classes for open-world classification.
It employs a previously proposed system, Open Classification Network, for classifying instances into known classes or rejecting as belonging to an unseen class, and applies hierarchical clustering to the rejected instances to identify unseen classes.
The key novel idea is to learn a pairwise similarity function using the examples from the known classes to apply to examples of unknown classes. The argument is that we tend to use the same notion of similarity and dissimilarity to define classes (known or unknown) and one can thus expect the similarity function learned from known classes to carry over to the unknown classes.  This concept is not new. Similar idea has been explored in early 2000 by Finley and Joachims in their ICML paper titled "Supervised Clustering with Support Vector Machines".  But to the best of my knowledge, this is the first paper that applies this concept to the open world classification task.

Once we learn the similarity function, the rest of the approach is straightforward, without any particular technical ingenuity.  It simply applies hierarchical clustering on the learned similarities and use cross-validation to pick a stopping condition for deciding the number of clusters.
I find the experiments to be limited, only on two hand-written digits/letters datasets.  Such datasets are too simplistic. For example, simply applying kmeans to PCA features of the images on the MNIST data can get you pretty good performance.
Experiments on more complex data is desired, for example on Imagenet classes.

Also the results do not clearly demonstrate the advantage of the proposed method, in particular the benefit of using PCN. The number of clusters found by the algorithm is not particularly accurate and the NMI values obtained by the proposed approach does not show any clear advantage over baseline methods that do not use PCN.

Some minor comments:
When applied to the rejected examples, wouldn't the ground truth # of clusters no longer be 4 or 10 because there are some known-class examples mixed in?
For the base line Encoder+HC, was the encoder trained independently? Or it's trained jointly with PCN and OCN?  It is interesting to see the impact of incorporating PCN into the training of OCN and encoder. Does that have any impact on accuracy of OCN?
It seems that one of the claimed benefit is that the proposed method is effective at identifying the k. If so, it would be necessary to compared the proposed method to some classic methods for identifying k with kmeans, such as the elbow method, BIC, G-means etc, especially since kmeans seem to give much better NMI values.

---

**Review 3:**

The main goal of this paper is to cluster images from classes unseen during training.
This is an interesting extension of the open-world paradigm, where at test time, the classifier has to identify images beloning to the C seen classes during training, but also identify (reject) images which were previously unseen. These rejected images could be clustered to identify the number of unseen classes; either for revealing the underlying structure of the unseen classes, or to reduce annotation costs.

In order to do so, an extensive framework is proposed, consisting of 3 ConvNet architectures, followed by a hierarchical clustering approach. The 3 ConvNets all have a different goal:
1. an Open Classification Network (per class sigmoid, trained 1vsRest, with thresholds for rejection)
2. Pairwise Classification Network, (binary sigmoid, trained on pairs of images of same/different classes)
3. Auto encoder network

These network are jointly trained, and the joint-loss is simply the addition of a cross-entropy loss (from OCN), the binary cross-entropy loss (from PCN) and a pixel wise loss (from AE).
Remarks:
- it is unclear if the ConvNet weights of the first layers are shared).
- it is unclear how joint training might help, given that the objectives do not influence each other
- Eq 1:
  *label "y_i" has two different semantics (L_ocn it is the class label, while in L_pcn it is the label of an image pair being from the same class or not)
  * s_j is undefined
  * relation between the p(y_i = 1) (in PCN) and g(x_p,x_q) in Eq 2 could be made more explicit, PCN depends on two images, according to Eq 1, it seems just a sum over single images.
- It is unclear why the Auto Encoder network is added, and what its function is.
- It is unclear wether OCN requires/uses unseen class examples during training.
- Last paragraph of 3.1 "The 1-vs-rest ... rejected", I don't see why you need 1vsRest classifiers for this, a multi-class (softmax) output can also be thresholded to reject an test image from the known classes and to assign it to the unknown class.


Experimental evaluation
The experimental evaluation uses 2 datasets, MNIST and EMNIST, both are very specific for character recognition. It is a pity that not also more general image classification has been considered (CIFAR100, ImageNet, Places365, etc), that would provide insights to the more general behaviour of the proposed ideas.

My major concern is that the clustering task is not extensively explored. Just a single setting (with a single random sampling of seen/unseen classes) has been evaluated. This is -in part- due to the nature of the chosen datasets, in a 10 class dataset it is difficult to show the influence of the number of unseen classes. So, I'd really urge the authors to extend this evaluation. Will the method discover more classes when 100 unknown classes are used? What kind of clusters are discovered? Are the types of classes in the seen/unseen classes important, I'd expect at least multiple runs of the current experiments on (E)MNIST.

Further, I miss some baselines and ablation study. Questions which I'd like to seen answered: how good is the OCN representation when used for clustering compared to the PCN representation? What is the benefit of joint-training? How important is the AE in the loss?

Remaining remarks
- Just a very simple / non-standard ConvNet architecture is trained. Will a ResNet(32) show similar performance?
- In Eq 4, |C_i || y_j| seems a strange notation for union.

Conclusion
This paper brings in an interesting idea, is it possible to cluster the unseen classes in an open-world classification scenario?  A solution using a pairwise convnet followed by hierarchical clustering is proposed. This is a plausible solution, yet in total I miss an exploration of the solution.

Both in terms of general visual classification (only MNIST is used, while it would be nice to see results on CIFAR and/or ImageNet as in Bendale&Boult 2016), as in exploration of different scenarios (different number of unseen classes, different samplings) and ablation of the method (independent training, using OCN for hierarchical clustering, influence of Auto Encoder). Therefore, I rate this paper as a (weak) reject: it is just not (yet) good enough for acceptance.

---



---

## Year 2018

### Paper 1 (paper_id: rylKB3A9Fm)

**Review 1:**

Update: Lower the confidence and score after reading other comments.
===

In this paper, the authors benchmark several RL algorithms on their abilities of generalization. The experiments show interpolation is somehow manageable but extrapolation is difficult to achieve.

The writing quality is rather good. The authors make it very clear on how their experiments run and how to interpret their results. The experiments are also solid. It's interesting to see that both EPOpt and RL^2, which claim to generalize better, generalize worse than the vanilla counterparts. Since the success rates are sometimes higher with more exploration, could it be possible that the hyperparameters of EPOpt and RL^2 are non-optimal?

For interpolation/extrapolation tasks, all 5 numbers (RR, EE, DR, DE, RE) are expected since the geometric mean is always 0 once any of the numbers is 0.

What does ``"KL divergence coefficient" in RL^2-PPO mean? OpenAI's Baselines' implementation includes an entropy term as in A2C.

---

**Review 2:**

This paper proposes a benchmark for for reinforcement learning to study generalization in stationary and changing environments. A combination of several existing env. from OpenAi gym is taken and several ways to set this parameters is proposed. Paper provides a relatively thorough study of popular methodologies on this benchmark.

Overall, I am not sure there is a pressing need for this benchmark and paper does not provide an argument why there is an urgent need for one.

For instance, paragraph 3 on page 1 details a number of previous studies. Why those benchmarks are in-adequate?
On page at the end of second paragraph a  number of benchmarks from transfer learning literature is mentioned. Why not just use those and disallow model updates?
In the same way, it is not clear why new metric is introduced? How does it correlate with standard reward metrics?

Overall, as empirical study, I think this work is interesting but I think paper should justify why we need this new benchmark.

---

**Review 3:**

This paper presents a new benchmark for studying generalization in deep RL along with a set of benchmark results. The benchmark consists of several standard RL tasks like Mountain Car along with several Mujoco continuous control tasks. Generalization is measured with respect to changes in environment parameters like force magnitude and pole length. Both interpolation and extrapolation are considered.

The problem considered in this paper is important and I agree with the authors that a good set of benchmarks for studying generalization is needed. However, a paper proposing a new benchmark should have a good argument for why the set of problems considered is interesting. Similarly, the types of generalization considered should be well motivated. This paper doesn’t do a good job of motivating these choices.

For example, why is Mountain Car a good task for studying generalization in deep RL? Mountain Car is a classic problem with a two-dimensional state space. This is hardly the kind of problem where deep RL shines or is even needed at all. Similarly, why should we care whether an agent trained on the Cart Pole task can generalize to a pole length between 2x and 10x shorter than the one it was trained on without being allowed to update its policy? Both the set of tasks and the distributions of parameters over which generalization is measured seem somewhat arbitrary.

Similarly, the restriction to methods that do not update its policy at test time also seems arbitrary since this is somewhat of a gray area. RL^2, which is one of the baselines in the paper, uses memory to adapt its policy to the current environment at test time. How different is this from an agent that updates its weights at test time? Why allow one but not the other?

In addition to these issues with the proposed benchmark, the baseline results don’t provide any new insights. The main conclusion is that extrapolation is more difficult than interpolation, which is in turn more difficult than training and testing on the same task. Beyond that, the results are very confusing. Two methods for improving generalization (EPOpt and RL^2) are evaluated and both of them seem to mostly decrease generalization performance. I find the poor performance of RL^2-A2C especially worrisome. Isn’t it essentially recurrent A2C where the reward and action are fed in as inputs? Why should the performance drop by 20-40%?

Overall, I don’t see the proposed tasks becoming a widely used benchmark for evaluating generalization in deep RL. There are just too many seemingly arbitrary choices in the design of this benchmark and the lack of interesting findings in the baseline experiments highlights these issues.

Other comments:
- “Massively Parallel Methods for Deep Reinforcement Learning” by Nair et al. introduced the human starts evaluation condition for Atari games in order to measure generalization to potentially unseen states. This should probably be discussed in related work.
- It would be good to include the exact architecture details since it’s not clear how rewards and actions are given to the RL^2 agents.

---


### Paper 2 (paper_id: r1fiFs09YX)

**Review 1:**

This paper proposes to apply MAML to a multi-agent setting. In this formulation each opponent corresponds to a task and two separate parts of the policy are learned via meta-learning:
1) the opponent modelling network that predicts the value function for a given opponent based on past actions and states.
2) the policy network which takes in the state and the predicted value function of the opponent.
The main concern with this paper is the lack of technical detail and an important missing baseline. The paper also suffers from lacking clarity due to a large number of grammatical mistakes.

Technical detail and concerns:
The paper mentions Duelling DQN as the RL algorithm in the inner loop. This is very unusual and it's a priori unclear whether MAML with DQN in the inner loop is a sensible algorithm. For example, DQN relies both on a target network and an argmax operator which seem to violate the differentiability requirements needed for MAML regarding higher order gradients. The authors entirely miss this and fail to address possible concerns.

The authors also fail to provide any details regarding the exploration scheme used. In fact, a value function is never mentioned, instead the authors talk about a policy pi^a_i, leaving it unclear how this policy is derived from the value function. When the Q-function takes as input the true opponent, there is no need for meta-learning of the policy: Given a known opponent, the tuple (s_t, opponent) defines a Markov state. As far as I could gather from the paper, the authors are missing a baseline which simply learns a single Q-function across all opponents (rather than meta-learning it per opponent) that takes as input the predicted opponent.
My expectation is that this is more or less what is happening in the paper. The authors also fail to compare and contrast their method to a number of recent multi-agent algorithms, eg. MADDPG, COMA and LOLA.

Furthermore, the results are extremely toy and seem to be for single runs , rendering them insignificant.

While the idea itself is interesting, the above concerns render the paper unsuitable for publication in it's current form.

---

**Review 2:**

The paper presents an approach to multi-agent learning based on the framework of model-agnostic meta learning. The originality of the approach lies in the decomposition of the policy in two terms, with applications to opponent modeling: the first part of the policy tries to predict some important characteristic of the agent (the characteristic itself is prior knowledge, the value it takes for a particular opponent is learnt from observations). The second part of the policy takes the estimated characteristic of the opponent as input, the current state and produces the action. All networks are trained within the MAML framework. The overall approach is motivated by the task of opponent modeling for multi-agent RL.

The approach makes sense overall -- the "value" of the opponent is valuable prior knowledge. The originality is limited though. In this kind of paper, I would expect the experiments to make a strong case for the approach. Unfortunately, the experiments are extremely toyish and admittedly not really "multi-agent": the "opponent" has a fixed strategy that does not depend on what the current is doing (it is therefore not really an opponent). The experimental protocol is more akin to multitask RL than multi-agent RL, and it is unclear whether the approach could/should work for opponent modeling even on tasks of low complexity. In other words, the experimental section does not address the problem that is supposed to be addressed (opponent modeling).

other comments:
- "The opponent in our game is considered as some player that won’t adapt its policy to our agent." -> in the experiments it is worse than that: the opponents actions do not even depend on what the agent is doing... So admittedly the experiments are not really "multi-agent" (or "multi-agent" where the "opponent" is totally independent of what the agent is currently doing).

- "Each method trains 800 iterations to get the meta learners and use them to initialize their networks. Then 10 new opponents are sampled as testing tasks. Four methods all train 4000 games for each testing task." -> what does 800 iterations mean? Does it mean 800 episodes (it would seem strange for a "fast adaptation task" to have fewer episodes for training than for testing).

- "Notice that the reward trend for MOA first drops and then raises as the testing process goes on. This shows the process that the meta-learner adapt to the current task." -> the adaptation to the new opponent does not really explain the drop?

- Figure 3(c): the MA baseline has a reward of ~-10, which is worse than random (a uniform random placement at the 5 strategic positions would get 10*1/5-10*4/5 = -6). On the other hand, MOA achieves very high rewards, which indicates that the "opponents" strategies have low entropy. What is the best achievable reward on the blocking game?

---

**Review 3:**

This paper focuses on fast adaptation to new behaviour of the other agents of the environment, be it opponents or allies. To achieve this, a method based on MAML is proposed, with two main components:
1) Learn a model of some characteristics of the opponent, such as "the final goal, next action, or any other character we wish to predict"
2) Learn a policy that takes as input the output of the model and the state, and that outputs the action of the agent.

The goal is that after a phase of meta learning, where the agents learns how to play against some new agents sampled from the distribution of opponents, it can quickly adapt to a new unseen agent. ("Experimental results show that the agent can adapt to the new opponent with a small number of interactions with the opponent")

While the motivation of this work is clear and the goal important for the RL community, the experiments fail to support the claim above.

The first task they demonstrate their approach on is a chasing game, where the opponent has a private goal cell it tries to reach, and the agent has to chase it. At the end of the game, it gets a reward of 10 if it is on the same cell, 5 if in an adjacent cell, and 0 otherwise. The exact details of the dynamic are not really clear, for example what happens in the event of a collision is not mentionned, and the termination condition is not mentionned either. (the text reads "One game lasts for at least 15 steps", maybe it was meant to be "at most 15 steps" ?).
The first incoherent aspect of this experiment is that they use 800 iterations of meta-learning, and then, when testing, they fine-tune their networks against each test opponent during 4000 games. That is, they use 5 times more game when fine-tuning as opposed to when pre-training, which contradicts the claim "the agent can adapt to the new opponent with a small number of interactions with the opponent" (this is not really few-shot learning anymore).
Further more, they compare their approach with various ablations of it: they either remove the meta-learning for the model (MA), for the policy (MO), or both (NM). The description of the NM baseline is not very precise, but it seems that it simply boils down to a regular (dueling) DQN: In this setting, since the opponent appears to have a fixed goal, finetuning against a single opponent simply boils down to learning a policy that reaches a specific cell of the grid, which we can expect DQN to solve perfectly on a 8x8 grid with 4000 training games. And yet, the curves for NM in graph 2c is not only really noisy, but also falls far from the optimum, which the authors don't discuss. There might be a problem with the hyperparameters used or the training loop.

The second task is a blocking game: the opponent has to choose amongst 5 paths to get to the top, and the agent has to choose the same path in order to block it. The action space should be precisely described, as it stands it is difficult to understand the dynamic. There are at least two possible ways to parametrize the actions:
1) Similarly to the blocking game, the agents could move in the 8 directions. In that case, based on the picture 3a, it seems that the agent can just mirrors the move of the opponent: since the moves are simultaneous, that would mean that the agent is always one step late, but each path is long enough for the agent to reach the exit before its opponent (it explicitly stated that the agent needs to block the exit, and that the opponent will not change path during one game). That would imply that perfect play is possible without any meta-learning or oponent modeling, and once again the NM baseline (or any vanilly DQN/Policy gradient method) should perform much better.
2) One other alternative is to have an action space of 5 actions, which correspond to the 5 paths. In that case the game boils down to a bandit, since both agents only take one action. Note that under this assumption, the random policy would get the right path (and reward +10) with probability 1/5 and a wrong one (reward -10) with probability 4/5, which leads to an expectated reward of -10*4/5 + 10/5 = -6. This is not consistent with the graph 3c, since at the beginning of the training, the NM agent should have a random policy, and yet the graph reports an average reward of -10 (the -6 mark seems to be reached after ~1000 episodes)

The last task boils down to one opponent that reaches one cell on the right, and the agent must reach the matching cell on the left. In this setting, the same discussion on the action space as the second task can be made. We note that the episode for 16 steps, and the distance from the center to any cell is at most 4 steps: an optimal policy would be to wait for 4 steps in the middle, and as soon as the opponent has reached its goal, use the remaining 12 steps to get to the mirror one. Once again, this policy doesn't require any prediction on the opponent's goal, and it's hard to believe that DQN (possibly with an lstm) is not able to learn that near perfectly.


In a last test the authors compare the performance of their algorithms in a one shot transfer setting: they sample 100 opponents for each task and play only one game against it (no fine-tuning). It is not clear whether special care has been taken to ensure that none of the sampled opponents has already been seen during training.
We note that the rewards reported for MO and MA (resp 0.0 and -0.08) are not consistent with the description of the reward function: on the worst case, the opponent chooses a goal on one extreme (say y1 = 1) and the agent chooses an object on the other end (say y2 = 7). In that case, the reward obtained is sampled from a gaussian with mean \mu = 10 - 3/2 * |y1 - y2| (which in this case evalutes to 1), and variance 1. This is highly unlikely to give such a low average reward over 100 episodes (note that this is worst case, if the opponent's goal is not on the extreme, the expected reward is necessarily higher). One possibility is that the agent never reaches an object, but in that case it would imply the that the meta-learning phase was problematic.
We also note that it is explicited that the MOA, MO and MA methods are tested after meta-training, but nothing is precised for NM. Has it been trained at all? Against which opponents? Is it just a random policy? There are too many missing details for the results to be interpretable.


Apart from that, the paper contains a significant amount of typos and gramatical mistakes please proof-read carefully. Some of them are:
"To demonstrate that meta-learning can do take"
"player 1 is the red grid and player 1 is the green one"
"we further assume that there exist a distribution"
" the goal’s location over the map is visualize in figure"
"Both players takes actions simultaneously"

---


### Paper 3 (paper_id: BJxssoA5KX)

**Review 1:**

Paper summary:
The paper proposes to predict bouncing behavior from visual data. The model has two main components: (1) Physics Interface Module, which predicts the output trajectory from a given incoming trajectory and the physical properties of the contact surface. (2) Visual Interface Module, which predicts the surface properties from a single image and the impact location. A new dataset called Bounce Dataset is proposed for this task.

Paper strengths:
- The paper tackles an interesting and important problem.
- The data has been collected in various real scenes.
- The idea of training the physics part of the network with synthetic data and later fine-tuning it with real images is interesting.
- The experiments are thorough and well-thought-out.

Paper weaknesses:
- It would be more interesting if the dataset was created using multiple types of probe objects. Currently, it is only a ball.

- It is not clear how the evaluation is performed. For instance, the length of the groundtruth and predicted trajectories might be different. How is the difference computed?

- The impact location (x,y) corresponds to multiple locations in 3D. Why not using a 3D point as input? It seems the 3D information is available for both the real and synthetic cases.

- Why is it non-trivial to use a deconvolution network for predicting the output point cloud trajectory?

- The length of the input trajectory can vary, but it seems the proposed architecture assumes a fixed-length trajectory. I am wondering how it handles a variable-length input.

- How is the bounce location encoded in VIM?

- I don't see any statistics about the objects being used for data collection. That should be added to the paper.

>>>>> Final score: The authors have addressed my concerns in the rebuttal. I believe this paper tackles an interesting problem, and the experiments are good enough since this is one of the first papers that tackle this problem. So I keep the initial score.

---

**Review 2:**

This paper presents a method for inferring physical properties of the world (specifically, normals and coefficients of restitution) from both visual and dynamic information.  Objects are represented as trajectories of point clouds used under an encoder/decoder neural network architecture.  Another network is then learned to predict the post bounce trajectory representation given the prebounce trajectory representation given the surface parameters.  This is used both to predict the post bound trajectory (with a forward pass) but also to estimate the surface parameters through an optimization procedure.  This is coupled with a network which attempts to learn these properties from visual cues as well.  This model can be either pretrained and fixed or updated to account for new information about a scene.

The proposed model is trained on a newly collected dataset that includes a mixture of real sequences (with RGB, depth, surface normals, etc) and simulated sequences (additionally with physical parameters) generated with the help of a physics engine.  It is compared with a number of relevant baseline approaches and ablation models.  The results suggest that the proposed model is effective at estimating the physical properties of the scene.

Overall the paper is well written and thoroughly evaluated.  The problem is interesting and novel, the collected dataset is likely to be useful and the proposed solution to the problem is reasonable.

---

**Review 3:**

The authors present both a dataset of videos of a real-world foam ball bouncing and a model to learn the trajectory of the ball at collision (bounce) points in these videos.  The model is comprised of a Physics Inference Module (PIM) and a Visual Inference Module (VIM).  The PIM takes in both a vector of physical parameters (coefficient of restitution and collision normal) and a point cloud representation of the pre-bounce trajectory, and produces a point cloud representation of the post-bounce trajectory (or, rather, an encoded version of such).  The VIM takes in an image and ground-truth bounce location and produces the physical parameters of the surface at that location.

I find the paper well-written and clear.  The motivation in the introduction is persuasive and the related work section is complete.  However, the authors are introducing both a new training paradigm (to my knowledge unused in the literature) and a new model, and without any existing baselines to compare against I find it a bit difficult to understand how well the model works.

Overall, the authors’ model is somewhat complicated and not as general as it initially seems.  To justify this complication I would like to see more convincing results and benchmarking or application to more than one single dataset (e.g. non-spheres bouncing).

Here are some specific concerns:

1)  I could not find a link to an open-sourced version of the dataset(s).  Given that the authors emphasize the dataset as a main contribution of the paper, they should open-source it and make the link prominent in the main text (apologies if I somehow missed it).

2)  The authors claim in multiple places that the model is trained end-to-end, but this does not seem to be the case.  Specifically, the PIM is pre-trained on an auxiliary dataset from simulation.  The trajectory encoder also seems to be pre-trained (though I could be wrong about that, see my question below).  Furthermore, there is a bit of hand-holding:  The PIM uses ground-truth state for pre-training, and the VIM gets the ground-truth bounce location.  In light of this, the model seems a lot less general and end-to-end than implied in the abstract and introduction.

3)  No comparison to existing baselines.  I would like to see how the authors’ model compares to standard video prediction algorithms.  The authors could evaluate their model with respect to pixel loss (after ground-truth rendering) and compare to a video prediction algorithm (such as PredNet by Lotter, Kreiman, & Cox, 2016).  Given that the authors’ method uses some extra “privileged” information (as described in point 2), it should far out-perform algorithms that train only on video data, and such a result would strengthen the paper a lot.

4)  Table 1 is not a very convincing demonstration of performance.  Regardless of baselines, the table does not show confidence intervals.  I would love to see training curves with errorbars of the models on the most important metrics (e.g. Dist and COR Median Absolute Error).

I also was confused about a couple of things:

1)  How was the PointNet trajectory encoder trained?  I did not see this mentioned anywhere.  Were gradients passed through from the PIM?  Was the same network used for both the simulation and real-world data?

2)  The performance of the center-based model in Table 1 seems surprisingly low.  The center-based model should be as good at the Train core, Fix traj. enc. model, since it has access to the ball’s position.  Why is it worse?  Is the VIM at fault?  Or is the sphere-fitting sub-optimal?  How does it compare on the simulated data with ground truth physical parameters?

3)  Lastly, the color-scheme is a bit confusing.  It looks like the foam ball in the videos was rainbow-colored.  However, in the model outputs in trajectory figures time is also rainbow-colored.  This was initially a bit confusing.  Perhaps grayscale for the model outputs would be clearer.

---


### Paper 4 (paper_id: rJgvf3RcFQ)

**Review 1:**

The paper presents and evaluates different common inductive biases in Deep RL. These are systematically evaluated on different experimental settings.

The paper is easy to read and the authors explain well the setting and their findings. The comparison and evaluations is well conducted and valuable contribution to the literature.  I would have liked some more details on the motivating example in section 3.1, maybe with a figure supporting the explanation of the example.

---

**Review 2:**

This paper contains various numerical experiments to see the effects of some heuristics in reinforcement learning. Those heuristics include reward clipping, discounting for effective learning, repeating actions, and different network structures. However, since the training algorithms also greatly affect the performance of RL agents, it seems hard to draw any quantitive conclusions from this paper.

Detailed comments:

1. It seems that actor-critic algorithms are defined for RL with function approximation. What is the tabular A2C algorithm? A reference in Section 3.1 would be better.

2. This paper claims to study the "inductive biases", which is not clearly defined. How to quantify those biases and how to measure "generality"?

3. Are there any quantitive conclusions that can be drawn from the experiments?

4. Since the performance of RL agents also relies on initialization and the training algorithms. There are a lot of tricks of optimization for deep learning. How to measure the "inductive biases" by ruling out the effects of training algorithms?

---

**Review 3:**

This paper focuses on deep reinforcement learning methods and discusses the presence of inductive biases in the existingRL algorithm. Specifically, they discuss biases that take the form of domain knowledge or hyper-parameter tuning. The authors state that such biases rise the tradeoff between generality and performance wherein strong biases can lead to efficient performance but deteriorate generalization across domains. Further, it motivates that most inductive biases has a cost associated to it and hence it is important to study and analyze the effect of such biases.

To support their insights, the authors investigate the performance of well known actor-critic model in the Atari environment after replacing domain specific heuristics with the adaptive components. The author considers two ways of injecting biases: i) sculpting agents objective and ii) sculpting agent's environment. They show empirical evidence that replacing carefully designed heuristics to induce biases with more adaptive counterparts preserves performance and generalizes without additional fine tuning.

The paper focuses on an important concept and problem of inductive biases in deep reinforcement learning techniques.
Analysis of such biases and methods to use them judiciously is an interesting future direction. The paper covers a lot of related work in terms of various algorithms and corresponding biases.
However, this paper only discusses such concepts at high level and provides short empirical evidences in a single environment to support their arguments. Further, both the heuristics used in practice and the adaptive counterparts that the paper uses to replace those heuristics are all available in existing approaches and there is no novel contribution in that direction too.
Finally, the adaptive methods based on parallel environment and RNNs have several limitation, as per author's own admission.

Overall, the paper does not have any novel technical contributions or theoretical analysis on the effect of such inductive biases which makes it very weak. Further, there is nothing surprising about the author's claims and many of the outcomes from the analysis are expected. The authors are recommended to consider this task more rigorously and provide stronger and concrete analysis on the effects of inductive biases on variety of algorithms and variety of environments.

---


### Paper 5 (paper_id: SJf6BhAqK7)

**Review 1:**

This work proposes a learning method based on deep subspace clustering. The method is formulated by identifying a deep data embedding, where clustering is performed in the latent space by a revised version of k-means, inspired by the work [1]. In this way, the proposed method can adapt to account for uni-modal distributions. The authors propose some variations of the framework based on soft cluster assignments, and on cumulative learning of the cluster means.
The method is tested on several scenarios and datasets, showing promising results in prediction accuracy.

The idea presented in this work is reasonable and rather intuitive. However, the paper presentation is often unnecessarily convoluted, and fails in clarifying the key points about the proposed methodology. The paper makes often use of abstract terms and jargon, which sensibly reduce the manuscript clarity and readability. For this reason, in my opinion, it is very difficult to appreciate the contribution of this work, from both methodological and applicative point of view.

Related to this latter point, the use of the term “Bayesian nonparametric” is inappropriate. It is completely unclear in which sense the proposed framework is Bayesian, as it doesn’t present any element related to parameters inference, uncertainty estimation, … Even the fact that the method uses an algorithm illustrated in [1] doesn’t justifies this terminology, as the clustering procedure used here only corresponds to the limit case of a Dirichlet Process Gibbs Sampler when the covariance parameters goes to zero. Moreover, the original procedure requires the iteration until convergence, while it is here applied with a single pass only. The procedure is also known to be sensitive to the order by which the data is provided, and this point is not addressed in this work.

Finally, the novelty of the proposed contribution is questionable. To my understanding, it may consist in the use of embedding methods based on the approach provided in [1]. However, for the reasons illustrated above, this is not clear. There is also a substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper (e.g. [2-5]).  For this reason, the paper would largely benefit from further clarifications and comparison with respect to these methods.





[1] Kulis and Jordan,  Revisiting k-means: New Algorithms via Bayesian Nonparametrics, ICML 2012

[2] Xie, Junyuan, Ross Girshick, and Ali Farhadi. "Unsupervised deep embedding for clustering analysis." International conference on machine learning. 2016.
[3] Ji, Pan, et al. "Deep subspace clustering networks." Advances in Neural Information Processing Systems. 2017.
[4] Jiang, Zhuxi, et al. "Variational deep embedding: An unsupervised and generative approach to clustering." IJCAI 2017
[5] Kodirov, Elyor, Tao Xiang, and Shaogang Gong. "Semantic autoencoder for zero-shot learning. CVPR 2017.

---

**Review 2:**

The paper proposes a meta-learning method that utilizes unlabeled examples along with labeled examples. The technique proposed is very similar to the one by (Ren et al. 2018), only differing in the choice of a different clustering algorithm (Kulis and Jordan, 2012) instead of soft k-means as used by Ren et al.

I feel the contrast to Ren et al, is not provided to the degree it should be. The Appendix paragraph A4 is not sufficient in terms of explaining why this method is conceptually different or significantly better than the related approach. It is hard for me to certify the merits of their work, including explaining the experimental results.

I also do not understand the significance of "multi-model clustering" in this context. Also, by their definition of "variadic", how is this more variadic than Ren et al. or Snell et al.?

---

**Review 3:**

Update after Author Rebuttal
--------------
After reading the rebuttal, I'm pleased that the authors have made significant revisions, but I still think more work is needed. The "hard/soft" hybrid approach still lacks justification and perhaps wasn't compared to a soft/soft approach in a fair and fully-correct way (see detailed reply to authors). I also appreciate the efforts on revising clarity, but still find many clarity issues in the newest version that make the method hard to understand let alone reproduce. I thus stand by my rating of "borderline rejection" and urge the authors to prepare significant revisions for a future venue that avoid hybrids of hard/soft probabilities without justification.

(Original review text below. Detailed replies to authors are in posts below their responses).

Review Summary
--------------
While the focus on variadic learning is interesting, I think the present version of the paper needs far more presentational polish as well as algorithmic improvements before it is ready for ICLR. I think there is the potential for some neat ideas here and I hope the authors prepare stronger versions in the future. However, the current version is unfortunately not comprehensible or reproducible.

Paper Summary
-------------

The paper investigates developing an effective ML method for the "variadic" regime, where the method might be required to perform learning from few or many examples (shots) and few or many classes (ways). The term "variadic" comes from use in computer science for functions that can a flexible number of arguments. There may also be unlabeled data available in the few shot case, creating semi-supervised learning opportunities.

The specific method proposed is called BANDE: Bayesian Nonparametric Deep Embedding. The idea is that each data point's feature vector x_i is transformed into an embedding vector h(x_i) using a neural network, and then clustering occurs in the embedding space via a single-pass of the DP-means algorithm (Kulis & Jordan 2012). Each cluster is assumed to correspond to one "class" in the eventual classification problem, though each class might have multiple clusters (and thus be multi-modal).

Learning occurs in an episodic manner. After each episode (single-pass of DP-means), each point in a query set is embedded to its feature vector, then fed into each cluster's Gaussian likelihoods to produce a normalized cluster-assignment-probability vector that sums to one. This vector is then fed into a cross-entropy loss, where the true class's nearest cluster (largest probability value) is taken to be the true cluster. This loss is used to perform gradient updates of the embedding neural network.

There is also a "cumulative" version of the method called BANDE-C. This version keeps track of cluster means from previous episodes and allows new episodes to be initialized with these.

Experiments examine the proposed approach across image categorization tasks on Omniglot, mini-ImageNet, and CIFAR datasets.


Strengths
---------
* I like that many clusters are used for each true class label, which is better than rigid one-to-one assumptions.


Limitations
-----------
* Can only be used for classification, not regression
* The DP-means procedure does not account for the cluster-specific variance information that is used at other steps of the algorithm


Significance and Originality
----------------------------
To me, the method appears original. Any method that could really succeed across various variadic settings would be significant.



Presentation Concerns
---------------------

I have serious concerns about the presentation quality of this paper. Each section needs careful reorganization as well as rewording.

## P1: Algo. 1 contains numerous omissions that make it as written not correct.

* the number of clusters count variable "n" is not updated anywhere. As writting this algo can only update one extra cluster beyond the original n.
* the variable "c" is unbound in the else clause. You need a line that clarifies that c = argmin_{c in 1 ... n} d_ic

Would be careful about saying that "a single pass is sufficient"... you have *chosen* to do only one pass. When doing k-means, we could also make this choice. Certainly the DP-means objective could keep improving with multiple passes.

## P2: Many figures and tables lack appropriate captions/labels

Table 1: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make very clear here how much labeled data was used.

Table 2: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make how many labeled and unlabeled examples were used easier to find.

## P3: Descriptions of episodic learning and overall algorithm clarity

Readers unfamiliar with episodic learning are not helped with the limited coverage provided here in 3.1 and 3.2. When exactly is the "support" set used and the "query" set used? How do unlabeled points get used (both support and query appear fully labeled)? What is n? What is k? What is T? Why are some points in Q denoted with apostrophes but not others? Providing a more formal step-by-step description (perhaps with pseudocode) will be crucial.

In Sec. 3.2, the paragraph that starts with "The loss is defined" is very hard to read and parse. I suggest adding math to formally define the loss with equations. What parameters are being optimized? Which ones are fixed?

Additionally, in Sec. 3.2: "computed in the same way as standard prototypical networks"... what is the procedure exactly? If your method relies on a procedure, you should specify it in this paper and not make readers guess or lookup a procedure elsewhere.


## P4: Many steps of the algorithm are not detailed

The paper claims to set \lambda using a technique from another paper, but does not summarize this technique. This makes things nearly impossible to reproduce. Please add such details in the appendix.

Major Technical Concerns
------------------------

## Alg. 1 concerns: Requires two (not one) passes and mixes hard and soft assingments and different variance assumptions awkwardly

The BANDE algorithm (Alg. 1) has some unjustified properties. Hard assignment decisions which assume vanishing variances are used to find a closest cluster, but then later soft assignments with non-zero variances are used. This is a bit heuristic and lacks justification... why not use soft assignment throughout? The DP means procedure is derived from a specific objective function that assumes hard assignment. Seems weird to use it for convenience and then discard instead of coming up with the small fix that would make soft assignment consistent throughout.

Furthermore, The authors claim it is a one pass algorithm, but in fact as written in Alg. 1 it seems to require two passes: the first pass keeps an original set of cluster centers fixed and then creates new centers whenever an example's distance to the closest center exceeds \lambda. But then, the *soft* assignment step that updates "z" requires again the distance from each point to all centers be computed, which requires another pass (since some new clusters may exist which did not when the point was first visited). While the new soft values will be close to zero, they will not be *exactly* zero, and thus they matter.

## Unclear if/how cluster-specific variance parameters learned

From the text on top of page 4, it seems that the paper assumes that there exist cluster-specific variances \sigma_c. However, these are not mentioned elsewhere, only a general (not cluster-specific) label variance \sigma and fixed unlabeled variance sigma_u are used.

## Experiments lack comparison to internal baselines

The paper doesn't evaluate sensitivity to key fixed hyperparameters (e.g. \alpha, \lambda) or compare variants of their approach (with and without soft clustering step, with and without multimodality via DP-means). It is difficult to tell which design choices of the method are most crucial.

---



---

## Year 2019

### Paper 1 (paper_id: B1xwcyHFDr)

**Review 1:**

This is a good multiview representation learning paper with new insights. The authors propose to learn variables z_1 and z_2, which are consistent, contain view-invariant information but discard as much view-specific information as possible.
The paper relies on mutual information estimation and is reconstruction-free. It is mentioned in some previous works (e.g. Aaron van den Oord et al. 2018), that reconstruction loss can introduce bias that has a negative effect on the learned representation.
Comparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output. The authors also draw clear connections between a few existing (multiview) representation learning methods to their proposed approaches.
The experimental results on the right side of Figure 3, deliver a very interesting conclusion. In low-resource case, robust feature (obtained by using the larger beta, discarding more superfluous information) is crucial for achieving good performance. While when the amount of labeled data samples is enough, vice-versa.

Here are my major concerns:
1.	In the paper, the authors said the original formulation of IB is only applicable to supervised learning. That is true, but the variational information bottleneck paper [Alexander A. Alem et al. 2017] already showed the connection of unsupervised VIB to VAE in the appendix.
2.	I would not consider the data augmentation used to extend single-view data to “pseudo-multiview” as a contribution. This has been done before (e.g. in the multiview MNIST experiment part of the paper "On Deep Multi-View Representation Learning").
3.	Which MV-InfoMax do you really compare to? You listed a few of them: (Ji et al., 2019; Henaff et al., ´ 2019; Tian et al., 2019; Bachman et al., 2019) in the related work section.
4.	I think the authors should also make a more careful claim on their results in MIR-Flickr.
I’d rather not saying MIB generally outperforms MV-InfoMax on MIR-Flickr, as MIB does not (clearly) outperform MV-InfoMax when enough labeled data is available for training downstream recognizers. But MIB does clearly outperform MV-InfoMax when scaling down the percentage of labeled samples used.
5.	Regarding baselines/experiments
a.	In Figure 4, it seems that VAE (with beta=4) outperforms MV-InfoMax. Why the ``"pseudo-second view" does not help Mv-Infomax in this scenario? Why VAE is clearly better than Infomax?
b.	In Figure 3, you might also tune beta for VCCA and its variants, like what you did for VAE/VIB in a single view.
6.	Do you think your approach can be extended to more than two views easily?
For me, it seems the extension is not trivial, as it requires o(n^2) terms in your loss for n views.
But this is minor.

---

**Review 2:**

In this paper, the authors extend the Information Bottleneck method (to build robust representations by removing information unrelated to the target labels) to the unsupervised setting. Since label information is not available in this setting, the authors leverage multi-view information (e.g., using two images of the same object) , which requires assuming that both views contain all necessary information for the subsequent label prediction task. The representation should then focus on capturing the information shared by both views and discarding the rest. A loss function for learning such representations is proposed. The effectiveness of the proposed technique is confirmed on two datasets. It is also shown to work when doing data augmentation with a single view.

Overall the paper is well motivated, well placed in the literature and well written. Mathematical derivations are provided. Experimental methodology follows the existing literature, seem reasonable and results are convincing. I do not have major negative comments for the authors. This is however not my research area and have only a limited knowledge of the existing body of work.

Comments/Questions:
- How limiting is the multi-view assumption? Are there well-known cases where it doesn't hold? I feel it would be hard to use, say, with text. Has this been discussed in the literature? Some pointers or discussion would be interesting.
- Sketchy dataset: Could the DSH algorithm (one of the best prior results) be penalized by not using the same feature extractor you used?
- Sketchy dataset: Can a reference for the {Siamese,Triplet}-AlexNet results be provided?
- Sketchy dataset: for reproducibility, what is the selected \beta?
- I find it very hard to believe that the accuracy stays constant no matter the number of examples per label used. How can an encoder be trained on 10 images? Did I misunderstand the meaning of this number? Can this be clarified?
- Again for reproducibility, listing the raw numbers for the MNIST experiments would be nice.
- If I understood the experiments correctly, "scarce label regime" is used for both the MIR-Flickr and MNIST datasets, meaning two different things (number of labels per example vs number of examples per label), which is slightly confusing.

Typos:
Page 1: it's -> its
Page 6: the the -> the
Page 7: classifer -> classifier
Page 8: independently -> independent

---

**Review 3:**

This paper extends the information bottleneck method of Tishby et al. (2000) to the unsupervised setting. By taking advantage of multi-view data, they provide two views of the same underlying entity.  Experimetal results on two standard multi-view datasets validate the efficacy of the proposed method.
I have three questions about this work.
1. The proposed method only provides two views of the same underlying entity, what about 3 or more views?
2. Can this method be used for multi-modality case?
3. What about the time efficiency of the proposed method?

---


### Paper 2 (paper_id: B1xoserKPH)

**Review 1:**

Summary: This paper looks at privacy concerns regarding data for a specific model before and after a single update. It discusses the privacy concerns thoroughly and look at language modeling as a representative task. They find that there are plenty of cases namely when the composition of the sequences involve low frequency words, that a lot of information leak occurs.

Positives: The ideas and style of research is nice. This is an important problem and I think this paper does a good job investigating this in the context of language modeling. I do hope the community (and I think the community is) moving towards being aware of these sorts of privacy issues.

Concerns: I don't know how generalizable these results would be on really well-trained language models (rnn, convolution-based, or transformer-based). The related work section doesn't seem particularly well put together, so its difficult to place the work in appropriate context and gauge its impact.

Other Thoughts: I'd like more thorough error analysis looking at exactly what kinds of strings/more nuanced properties of sequences that get a high differential score.

Overall I think this work is interesting and I would encourage the authors to try and add as much quantitative evaluation as possible, but also try and include qualitative information regarding specific sequences after prodding the models. Those could go a long way in strengthening the paper.

---

**Review 2:**

This paper provides an empirical evaluation of the privacy implications of releasing updated versions of language models. The authors show how access to two sequential snapshots of a trained language model can reveal highly specific information about the content of the data used to update the model, even when that data is in-distribution.

The paper contains easy to understand, concrete experiments and results, but seems altogether a little underdeveloped. The methodology is sound, but the synthetic experiments around which much of the paper is based may not be sufficiently novel and give little indication of broader implications. It would have been more convincing if these results replicated with two splits of the same dataset, rather than identical datasets with one augmented by canary tokens.

The qualitative evaluation of subject-specific updates is also not sufficiently informative. It would have been useful to define a specific attack and see under what circumstances such an attack would succeed. In the current results, I am not convinced that any of the phrases in Table 3 represent a privacy violation.

The differential privacy experiment seems to be missing many details: what dataset was this trained on? Are the accuracy values for the training set or a separate testing set? Other works have shown that it is possible to train a differentially private language model without large sacrifices in accuracy, so it would be helpful to know what differentiates this experiment.

I would also note that the motivation, a predictive keyboard, is not a situation in which maximizing accuracy is generally desirable: users tend to find this creepy rather than helpful.

This is a nice idea but would benefit from some more polishing and more extensive testing.

---

**Review 3:**

This paper studies the privacy issue of widely used neural language models in the current literature. The authors consider the privacy implication phenomena of two model snapshots before and after an update. The updating setting considered in this paper is kind of interesting. However, the contribution of the current paper is not strong enough and there are many unclear experimental settings in the current paper.

According to the current paper, the privacy implication seems to be defined in terms of general sequences in training datasets. If this is the case, I don’t think such privacy implication is meaningful because our language models should memorize some general information to achieve their tasks.

There are some unclear settings in the experiments:
1.In the experiments, why there are only 20000 vocabulary size for Wikitext-103 datasets?
2.It is unclear how to construct canary phrases.
3.After constructing the new dataset, the model is retrained or trained in the online way?
4.Since the results for Wikitext-103 is not finished, the authors should remove the results on this dataset.
5.What is the perplexity of the trained models?
6.How to choose initial sequence in real data experiments?
7.When you applying DP mechanism, how did you define the neighboring datasets, and how did you implement it (what is the clipping level, how did you calculate privacy loss for language models)?
8.$\epsilon=111$ seems that the model will provide no privacy guarantee according to the definition of differential privacy?

---


### Paper 3 (paper_id: BklVA2NYvH)

**Review 1:**

Summary:
The goal of this paper is to train neural networks (NNs) in a way to be robust to adversarial attacks. The authors formulate training a NN as finding an optimal controller for a discrete dynamical system. This formulation allows them to use an optimal control algorithm, called method of successive approximations (MSA), to train a NN. The authors then show how constraints can be added to this optimization problem in order to make the trained NN more robust. They show that the resulted constraint optimization problem can be formulated as a semi-definite programming and provide some experimental results.

Comments:
- Although the problem studied in the paper is important and the approach is interesting, it seems the paper has been written in rush and in my opinion is not ready for publication. The writing is not good. The introduction and related work sections are incomplete and not very informative. It is not clear what has been done before and what is the contribution of this paper. The main technique/algorithm of the paper has not been explained clearly that someone can easily understand and implement it. The experimental results are not convincing.
- There are strong claims in the paper such as "experiments show that our method effectively improves deep model's adversarial robustness", this is too strong, given the quality of the experiments of the paper. Or "the constraint optimization problem can be formulated as a semi-definite programming (SDP) problem and hence can be solved efficiently", to the best of my knowledge, SDP solvers are limited to small problems and cannot solve the large problems efficiently.
- The area of making NNs robust to attacks is a very active area and there are many attacks and solutions out there, which require more comprehensive empirical studies of any new method. I do not see this in the paper.
- Overall, I think this paper requires a major revision in order to be evaluated better and to be more useful for the community.

---

**Review 2:**

The paper contributes to the robust training of neural networks as follows:
  1) The paper uses the theoretical view of a neural network as a discretized ODE to develop a robust control theory aimed at training the network while enforcing robustness;
  2) Such an objective is achieved by introducing Lyaponov stability and practically implemented through the method of successive approximations;
  3) Empirical evaluation demonstrate that the newly introduced method performs as well as the SOTA in terms of defensive training.

The paper is well written and proposes a well motivated and theoretically original strategy to robustly train neural networks against adversarial examples.
The strength of the paper is definitively in its theoretical section, it would be really great to see an empirical improvement improvement on the SOTA.
However, I do not believe the paper should be penalized for only matching other algorithm as it relies on a tractable and principled theoretical analysis.

---

**Review 3:**

Neural Networks are vulnerable to adversarial perturbations. This paper proposes a method that based on optimal control theory that uses semidefinite-programming. This is a quite popular topic in Adversarial training recently, there has been a few works in that line. There are almost no experiments in this paper. There are several typos in the paper and writing of this paper requires more work. There are several typos in this paper, for example STOA, should be SOTA (in the Section 6.) In its current state, this paper looks very rushed.


As Yiping Lu pointed out, the PMP statement in this paper is also wrong. At this current stage, unfortunately this paper doesn’t meet the standards of ICLR. I would recommend the authors to go over the paper carefully and resubmit to a different venue.

---


### Paper 4 (paper_id: HkgH0TEYwH)

**Review 1:**

Summary of the work
- The work proposes a new method two find anomaly (out of distribution) data when some labeled anomalies are given.
- The authors apply information theory-derived loss based on that the normal (in distribution) data usually have lower entropy compared to that of the abnormal data.
- The paper conducts extensive experiments on MNIST, Fashion-MNIST, and CIFAR 10, with varying the number of labeled anomlies.

I think the paper is well written and the experiment seems to support the authors argument. Unfortunately, this field is not overlapped to my research field, and it is hard for me to judge this paper.

---

**Review 2:**

[Summary]
The paper proposes an abnormal detection (AD) framework under general settings where 1) unlabeled data, 2) labeled positive (normal) data, and 3) labeled negative (abnormal) data are available (with the last two optional), denoted as semi-supervised AD. Starting from the assumption that abnormal data are sampled from background unpredicted distribution, rather than “cluster” assumption, it is argued that conventional discriminative formulation is not applicable. Motivated by the recent deep AD methods (e.g., deep SVDD), the paper proposes to approach semi-supervised AD from the information theoretic perspective where 1) mutual information between raw data and learnt representation should be maximized (infomax principle), 2) entropy of labeled positive data should be minimized (“compactness” constraint), and 3) enrtropy of labeled negative data should be maximized to reflect the uncertainty assumption of anomaly. The solution is implemented by the encoder of a pre-trained autoencoder that is further fine tuned to enforce entropy assumption on all types of training data. Extensive experiments on benchmarks suggests promising results on the proposed framework versus other state-of-the-arts.

[Comments]
The paper is well written and easy to follow (the presentation is especially pleasant to read). The problem is well defined and of interest to the community under fairly general and practical conditions. Despite the fact that the implementation is only marginally tweaked from previous work (deep SVDD), the theoretical motivation, nevertheless, is sound and well justified, and the empirical evaluation is extensive to reveal the behaviors of the proposed method. It would be better if complexity analysis can also be provided for all concerning methods. Overall, the value of the paper is worth circulation in the community.

[Area to improve]
The manuscript could be further improved by exploring the training process more. In the current format, the solution follows the strategy of deep SVDD that learns the model in two separate stages: pre-training the autoencoder, and then fitting the encoder to enforce compactness and entropy minimization/maximization. What if these are implemented in an end-to-end fashion? Will this help to achieve a better result?

---

**Review 3:**

The authors propose in this paper a variant of Deep SVDD which brings semi-supervision to this model. The paper is well written and contains a thorough experimental evaluation (even disregarding the supplementary material). As far as I know, the proposed method is new and improve anomaly detection. The modification of Deep SVDD are in a way minimalistic, but they do the job.

The only negative aspects, in my opinion, is the "information-theoretic view" which is close to hand waving. The authors are indeed 1) disregarding the regularization term 2) considering an upper bound of the entropy 3) pretending the results on the pre-trained NN hold after post training. Putting everything together, I do not see how this reasoning could accepted. In fact, its extension to Deep SVDD is even more problematic as the discussion in the paper contradicts the reasoning. The authors emphasize the fact that anomalies should not fulfill the clustering assumption (which is indeed an important remark). But then the distribution of phi(x,W) cannot be approximated by a Gaussian for anomalies and thus the bound on the entropy is not valid.

I strongly recommend to remove this part of the paper and to derive Deep SAD from Deep SVDD from heuristics consideration (which is fine!). This will provide an opportunity to remove the cute sentence "We are happy to now introduce Deep SAD".

---


### Paper 5 (paper_id: HklvmlrKPB)

**Review 1:**

Summary:

The paper discusses ways to use autoregressive flows in sequence modelling. Two main variants are considered:
(a) An affine autoregressive flow directly modelling the data.
(b) An affine autoregressive flow whose base distribution is a sequential VAE; equivalently, a sequential VAE whose decoder is an affine autoregressive flow.

Pros:

The paper is very well written and crystal clear. I particularly appreciated the motivating example that shows how each layer of an affine autoregressive flow reduces the order of a linear dynamical system by 1, and the connections with modelling temporal changes and moving reference frames.

The methods are technically correct and well-motivated. The experiments are done well.

Overall, the paper scores high on writing and technical quality.

Cons:

In my opinion, the paper scores low on novelty and original contribution.

In general, it's not clear to me what the claimed contribution is. More specifically:

Is the claimed contribution new methodology for modelling sequences? In my opinion, using flows as VAE decoders, or adding latent variables to a flow model and training it variationally, are standard applications of existing techniques and I wouldn't consider them particularly novel.

Is the claimed contribution improved modelling performance? The main results are that (a) replacing Gaussian decoders with autoregressive flows improves performance, and (b) adding latent variables to the base distribution of an affine autoregressive flow also improves performance. Both of these results are exactly what one would expect from our experience with these methods. Other than that, the paper doesn't present any results that indicate the particular models used enable us to do things we couldn't do before, or improve against the state of the art in sequence modelling.

Is the claimed contribution useful representations? The motivation for using the flow in this particular way as a VAE decoder is that the flow will model low-level correlations whereas the latent variables will capture high-level dynamics. However, the experiments (e.g. the visualizations) don't support this claim, and the usefulness of the learned representations hasn't been demonstrated in an alternative way,

Decision:

Even though the paper is technically correct and well written, my decision is weak reject because of the lack of novelty and original contribution.

Suggestions for improvement:

My main suggestion to the authors is to keep up the good work, but also reflect on what the specific contribution of the paper is, and try to make a stronger case for it. Some minor suggestions/corrections follow:

Eq. (8): As written, the expression makes little sense as \sigma is a vector. I understand that there is supposed to be a sum over the elements of log\sigma, so I'd suggest expressing that more clearly.

Eq. (9): It seems to me that the last Jacobian is upside down.

In general, it would be good to be more thorough on how this paper is similar to related work and how it differs. There is also this related work which may be good to discuss:

Latent Normalizing Flows for Discrete Sequences, https://arxiv.org/abs/1901.10548

In the particle analogy of the motivating example of section 3.1, it would be good to say explicitly that x is the position, u is the velocity and w is the force, to make the example even more intuitive.

The paper only considers affine autoregressive flows, but there has been a lot of recent work on non-affine autoregressive flows that are more expressive, for example:

Neural Autoregressive Flows, https://arxiv.org/abs/1804.00779
Sum-Of-Squares Polynomial Flow, https://arxiv.org/abs/1905.02325
Neural Spline Flows, https://arxiv.org/abs/1906.04032

Such flows could improve the experimental results of the paper. At the very least, it would be good to discuss them as more flexible alternatives.

In section 3.2, a third and very significant limitation of the flows discussed here is that they act elementwise on the dimensions (e.g. pixels) of y_t.

In the experimental section, it would be good to describe on a high level what the architecture of the VAE is, especially the architecture of the prior and the encoder, and the types of distributions used there (e.g. diagonal Gaussians or otherwise).

It would be good to show samples from the models in the experimental results.

---

**Review 2:**

Summary
The paper proposes to combine the video modeling approaches based on autoregressive flows (e.g. Kumar’19) with amortized variational inference (e.g. Denton’18), wherein an autoregressive latent variable model optimized with variational inference is extended with an autoregressive flow that further transforms the output of the latent variable model while allowing to compute exact conditional probability. This is motivated with a physical intuition, where a dynamics model can benefit from decorrelating the inputs, and it is demonstrated that layers of autoregressive flows can represent derivatives of the original signal. In a proof-of-concept experiment, it is shown that using a layer of autoregressive flow improves NLL of a latent variable model.

Decision
The paper presents an interesting method and tackles an important problem. At the same time, the properties of the proposed method are not well exposed and the experimental evaluation is incomplete. Moreover, the motivation of the paper is confusingly disconnected from the proposed model. I rate this paper as borderline, but am hopeful that some of the issues will be clarified during the discussion period.

Pros
- The paper is well-motivated and tackles a significant problem.
- The proposed method is novel.
- The paper is well-written.

Cons
- The experimental evaluation is incomplete and does not expose the properties of the method fully. Comparisons to prior art are missing. (see below)
- The motivation is disconnected from the proposed model. The introduction of the paper motivates a model that hierarchically decorrelates a sequence of frames to arrive at a fully factorized model, which is later motivated with a physical example. However, the method proposed in the paper is instead a single layer of autoregressive flow on top of a powerful latent variable model! This is expressed in the title, but only glossed over in the abstract and introduction. The writing has to be updated to coherently focus on the contribution of the paper.

Questions (ordered by decreasing importance)
1. In table 1, quantitative results are reported for the introduced methods. It is shown that introducing autoregressive flows achieves better likelihood and better generalization. However, quantitative comparisons with published methods that were evaluated on these datasets are missing, such as Denton’18 and Kumar’19. A quick calculation shows that Kumar et al. achieves a log-likelihood of -0.43 in Table 1 when converted to this paper’s metric, although it is possible my conversion is incorrect. Is the presented model competitive with previously published results?
2. No qualitative generation results are presented. Since the model achieves a high likelihood it is likely to do well on one-frame prediction, and possibly would even work on autoregressive multi-step prediction. Is the model capable of generation of diverse and plausible video?
3. The paper has a lengthy section 3.1 that convincingly explains that decorrelating latent variables in time is important for sequence modeling. However the proposed approach in fact produces latents that are correlated in time! Since the prior over latent variables is conditioned on past frames, the model can in fact learn a correlated representation and still achieve optimal likelihood. Moreover, the position of both the digit and the robot arm could be seen in what should be the decorrelated image in Fig 4. Is there solid quantitative (or even qualitative) evidence that the model learns a ‘more decorrelated’ representation beyond the fact that it copies the background and that the likelihood improves? The evaluation in this paper does not convince me that the model learns a temporally decorrelated representation.
4. Were modern techniques beyond affine flows considered, such as from Kingma’18, Kumar’19? Two layers of affine flows are likely insufficient to model the complexity of these data, which makes the comparison to the purely flow-based models somewhat unfair.
5. It is stated that the paper is “the first to demonstrate flows across time steps for video data”, however, the related work by Kumar et al. proposes a somewhat similar model in which conditional flows are used to model video data. Do Kumar et al. not “demonstrate flows across time steps”?

Minor comments
1. Eq (10) and (12) seem to be inconsistent. Perhaps x_t = x_t-1 + u_t-1 was meant in eq (10)?
2. Line before eq(14): it not true that u_t-1 = x_t-1 - x_t-2. It would be true if the deterministic x_t = x_t-1 + u_t-1 model was assumed instead of the gaussian N(x_t; x_t-1 + u_t-1, Sigma). It is possible that eq(14) is still correct as the variance of Gaussians is additive.
3. The following work uses autoregressive flows for modeling temporal dynamics and should be cited: Rhinehart’18,19

Rhinehart et al, Deep Imitative Models for Flexible Inference, Planning, and Control
Rhinehart et al, PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings

--------------------- Update 11.19 -----------------------
The newly provided experiments support some of the claims of the paper. In particular, I appreciate the plot showing that the proposed method successfully learns a more decorrelated representation over time, and the provided qualitative samples from the model. The authors also clarified my questions about motivation. At the same time, the proposed method is not shown to compare well to state-of-the-art approaches. I am leaning towards accepting the paper, but I believe the method would have a much larger impact if its properties were more fully exposed.

== comparison with Denton&Fergus'18 (SVG) ==
When trained with beta=1, as the authors suggest for comparison, this method is known to perform poorly. There are two possible ways of alleviating this: 1) to train with the modified objective as in the paper but evaluate the true lower bound on the likelihood, or 2) interpret the beta as the fixed variance of the decoder distribution. Given the results the authors have provided, I believe the latter option will lead to SVG outperforming the proposed approach.

== Correlation plot ==
Thanks for performing this experiment! While measuring correlation only captures linear dependencies, which is likely mostly the background image, this plot shows that the model indeed learns to (linearly) decorrelate the frames in the sequence.

== Samples ==
Thanks for providing samples from the model! While the performance on BAIR is not quite convincing, the MNIST samples look very good.

= Kumar et al. comparison ==
The author's response convinces me that the proposed model is significantly different from Kumar et al. in scope, as Kumar et al simply use a per-frame normalizing flow encoder coupled with a sequential prior.

== eqs. 10, 12 ==
The authors' response cleared my confusion, the equations are correct.

---

**Review 3:**

This paper proposes to model temporal sequences using autoregressive flows across time steps, that allow to model more explicitly temporal changes of the input, i.e. how the input x_t has changed w.r.t x_{<t}. As also stated by the authors, this is a generalization of other work that instead of modelling the input at each time step, models temporal differences between consecutive time steps.
To the best of my knowledge, this is the first work that models normalizing flows in the sequential setting in this way (to be fair however, the idea is fairly obvious).

Overall I found the paper interesting, and I think it is well written, so I am leaning towards acceptance. My biggest concern in the paper is the experimental section that could be improved in several ways:
- the paper misses broader perfoemance comparisons against other state of the art models, in particular videoflow which is quite related to the models introduced in this paper.
- how does the model perform on longer sequences, e.g. for long term generation? I would expect that such a direct dependence of the temporal dynamics on the frames of the video may make it hard for the model to coherently predict future latent states for many time steps.
- What would happen if we used the same trick of modelling the conditional likelihood in this way in other SOTA models?
- what are the computational requirements of the models presented in this paper?

---



---

## Year 2020

### Paper 1 (paper_id: mzfqkPOhVI4)

**Review 1:**

This paper presents a GCN-based solution for multi-step spatio-temporal data forecasting. The main contributions are 1) a spatio-temporal joint graph convolution network to simultaneously capture spatial and temporal correlations; 2) a combination of the sequence decoder and short-term decoder to alleviate the error accumulation when modeling the temporal dependencies. I agree with the incremental contribution of this paper. To verify the effectiveness of their approach, the authors conduct experiments on three real-world datasets.

However, I have the following concerns:

Presentation:
The major concern is the writing of this paper. There are many parts (e.g., notations, statements) that are unreadable or hard to understand. For example,
1) What is “ST in GCM” in Figure 2(a)?
2) In page 4, x \in \mathbb{R}^n. What is n here? It seems to be inconsistent with the notation N in Section 3.1.
3) This paper introduces a learnable spatial mask matrix W_{mask}, but how to use it is not clear.
4) Does Figure2(c) in page 5 mean Figure 2(b)?
5) In the last equation of page 5 (for computing Y_{s_out}), what is t? Moreover, since W_F is of shape F by 1 and W_t is t by M, how can we multiply W_F by W_t?
6) How to obtain Y_{m_out} in equation 7?
7) Many typos and grammatical errors. I only point out several of them. Spatial-temporal data is -> are, correlations STG2Seq -> correlations. STG2Seq, Laplqaacian->Laplacian, which needed -> which is needed, we conjecture the reason is-> we conjecture that….

Technical:
1) Novelty of the proposed model is limited. The proposed spatio-temporal joint graph convolution is very similar to the concept of 3D GCN [1]. However, in this paper, I could not see any discussion about the difference and comparison between them.
2) In section 3.1, this paper assumes that the graph is undirected. However, the propagation of traffic in spatio-temporal domains are certainly directed (e.g., downstream, upstream).
3) What is the difference between the temporal attention in Section 3.4 and an FFN (two fully-connected (FC) layers)? In my view, this paper only uses two matrices for the dimension transformation, which is the same as two FC layers.

Experiments:
1) It is more reasonable to compare the proposed model with CNN-based solutions in the Mobile traffic dataset, as there is no explicit graph structure (only Euclidean structure exists). Baselines like ST-ResNet, ConvLSTM, DeepSTN+ or DeepLGR should be included for comparison.
2) According to the paper, I cannot see any detail of the multi-run experiments (e.g., in Table 1 and Figure 3). How about the stability (i.e., variance) of the proposed method?
3) The experiment shows little improvement to the existing approaches in the first two datasets. There should be significant test (e.g., student t-test) conducted to make sure the experimental result is reliable. Considering the limited improvement, it is also questionable whether adding such complexity to the model is worthy. It may not make sense to improve the MAE by 0.02 in the first dataset by increasing the runtime a lot.
4) No experiments to show the effects of the spatio-temporal kernels. Why should we set it 3x1, 1x3 and so on?
5) It seems that the MAPE in Table 1 largely exceeds 100%, which is unusual to see. What is the unit of MAPE here?

Minor issues:
1) All acronyms should be expanded for the first time in text (e.g., HA is not expanded. What is HA? Historical average?).
2) There should be one space before each citation. For example, DCRNN(Li et al., 2017) -> DCRNN (Li et al., 2017).
3) In the first paragraph of Section 1, the paper claims that “Typical applications include …, traffic road condition forecast (…, Liang et al., 2018), … and geo-sensory time series prediction (Li et al., 2017).” However, Liang et al., 2018 is for geo-sensory time series modeling while Li et al., 2017 is for traffic prediction. Please carefully read these papers before citing them.
4) It is difficult for those who are not familiar with the Inception Network to understand how your model draws insights from it. In addition, the paper proposing the Inception Network should be cited.
5) No future work discussed in this paper.

In summary, I recommend not to accept this paper in its present form.

Reference:
[1] Yu et al., 3D Graph Convolutional Networks with Temporal Graphs: A Spatial Information
Free Framework For Traffic Forecasting, Arxiv 2019.

---

**Review 2:**

This paper proposes a spatial-temporal graph neural network, which is designed to adaptively capture the complex spatial-temporal dependency. Further, the authors design a spatial-temporal attention module, which aims to capture multi-scale correlations. For multi-step prediction instead of one-step prediction, they further propose the sequence transform block to solve the problem of error accumulations. The authors conducted experiments on three real-world datasets (traffic on highways and mobile traffic), which shows their method achieves the best performance.

Overall, I think the problem of capturing spatial-temporal dependency shown in Figure 1 is interesting. But the description of the method is unclear and the writing of this paper still needs improvements. Furthermore, I didn't find enough novelty that meets the standard of ICLR.

Concerns:

Methods:

I don't understand why the authors introduce spectral graph convolution networks while it seems that the authors only use GCN in the spatial domain.

The equation (4) is confusing: $X$ in the left changes to be $T_K(L)X$ without more explanations.

It's unclear about how to use the mask matrix $W_{mask}$ in the proposed model.

The equation $C_{gl} = \sum_{i=1}^T \sum_{j=1}^K C_{out}$ is confusing.

There are many method descriptions like the above examples, which the authors should describe more clearly.

Typos:

Page 3 Last Line: Laplqaacian

Page 5 Paragraph 2 Line 12: node $v_i$as

Page 8 Line 4: performed respectivelywithout

Page 8 Line 17: both of them..

---

**Review 3:**

Summary:

The authors propose ASTI-GCN to solve the multi-step spatial-temporal data forecasting problem. The model uses a convolutional block to model the spatial-temporal correlations and an inception attention based module to capture the graph heterogeneity. They evaluate the proposed method on three different traffic prediction datasets.


Pros:
1. The problem of traffic prediction is important.

Cons:
1. The contributions are limited in this paper. The ideas of jointly model spatial-temporal information via convolutional layer (see 3D GNN to model irregular regions [1] and 3D CNN to model regular regions [2]), and multi-scale spatial-temporal modeling (see [3, 4]) are not new. These papers have been released for more than one year. Especially, the multi-scale motivations in [3] [4] are almost the same in the paper. The only difference is that this paper involves attention to weight different scales, which is also a very common practice. Thus, I think the contributions are not enough to be accepted by top machine learning conference.

2. The experiments could be improved.

- Besides the ablation studies in this paper (add modules to the base model), it would be more convincing to add the ablation studies by removing some components. Since only combining one module (results in Table 2) performs worse than AGCRN, removing some components and keep the rest could provide deeper analysis. In addition, it would be more convincing to conduct all the ablation studies on all three datasets.

- To support the claim of region heterogeneity, it would be more interesting to show some case studies to verify the motivation and see the reasons for the improvement. Otherwise, the improvement may come from the increasing of the number of parameters.

- It would be better to show the error bar for each result since the improvement in some datasets is limited (e.g., PeMS04).

3. Some figures could be improved. For example, some arrows in Figure 2(b) are broken.

[1] Yu, Bing, et al. "3d graph convolutional networks with temporal graphs: A spatial information free framework for traffic forecasting." arXiv preprint arXiv:1903.00919 (2019).

[2] Chen, Cen, et al. "Exploiting spatio-temporal correlations with multiple 3d convolutional neural networks for citywide vehicle flow prediction." 2018 IEEE international conference on data mining (ICDM). IEEE, 2018.

[3] Geng, Xu, et al. "Spatiotemporal multi-graph convolution network for ride-hailing demand forecasting." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.

[4] Cui, Zhiyong, et al. "Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting." IEEE Transactions on Intelligent Transportation Systems (2019).

---

**Review 4:**

This paper investigates the important problem of spatial-temporal forecasting, and proposes a multi-scale spatial-temporal joint graph convolution that jointly model the heterogeneous spatial-temporal correlations. Empirical results on multiple real-world datasets shows promising results.

The draft seems to be written in a rush, with mistake/typos in paragraphs, figures and tables. Besides, the some of the major claims are not well supported by the empirical results. Here are details about the main concerns:

D1: Some major claims are not well supported by the experimental results.
- The main novelty of this paper the is  the spatial temporal joint convolution. Yet, its design needs more theoretical and empirically justifications.  The idea of the concatenating K order of Laplacian matrix is a bit ad-hoc lacking of theoretical justification.  Besides, the spatial temporal joint graph convolution is not necessary "more joint" than baseline algorithm like DCRNN which incorporates the graph convolution operation into the each sub-step of the RNN operation.
- Besides, the claimed convolution in the spectral domain is actually operations in the spatial domain since the final form of the convolution is essentially a polynomial of the Laplacian matrix without involved the transformation into the spectral domain.
- Limited ablation studies are conducted to show its effectiveness. There are some ablation study in Table 2, but not enough to well support the claims.  For example, what is performance gain by simplify replacing the graph convolution with the proposed spatial-temporal joint graph convolution? Is inception style convolution is actually needed? What is the different with and without it?
- Besides, as shown in Table 2, even after the adding the spatiotemporal convolution and the inception attention mechanism, the performance is still worse than some baselines, including DCRNN and AGCRN. Potentially it is also possible that the mask (which can be applied to other baselines) actually contributes to the major improvement.

D2: Presentation. This paper seems to be written in a rush, and the presentation need more polish.

- The technical part is a bit hard to follow. For example, when explaining the spatial-temporal joint convolution, it might be easier to understand if the author provides an additional figure about it, e.g., a 2-D matrix with size T x K, with filters of size K_t and K_s applied on it, where graph convolution is used for the feature extraction.
- Inconsistent name of the proposed model. The name of the proposed model is not consistent, for example it is called ASTI-GCN in abstract, while in Figure 2a it is called ATI-GCN. Yet, in Section 4 is is call ASTIGCN, and in Table 2 it is also called ours.
- Errors/typos in table and paragraphs. For example, in Table 1 the RMAE should RMSE. The Laplqaacian in Section 3.2 should be Laplacian. - In Equation (8), we may rewrite \hat{Y}_i as \hat{Y}(\Theta)_i so the loss is a function of the parameter \Theta.  In Section 4.2, we may Capitalize the first letter of alternative to make it consistent with the other three. Also remove the duplicate periods at the end of the sentence.
- In Equation (2), the T_k(L) probably represents the order k instead of order k-1 otherwise T_0(L) will become order -1.
- The datasets use, i.e., PeMS04, PeMS08, seems to be PeMSD4 and PeMSD8, we may keep it consistent with previous methods to facilitate comparison of metrics in baseline papers.
-  In the Table 2, it is mentioned that each setting has been run for 10 time, and it is helpful to also include the standard deviation to show the statistical significance of the improvement

---


### Paper 2 (paper_id: whE31dn74cL)

**Review 1:**

This paper proposed a general deep learning method with temporal-kernel for continuous time series modeling.

The proposed method is technically sound and solid. The decomposition of the neural and temporal kernel brings together the kernel methods and deep learning, which delivers a general and fundamental solution to time series, especially the irregularly sampled ones or those with missing values.
In brief, this work may demonstrate a promising way of handling such problems and inspires and encourages other research in this direction.

The writing is thorough and clear. Though I did not check all proofs and the supplementary, the descriptions and arguments in the paper are properly delivered.

The proposed model consistently outperforms RNN, TCN, and attention baselines on a variety of datasets. The settings of the case 2/3 are reasonable.
Besides, it is interesting to see that the speed is still comparable to the baselines.

However, in my opinion, more baseline comparisons need to be added. I did not quite buy the claim that the proposed method is not compared with recurrent neural ODE-type models and point process models because of its more generalization and flexibility.

Minor typos:
In page 3: infitnitesimal -> infinitesimal; covaraince -> covariance
The font size in figures may be increased for better readability.

---

**Review 2:**

The ms introduces a time component in the traditional NN setup, where the hidden layers change dynamically according to time. The idea is to borrow strength from the newly introduced time dimension to improve prediction performance.

One of the key idea is to treat each hidden layer in NN as a Gaussian process, which is represented  as “neural network kernel”. Functions drawn from this Gaussian process at different time points are assumed to follow a continuous-time system, which is actually a ODE. For some reason it is difficult to use the continuous-time system to compute the temporal kernel directly in the time domain. The ms proposes to convert the functions to the frequency domain, which leads to a nice property such that we can compute a temporal kernel (Eq. (2)) in frequency/spectral domain. Hidden layer of NN at different time points can be seen as a large Gaussian process, whose kernel could be composed by the aforementioned NN kernel and temporal kernel (Eq. (5)). This decoupling is the key of this ms.

Regarding the computation of the kernels, NN kernel can be computed by extracting the features of the hidden layer. The temporal kernel can be computed by sampling the spectral distribution, which is called the random feature representation (Eq. (7)). However, it is not clear how to specify the spectral distribution. In all examples, Normal distribution is used, which is OK but may not be able to capture the complexity of ODE.

The ms applies the proposed method to real dataset and achieve better performance than baseline methods in prediction tasks involving irregular time points setup.

In general I feel this is a nice paper. The idea of learning NN and time dynamics at the same time seems to be useful in many applications. The ms cleverly decouples the learning of NN kernel and temporal kernel in two independent modules, which can maximumly utilise current implementations.

However, due to my limited knowledge in signal processing, I am not able to dig into the mathematical details and make strong recommendations (especially Claim 2).

Some minor comments
1. What is the purpose of Claim 1? From the supplementary it just shows that f(t) and f[i] are not equal, but they may be very close and does not have a huge impact of the result. There are lots of approximations in other parts of the model.
2. Why f(t) needs to be ODE ?
3. Page 3, section 3, line 2, in the formula of f(iw), the second derivation term seems to be missing
4. Page 4, 5 lines after Eq. (3), a_2(x)!=0 => a_1(x)!=0
5. Page 5, Eq. 7, cos(tw_n) => cos(tw_m)

---

**Review 3:**

This article proposes a methodology to *adapt* NNs to continuous-time data though the use of a (temporal) reproducing kernel. I enjoyed reading the paper, the message is clear, illustrative and the connection with other existing works is to the point. Although I am unfortunately unable to assess the theoretical novelty of the paper (I am unaware of the details of the state of the art in the subject) the contribution of the paper relates to the study of a kernel, given by an ODE, attached to the input of the NN. This kernel is also represented using Fourier feature expansions.

Though the paper heavily relies on well-known concepts  (standard NN, GPs, Fourier features), I see that is has a contribution.

I suggest the following amendments:
-for some readers, the general proposed architecture might be confusing. Perhaps a diagrams (similar to that in Fig A.1) would be useful in the first pages of the paper. How does the kernel turn the continuous-time data into NN-ready?
-much useful material is relegated to the appendix, if key results, scope and more are only in the appendix, they might not receive the deserved attention.
-please better clarify how different your work is from the existing literature: NTK, deep kernel learning, neural ODEs, etc

---

**Review 4:**

##### Post-rebuttal update

I've read the rebuttal and updated my score.

---------------

This paper proposes a deep learning model for incorporating temporal information by composing the NN-GP kernel and a temporal stationary kernel through a product. The temporal stationary kernel is represented using its spectral density, which is parameterized by an invertible neural network model. This kernel will be learned from the training data to characterize its temporal dynamics.

##### Originality & Significance
The modeling approach taken in this paper is original to my best knowledge. Although it is well-known that second-order stationary processes has a SDE correspondence, it is rare that this property is connected to NN as GPs and this work finds an application where such ideas can be potentially useful. However, I find it difficult to say anything about significance of this idea since it is not very clearly described. I encourage the authors to make a substantial revision to clarify the issues listed below.

##### Clarity
The clarity is low. Although the motivation and the high-level idea is clear, I find it very difficult to understand the actual approach taken by this work. There is no description of the actual algorithm and I can see many algorithmic and computational issues left without discussion:
* How is prediction made at a specific (t, x)? Do you use a GP predictive mean conditioned on the training points?
* If the prediction is made by GPs, how do you solve the scalability issue? When the training set is large, do you take a sparse approach? The temporal kernel is defined through a random feature representation, do you take advantage of it for fast computation?
* or you just take a weight-space approach and compose the features (take pairwise product of the features of k_T and \Sigma to form the new features)?
* Is NN-GP or NTK kernels used to compute the kernels? How do you compute them? Do you use a Monte-Carlo estimate or the closed-form (computed through a recursion)?
I will be happy to raise the score if these questions are properly addressed.

##### Strengths
* The modeling approach is novel.
* The proposed method consistently outperforms other baselines in handling irregular continuous-time data.

##### Weaknesses
* The method used is not clearly described.
* The non-stationary extension to Bochner's theorem is a known result.
* Although the performance is shown to outperform other NN-based approaches in experiments, there might be scalability issues to apply the approach to larger-scale problems with long sequences (assume a non-weight-space approach).

##### Minor
P16: A.4.1: "We the Fourier transformation transformation on both sides"?

---


### Paper 3 (paper_id: 1NRMmEUyXMu)

**Review 1:**

This paper generalizes the previous graph-based planning algorithm for goal-conditioned RL algorithms by learning a latent metric space and building the graph by clustering. The experiments show that their approach outperforms both the HER and previous MSS/SORB baselines.

Strong points:
1. The paper achieves good performance on the challenging AntMaze and PickAndPlace environments.
2. The clustering methods to find latent landmarks are novel in this setting.

Weak points:
1. The novelty of the paper is limited. Planning on the graph or learning a latent distance embedding for planning are all existing ideas. The combination is straightforward. The robustness of soft-min is well known.
2. I am confused about why the authors' approach is better than MSS or SORB. It's not obvious that a distance-based clustering will perform significantly better than sampled landmarks (like farthest point sampling). I guess the reason is that clustering helps to avoid the landmarks near the wall (which often appears in the FP sampled methods). Those states are challenging as it has a higher possibility for the ant to collide with the wall. Can the author give some words about this? Moreover, the paper is not clear about the contributions of each component in the improvements. For example, no table or figure shows the benefits of the soft value iteration over the hard version. The author shall provide more ablation studies and explanations.
3. Although the paper presents the overall idea well, there is still a lot of room for improvement in writing. For example, I suggest that the author replaces the hyphen with the comma in the abstract.

Based on the current presentation, the lack of ablation study, and the limited novelty, I think this paper is not good enough to be accepted. 

Here are some questions:
1. Can we build graphs by sampling goal states and cluster them based on the metric space defined by the Q networks? What's the performance of this approach? Will it be worse? Why?
2. Can a sample-based approach work in non-navigation environments? Motion planning algorithms can solve the high-level part of both pick&place or ant maze problem once the geometric model is known. If so, why do we need the expensive RL algorithm?


---

After author response：

The authors have improved the presentation and added the necessary ablation studies. I appreciate the authors' effort. I am glad to raise the score to reflect the changes. I hope this work will inspire future research on hierarchical planning algorithms.

---

**Review 2:**

This paper proposes an approach for automatically learning state abstraction on a RL problem, which can then be used for temporally extended planning using a search algorithm. The main contribution is introducing the concept of latent *landmarks*, a clustering of low dimensional state embeddings. Landmarks are defined on a latent space wherein the distance between two latents code is small if their corresponding high-dimensional states can reach each other in few environment steps. The paper proposes to cluster latent states, so that each cluster contains states that are easy to reach from each other; the landmarks correspond to the centers of these clusters. An algorithm for automatically learning this clustering is introduced in the paper.  With landmarks in hand, the paper proposes a soft-value iteration method to compute shortest path distances to the problem's goal. The approach is evaluated in a variety of domains, and compares favorably with recent state-of-the-art methods.

In general, I found the paper interesting and the key ideas intuitive and sensible. The paper is reasonably well-written, but there are some important points that are unclear (see below). The experimental section compares with very recent algorithms for planning over learned value functions (SORB and MSS), and the results on five continuous control task show substantial improvement over these methods.

In terms of clarification, I have a few questions for the authors:
- With regards to the embedding used and $\Psi$:
  - The embedding operates over goals. Are these the goals of the problem? For example, in the AntMaze problem (Fig. 4), is $G = \{ \textit{red-square} \} $? If so, doesn't this mean that  $\forall s \Psi(s) = \textit{red-square}$, and thus $V(\Psi(s), \Psi(s')) = 0$ for all pairs of states?
  - More reasonably, $V$ could be defined over states, so that $V(s_1, s_2)$ indicates the number of steps between $s_1$ and $s_2$. This can be achieved under the current formulation with $G = S$ and $\Psi(s) = s$. Then $D(s_t, a, \Psi(s_t))$ represents the number of steps between $s_{t+1}$ and $s_t$. Is this the case for most experiments? No details of the $\Psi$ function used in the experiments was given, so I find this point somewhat confusing.
  - If this is not the case, then I think it's important to explain where does this mapping function comes from, since it essentially provides some reward shaping for the problem.

- Can you explain what is the motivation for using Algorithm 2? A common approach is to use Djikstra on the graph representation. Is there a reason why this can't be done (or is worse) in this case?

- I don't fully understand Algorithm 1 to create the batch for Eq. (5). Can you explain this algorithm in more details? For example, the definition of $\texttt{dist}$ is ambiguous; is it pairwise distance between all sampled goals? distance between $g_1$ and all the others? something else? In general, I'm having quite a hard time figuring out what this is doing.

In light of above, I think the clarity of the paper can be improved in many places. But, overall, I'm positive towards this work and I think it is a nice contribution, particularly since I'm not aware of other work creating explicit low-dimensional landmarks to be used for search. That being said, one exception that is missing from the literature review is [1], which creates a discrete latent representation of the environment and solves it using prioritized sweeping.

[1] Corneil, Dane, Wulfram Gerstner, and Johanni Brea. "Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation." International Conference on Machine Learning. 2018.

---

**Review 3:**

#### Summary:
This paper presents a method for learning a sparse set of latent subgoal states during training. Using a goal-conditioned policy and the latent states, a simple planning algorithm that performs soft value iteration between the subgoal states is proposed to facilitate within-dataset generalization. The proposed method outperforms competing approaches on multiple simulated navigation and robotics tasks.

##### Pros:
- The results seem strong.
- The method is straightforward and nice, and it seems to work more efficiently than previous methods.

##### Cons:
- Overall, the paper is incomplete. It’s very difficult to understand the whole system, how training proceeds, what the hyperparameters are, etc. There is very little detail about how this all works. The specific components of the latent landmark learning and the planning are mostly fleshed out (albeit with some missing details still) but the rest of the setup is mostly not described. How are the losses combined? Are there multiple learners or does the DDPG agent also do the latent space learning? How is training done? What are the hyperparameters used?
- The generalization claims are a bit oversold as the generalization achieved is weak. In particular, the method requires that the test goal distribution have sufficient overlap with the train goal distribution for latent landmarks to be created near the desired test goals.
- Are the experiment domains created here or existing work? If the former, more details are required. If the latter, citations are required.
- Given the multiple changes with respect to prior work, additional ablations and experiments are needed to better understand why this method is performing well, and what contributions are important.

#### Decision:
Overall, I find it too hard to know exactly what is happening in this work. It would be very difficult, if not impossible, to reproduce this work from this paper alone. Without sufficient details about the system, it’s hard to evaluate, which means that I default to a reject, especially in conjunction with the other weaknesses listed above.

#### Questions:
1. What is the goal space G? Is it the state space? This should be defined.
2. What exactly is \Psi? Is it learned or is it given? This should be explained better.
3. How is the autoencoder for the latent space learned within the overall system?
4. The connection between D and Q is poorly explained. From the text, it reads as if you get Q from D. However, it makes more sense if the DDPG agent estimates Q and then you use eq(3) to compute D from Q. Which is it?
5. Are both V and D really necessary? It seems like they encode the same thing, essentially.
6. Why do you use soft value iteration? Is it a good choice? Is there a reason for this choice? Would other choices be better or worse? It seems likely that the need for the dist_max penalty is due to the use of the soft value iteration with a poorly set soft value iteration temperature. Is this true?
7. Are the domains from Duan et al 2016 or some other existing work? Or did you create them from scratch (unlikely). Why are they labeled “-hard”? What is hard about them? These need to be cited and described. You have tons of space in the appendix to use freely.
8. In Figure 3, is the x axis training timesteps?
9. Figure 5 caption says the agent has learned landmarks that help avoid collision with the box. Where are these? Is there a figure? Any sort of information on these? Wouldn’t any landmark away from the box accomplish this?
10. For Figure 6, why is choosing the maximum distance bound important? Why would the planning algorithm choose subgoals that are near the max distance bound given that the planner is finding the argmin subgoal? It seems likely that this hacky distance bound is unnecessary.
11. The “planning algorithm” seems trivial. Am I missing something? Given the latent landmarks and the estimates for $d_{c \rightarrow g}$, the planner just gets the next estimated subgoal and the distance to it, executes the goal-conditioned policy for that subgoal for that many steps, crosses out that subgoal, and then goes to the next subgoal. Is it non-trivial because $d_{s \rightarrow c}$ gets updated after each step (not included in the pseudocode)?
12. Is the GLS algorithm a contribution of this work or something from a previous work? Again, it's neither described sufficiently nor cited sufficiently.

#### Comments:
- The introduction uses unnecessary hyperbole and insufficient citations to make its points. It comes off as less compelling as a result.
- Section 3 is called Background but really it contains the preliminaries and definitions of this work. It should not be called Background.
- Should the variance vector of the centroids appear in equation (5)? It is mentioned once and then never discussed again. What values of it are learned? Is it important?
- Similarly, the temperature for the soft value iteration is mentioned once and then never discussed again. What role does it play? Is it important? Would tuning this better remove the need for the dist_max penalty?
- The notation $f_D(c_i)$ is unnecessarily complicated. These values correspond to states, no? So define them as $s_{c_i}$ or something similar to make the math and text more clear. As it is, equation (6) is unnecessarily cluttered.
- I assume that the argmax in the subgoal computation at the end of section 4 should be an argmax over c?
- There is a typo in the Figure 3 caption (PointmMaze). Further, the text in the subfigures is illegibly small.
- Break lines 11 and 13 of algorithm 2 into two lines each.


*****************
After author response:

I appreciate the updates you've made to the paper to better flesh it out, including the diagrams, pseudocode, and additional ablations. I've increased my score accordingly. Regarding generalization, I fully agree that it can be difficult to show. That said, when it's advertised in the title of the paper, I expect it to be clearly shown in the paper itself. It seems that the language has been greatly toned down in the updated version. However, without entirely re-reviewing the paper, I am unable to fully recommend acceptance.

Aside: I would have appreciated responses to my questions directly so that I don't need to dig through the rewritten paper to find the answers to my questions.

---

**Review 4:**

This paper approaches long horizon planning by learning a sparse graphical representation. The proposed algorithm, L3P, proceeds by learning a latent space which enforces a distance measure, where this distance is learned to mimic the number of steps between states via a goal conditioned Q-function. A clustering algorithm is then used to represent this latent space through only a small, efficient set of latent landmarks. These landmarks are then connected if nearby via the distance, the estimate of which is refined via soft value iterations over the graph. L3P is demonstrated on a number of environments to be both data efficient and high performing compared to baselines.

The paper is clear and well written. The topic of graphical representations of state spaces to plan long horizons over via local predictions holds significant promise, as does sparsifying this representation. The latent space construction and sparsification procedures are reasonable, particularly tying the distance to the policy Q-function as in SoRB. A few notes on clarity:
- The algorithm should be moved to the main body of the paper. (to reduce space, potentially remove a row of images in Fig. 4, stretch figure 1 to use the full width)
- A video would be helpful of the full process, including the soft value iterations.
- What algorithmic parameters (e.g., latent space dimensionality, network sizes, training data and number of states from which the latent space was constructed) were used?
- How are L_rec and L_latent traded off?
- The tasks, e.g., Fig 2 and 5, should be shown before the results in Fig. 3.
- More intuition on the soft value iteration would be helpful.

The results on the shown tasks show clear benefits of L3P, particularly on the tasks where HER has most difficulty (likely the more long range tasks) L3P outperforms substantially. However, the results are somewhat lacking (1) on ablation, (2) demonstrating new environment generalization, and (3) showing significantly complex tasks.
1) The authors show ablation for number of landmarks and d_max, but not for algorithmic changes like the soft value iteration. How much does this procedure help versus the use of latent marks to sparsify the space? It would also be interesting (though not fully necessary) to see comparison to Savinov 2018a (SPTM), which uses a learned distance predictor rather than the Q-function as in SoRB and L3P.
2) It is not clear how this method would generalize to new environments, e.g., Section 5.5 in SoRB. This to me is key (particularly with generalization in the title, which I believe currently only refers to task length) and may be a challenge for L3P due to finding landmarks from limited coverage of a new scene.
3) L3P should be shown on higher dimensional and longer horizon problems, as the maze is fairly short and simple , as is the box task (as can be seen by HER’s performance). These do not push the boundaries of the algorithm. Higher dimensional problems may particularly challenge the landmark learning (e.g., if the landmarks must be learned too in a higher dimensional latent space)


__________

After author response:
I appreciate the author’s response. Previous topics:
1) The author’s add ablations on the hard vs. soft min during the graph search, the additional results are informative, but not conclusive. Given that the overall performance is similar, the authors need to demonstrate the soft-min’s benefits for each experiment and over more training seeds.
2) For generalization, the author’s confirm that this method is unable to generalize to new environments, though clearly it has other benefits in terms of data efficiency and robustness of solutions. I believe these are still important benefits, though it would be useful to discuss how generalization may be achieved.
3) For harder experiments, the authors note that the baselines perform poorly there and thus these tasks were not considered. Though reasonable that the baselines are unable to perform in such cases, harder experiments would show the limit of the proposed algorithm. It would be useful to see for instance how well it scales with dimensionality, how quickly the success rate falls off.

New comments:
a) I believe the title change away from Generalization is an improvement, though the algorithm name "WORLD MODEL AS A GRAPH" seems to not capture the novel aspects of this work. This name I believe would be more readily applied to search on the replay buffer or semi-parametric topological memory.
b) R2's point that much of the robustness may be a factor of choosing states further from the wall is an interesting one. It would be interesting to examine exactly *why* the method is robust.
Overall, I believe the paper is interesting and proposes some novel ideas that have benefit, it requires more thorough analysis, and thus I am leaving my score unchanged.

---


### Paper 4 (paper_id: t0TaKv0Gx6Z)

**Review 1:**

The paper proposes a new discrepancy between probability distributions called the sliced Kernelized Stein Discrepancy.  It is based on the idea of computing the standard KSD on random 1 dimensional projections and then average those. Some other proposed variants are based on optimizing over the projections.

The motivation behind this new discrepancy is to overcome the 'curse of dimensionality' that standard KSD suffers from. Thus the main selling point is that the sliced version has a much better behavior as the dimension increases.

The authors then propose to use the new divergence for two applications: goodness of fit testing and for models learning.

The robustness to increasing dimensions is illustrated through multiple experiments on both tasks and yields convincing results.

Strength:
	- The proposed framework is neat and the experiments are thorough and convincing with many additional discussions and results in the appendix.
	- The proposed method is relatively easy to implement and seems to address several known limitations of KSG and in SVGD: the scaling with dimensions.



Weaknesses:
 The paper is a bit dense with many references to the appendix. However the main idea is clearly explained  and the advantage is clear both in terms of theory and throughout the experiments.


Questions:
	- In 4.1.1, the distributions p and q although high dimensional, they often have independent components. This might be very advantageous for the sliced version of the algorithm, especially when using a set of orthogonal projections for the projections $r$. What happens to the Null rejection rate when more dependence between the dimensions is introduced ? What is the exact parameter choice for the multivariate-t distribution, I couldn't find this in the appendix?

 Minor remarks.
 - A discussion on the limitations of existing methods KSD and SVGD could be useful as a transition from section 2 to 3 to motivate the slicing.
- Figure 1 is a bit dense especially with all the equations
- The subscript notation $f_{rg}$ is sometimes confusing as f depends on $r$ and $g$ only implicitly after optimizing the objective in 5. It might be worth either mentioning where this dependence comes from or even remove it.
- In the paragraph right after corollary 3.1. The authors mention a limitation of a particular version of Max Sliced KSD over the other but then refer to appendix F without really saying what this limitation is. It might be worth saying a little bit more about those limitations.

---

**Review 2:**

This paper tries to solve the curse-of-dimensionality problem of KSD and corresponding mode-collapse problem of SVGD by projecting both the input and output of test function onto 1D slices. By doing so, the paper proposes the new discrepancies called SSD and maxSKSD, and a new variant of SVGD called S-SVGD. Experiments on goodness-of-fit test (synthetic high-dim Gaussian & RBM) and model learning (ICA on synthetic data & amortized SVGD on MNIST) are reported in the main body of the paper.

This paper is well motivated and the writing is good. Curse-of-dimensionality problem is common in kernel based methods like KSD and corresponding SVGD. To the best of my knowledge, the idea of 'slicing' is novel to solve the high dimensional problem of KSD and SVGD. However, the derivation and analysis (e.g., computational complexity, whether closed form or not, how to get it in each experiment, etc...) of the optimal test direction $g_r$ are missed in the main body of the paper, which is critical to evaluate the quality of the proposed maxSKSD and S-SVGD according to my understanding.

Pros:
1. The 'slicing' idea is novel to KSD and SVGD;
2. The paper is easy to follow in general, though some minor points need further improvement;
3. Experiments are conducted on various tasks and datasets, which provides a thorough comparison.

Cons:
1. Lack of analysis of the optimal test direction $g_r$. According to Eq. (6), it seems that $g_r \in R^D$ has no closed-form solution and corresponding $G \in R^{D\times D}$ in general.This raises several questions:

     1.1 In the high dimensional case (D >> 1),  $D\times D$ seems unacceptable. Is it possible to provide the empirical storage consumption of both S-SVGD and SVGD in experiments, especially for neural networks?

     1.2 Getting $g_r$ requires solving a maximization problem (see Algorithm 2 in the appendix), which introduces an additional inner loop for each iteration of S-SVGD. So what is the time complexity? Is it possible to provide the comparison of wall clock time of S-SVGD and SVGD in experiments?

     1.3 Does $g_r$ have closed-form solution in some special case, e.g., Gaussian?

2. The notation $f$ is used for denoting both $R^D\to R^D$ and $R^D \to R$ mapping, which is misleading. $f(\cdot;r,g):R^D \to R$ and $f_{rg}:R\to R$ are even more confusing. Besides, it is a little hard to understand how Eq. (5) can be derived based on Eq. (2).

---

**Review 3:**

##  Summary of the paper
The authors proposed the `sliced version of the kernelized stein discrepancy and solved the collapsed problem for high dimensional GOF and particle based model learning.

## Strong and weak points of the paper
### Strong points
- Provided a novel slicing technique for KSD and provided its statistical estimator based on U-static.
- Experimental results supports that the proposed sliced KSD is very promising approach for high dimensional models.

### Weak points (Questions)
- I think the presentation should be modified so that the reader can easily understand the proposed methods. At least, the main algorithms should be presented in the main paper. For example, the algorithm for GOF test is not shown in the main paper but included in Appendix E, although it is the main algorithm of this work.
- I could not understand when I should use MAXSKSD-G, not MAXSKSD-RG although detailed discussion is shown in Appendix F. To me, based on the discussion of Appendix F, MAXSKSD-RG seems always be better than MAXSKSD-G. So don't we need MAXSKSD-G?
- (I might overlooked, but) the strategy on how to choose optimal G for GOF test is not shown in the main paper although it is expained in Appendix G. The explanation of how to tune G should be included in the main paper.

## Rating
- Clarity: For me, the main paper is not enough to understand the paper and the proposed method clearly. I needed to read the Appendix carefully.
- Correctness: I did not check the proofs of each lemma in Appendix B.
- Novelty: The idea seems very interesting and important in the community.

## Comments and Questions
- Comments) I think, instead of Figure 1,  Appendix B.1 should be included in the main paper, which is very helpful to understand the overall strategy of the proposed discrepancy.

- Minor comments )
I think Eq numbers in Figure 1 Left seems wrong, especially for SSD and max SSD.
On page 17, appendix  B.2, the sentence above Eq.29 says, ``"~~~ as in Theorem 3~~~", I think this is a typo and should be replaced with "Lemma 3".

---


### Paper 5 (paper_id: JBAa9we1AL)

**Review 1:**

The authors presented in the submission a thorough study on enforcing the aggregated individual fairness with non-differentiable ML models. The proposed method generates individually fair and robust ML models in a minimax fashion among all possible samples that are close to the true distribution w.r.t. a given fair metric. They introduce the augmented support and transfer the standard gradient descent to a gradient descent in the functional space anchored by the augmented support to optimize the adversarial risk function with non-smooth ML models, e.g. decision trees. Solid theoretical guarantees and convincing empirical study results are provided to support their claims. The paper is highly completed, well-structured (though a bit dense given the page limit) and well-written - a clear accept.

The only major issue in my opinion is that the submission lacks the formal definition and discussion regarding "individual fairness", which is defined on the second page as "f(x_1) \approx f(x_2) if d_x(x_1, x_2) is small" for a classifier "f: X \to {0,1}". This \approx is unclear under this binary classification setting, and the definition leaves me with the impression that it means two close examples by the fair metric should have similar predictions.

However such understanding seems problematic and inaccurate throughout the rest of the paper. Decision trees are almost certain to have a value jump near the boundary of any node, therefore any quantification of the definition (e.g. I am thinking of a \epsilon-\delta argument that |f(x_1)-f(x_2)| < \delta when d_x(x_1, x_2) < \epsilon for fixed \epsilon and \delta) above has to deal with the case when d_x(x_1, x_2) is small but x_1 and x_2 are in two nodes in any single tree in the GBDT ensemble. Also, the restricted empirical adversarial cost function (2.4) appears more like a robust cost function that guarantees the model accuracy for all possible data generating process similar to the truth (which is the d_x part), whereas it has no reference to "f(x_1) \approx f(x_2)".

After (2.4) I feel the individual fairness in the paper might actually mean that "two close examples by the fair metric should be equally accurate", which seems to make more sense given the transportation between similar examples and the risk being the sole optimization target. If this is the case, it might be worth pointing it out at the beginning of the section.

Other than that, it is a very smooth experience of reading the paper.

Minor and editorial issues:

1.  Though stated by the authors that "our method readily extends to other supervised learning setting", the theoretical discussion and the empirical study covered in the paper are both based on solving a binary classification problem.

2. Assumption A.1 requires the loss $l$ is convex, while Assumption 3.2 requires $l$ to be bounded. These two points together make the loss constant unless, e.g., f(x) is uniformly bounded for all f \in \mathcal{F}. In such case, to achieve convergence and generalizability simultaneously, GBDTs might need to be bounded, which undermines Assumption A.2. The interaction between the assumptions seems non trivial and is worth its own discussion.

3. Page 14, Proof of Theorem, "Under Assumptions A.1(iii)-(v)". Assumptions A.1 (iii) - (v) seem missing?

---

**Review 2:**

### POST-REVISION

Thanks for the revisions made to the paper, particularly for elaborating on the heuristic to speed-up the inner subroutine, and the clarification on the generalization bound.

Also thanks for the running time numbers. Would be great if you could report them in the paper (if you haven't already).

I'm revising my score for the paper to 7.
**********

The paper presents an interesting idea to train boosted decision trees to satisfy individual fairness constraints (for a pre-specified similarity metric "d_x"). The prior gradient-based method of Yurochkin et al. for individual fairness constraints does not apply to non-continuous (or non-smooth) models such as decision trees. The authors borrow the same distributionally robust loss setup as Yurochkin et al, where the goal is to minimize classification performance over all distributions that are close to empirical distribution, with the closeness measured in terms of the similarity metric "d_x" (via the optimal transport distance). However, unlike the prior method which works with the dual formulation, the authors show how one solve the primal problem directly with functional gradient descent as long as the "supremum" over distributions in robust loss function can be computed efficiently. They then show how the supremum can be computed using an LP. Interestingly, by directly computing the supremum, the resulting gradient boosted training does not require the model to be continuous in its inputs. The authors provide generalization bounds and experiments on three benchmark fairness datasets.

Pros:
- Training individually fair models is still a nascent area of research, and so the paper makes a valuable contribution to this subarea of ML fairness.
- The trick the authors use to accommodate non-continuous models by directly applying functional gradient descent to the distributionally robust loss function is pretty neat.

Cons:
- The main concern is the scalability of the approach. Each iteration of the gradient boosted training requires solving an LP with n^2 variables, where "n" is the number of training points. The authors provide a solver for the LP (based on some practical tricks) which requires O(n^2) computation, but even this does not seem scalable for large problems.
- I think the main difficulty in scaling up the proposal is having to perform operations on n x n matrices to compute the supremum (e.g. R, C, \Pi in Alg 2). But I guess in practice, you may be able to reduce the storage costs, exploit sparsity patterns (via the proposed regularization) and avoid having to visit all the matrix entries (e.g. using the fact that you only care about examples within a minibatch while performing SGD).  Would be great if you could provide some run-time numbers for your approach for different dataset sizes.
- The generalization bound has an exponential dependence on the dimension "d". The authors say this is unavoidable because of the  relaxation they introduce to the original robust loss where they replace the original dataset with an augmented dataset. How important is the use of the augmented dataset "D_0" to the working of the gradient boosted training (the exponential dependence on "d" seems like a heavy price to pay to enable this relaxation)? I understand that this was needed to ensure that you only need to compute gradients for the loss \ell w.r.t. the scores f(x) and not w.r.t. the labels "y", but I wonder if there's any another way to handle the labels without worsening the generalization bound.

Finally, I have one other questions about the generalization bound Thm 3.4:
- In your proof, you seem to make use of the dual formulation for the robust loss L. But would you need the model to be continuous in its inputs for strong duality to hold? In other words, looking at proposition 1 in Sinha et al. (2018), the equality between the primal and dual objectives seem to require some form of continuity assumption on the loss "l" as a function of "x". Indeed Assumption 3.2 in your paper does assume the loss is Lipschitz continuous but wouldn't this require the model to be continuous in its inputs?

Other comments/questions:
- Sec 2.2: I thought the joint distribution was over "D_0 × D_0" from the previous subsection, but you mention "D_0 x D" here. Is "D" the un-augmented training distribution?
- Convergence of functional gradient descent (Appendix A.1): I think assumption A.2 assumes that the base learners are rich enough to model a close approximation to the gradients ∇L(f). I'm not too familiar with the analysis of gradient boosted DTs, but is there an interpretation of gradient boosting where line 6 in Alg 1 is seen as a "projection step", so that you don't have to make a strong assumption about the base learners? In other words, can gradient boosting be seen as a form of functional gradient descent with a projection step (the only difficulty I guess is that the projection happens before the update step in line 7)
- Proof of Theorem 3.4 in Appendix A.2: Does "L_f" actually mean "L_e"? What does the notation \pi_i(D) denote (I might have missed this from earlier in the appendix)?
- Typo in page 3, para 1: y_1 -> y_i?

---

**Review 3:**

This paper proposes a non-smooth method to enforce individual fairness in gradient boosting. To deal with the non-smoothness of the model, it restricts the optimal transport distance to that defined on an augmented training support set and thus reduces the search of a worst-case distribution to solving an LP problem, where an approximate solution can be found efficiently by SGD on the dual space. The authors provide convergence and generalization properties of the algorithm, and demonstrate its improvement of group and individual fairness metrics in several numerical experiments.

The paper is well established and written. The proposed method is novel and the experiment results look promising.

Comments and Questions:
1. It seems that this method can be readily extended to multi-class classification problems. Does this method work with continuous output space? What would be the augmented set like?
2. The definition of W_D above (2.4) looks confusing as it is exactly the same as W. We may want to highlight the distinction as it is defined on the augmented support set.
3. A typo in (2.5) where there is no summation over i?
4. The definition of R is confusing, is it just \ell(f(x_i), y_j)?
5. How long does it take to run BuDRO on the three experiments, and compared to other methods?

---



---

## Year 2021

### Paper 1 (paper_id: qTBC7E4c454)

**Review 1:**

The submission proposes new theorems showing the stability of a class of RNNs. Further by combining these RNNs into hierarchical and feedback superstructures, the submission achieve SOTA performance on a number of tasks.

## Theoretical results

The theorems 1-5 constitute an evolutionary step in the understanding the conditions of stability. The authors also show a counterexample for the common belief that linear contraction leads a sufficient condition for non-linear stability. These results are then (partly) used to construct provably stable RNN combinations.

## Experimental claims

The experimental section, while interesting, seems to lack a main takeaway. Also some very dubious choice of reporting in the table.

* The authors claim that for the result shown on the Tab. 1, they run the Perm-MNIST trial 4 times and the results fall between 96.65 and 96.94. I was frankly shocked to find that they choose to only report 96.94 on Tab.1! Running the same experiment multiple times and only reporting the best case scenario is not good practice and leads to misunderstanding at best. I would recommend that the authors report mean + variance. (If one were to only report the best case, one could get much better performance than is achievable on average by running the experiment many many times.)

* Some claims are backed up by only a single data point. For example, the claim that increased modularity benefits performance to some point is only backed up by the fact that 44x8 performs better than 22x16 in Sec. 3.2.1. To draw a significant conclusion and demonstrate a trend, the authors can perhaps look at 50x7 and 39x9.

* In general, more data points and error bars would help convince the reader that the conclusions are real and not flukes.

* The results of 'performance vs network size' and 'performance vs modularity' for Sec. 3.1 and 3.2 are opposite each other. In 3.1 increased size makes the network better monotonically but in 3.2 it is inverted U shape. Similarly in 3.2 modularity makes the performance better monotonically, but it is inverted U in 3.1. What is the conclusion to be drawn here?

* In general, the experiment section is a little hard to read and can use a summary of the main conclusions and clearly demarcated paragraphs and sections of the experiment that demonstrate each point.



### Clarity and other minor points

* The paper is at times very clear and at times very confusing. For example, the discussion of stability and contraction are clear but then in the paragraph at the top of page 3, the authors use the symbol g for two different thing in the same paragraph.
* Theorem 7 should be slightly reworded so that it is clear that the first inequality is a condition and not a statement (this is rather obvious in hindsight but for a new reader it is very confusing).
* For denoting multiplication, I would suggest using $\times$ instead of x (e.g. 22x16 etc.)
* Why is there a section 3.2.1 instead of just 3.2?
* I would suggest an overall re-read of the paper to maximize readability.




The paper constitutes an evolutionary step in understanding and designing stable RNNs. The theoretical results are novel and noteworthy. Unfortunately the experimental results lack a clear conclusion and at times do not follow best practices (i.e. reporting only the best run out of many).

---

**Review 2:**

This paper is primarily a theoretical contribution to the construction of assemblies of recurrent neural networks. We know that combinations of learned modular components can be powerful and far more tractable than learning bespoke models from scratch, particularly in applied domains (e.g. AlphaGo). Yet so far, we have no theoretical guarantees that these combinations will actually remain stable. This paper develops the theory behind provably-stable combinations of RNNs using weight constraints and feedback mechanisms. Then, using fixed RNNs generated according to these constraints (leaving the connections between them as antisymmetric learnable parameters), the authors show that their sparse combination network is able to achieve SOTA performance on sequential image classification benchmarks with far fewer learned parameters and the previous stability guarantee.

Strengths:
- I thought that the empirical results were rather convincing for what is primarily a theoretical contribution. The authors first thoroughly investigate various permutations of their modular sparse combination network framework (# RNNs vs size of each using absolute value weight constraints) and do another investigation of their alternative SVD weight constraint network (which doesn’t perform as well or train as quickly). Most importantly, they then show that they can best SOTA algorithms on some of the common (albeit easier) benchmarks in the field, even under (and perhaps because of) these constraints.
- The theoretical contribution is quite powerful. There has been a lot of recent work in networks with many individual recurrent components, such as the aforementioned AlphaGo or the more general recurrent independent mechanisms (RIMs) framework, but for the most part, they rely on intuitive explanations and empirical results over theoretical guarantees. Clearly specialized RNN modules can be quite powerful, but RNNs are notoriously unstable and difficult to learn, and learning such models end-to-end is tricky. If we can apply these constraint conditions and still achieve good performance (which seems like it could be realistic, particularly in the absolute value constraint case), then we can develop sets of useful modules and mix-and-match to the task in question. This paper doesn’t answer all of the intermediate questions, but the stability analysis is a key step.
- The proofs in the appendix are well-done and easy-to-follow, given a sufficient math background.

Weaknesses:
- This paper is very dense and difficult to follow. It took me a few reads to really understand the value of network stability and how it’s achieved in this case. The appendix is a mandatory read as are some of the references. None of the use cases are particularly intuitive. I think I would have liked to see a graphical representation of the sparse combo network (rather than the weight matrices in Figure 2), some pseudocode for the algorithms (tossed in the Appendix), and maybe an example case of an unstable network assembly diverging. I also feel like my familiarity with AlphaGo and other methods gave me more of an insight into how this would help in practice than the actual paper did.
- As much as I liked the empirical results that were provided, they’re all of a kind: sequential image prediction. I would have liked to see at least one application in a different domain (NLP, RL, continuous control, etc).


Overall, I would accept this paper. Although it was difficult to follow and required a lot of consultation with the literature, I do ultimately think that this is a direction that DL algorithms are going in and that the theoretical and practical results from this work could be quite powerful. To make the paper better, I would like to see some results in a different domain and more effort towards improving the readability. Too often, valuable theoretical works go underutilized because they’re difficult to understand or don’t seem relevant to the empiricists and engineers who could build on them.

---

**Review 3:**

The authors studied contraction properties of continuous-time recurrent neural networks. They further showed that a network of provably stable RNNs (net of nets) can be trained to reach competitive performance on several benchmarks, including sequential CIFAR10, even when only connections between modules are trained.

Strength
How to assemble a network of RNNs is an interesting problem. The theorems on contraction properties are helpful to people thinking about provably stable RNNs.


Weaknesses
(1)	Section 3.2.1 is pretty dry to read. Reporting results from many individual AxB networks seem unnecessarily.

(2)	The authors showed performance comparison with other types of networks in Table 1. I think it would be quite informative to show performance of networks where everything is kept the same, except that the RNNs are no longer provably stable.

(3)	It would also be good to know what happens if all connection weights are trained, not just the connections between modules. Does the performance actually decrease despite having more parameters?

(4)	The provably stable part is kind of separated from the training modular network part. How closely are they related? Is having stable RNN modules particularly important for sparsely connected modular networks?



Overall, this is an interesting paper that takes a less common approach to RNNs: provable stability and net-of-nets. The results are at places more difficult to read, but overall it is clear.

I want to add that I cannot evaluate whether the mathematical derivations are correct.

---

**Review 4:**

In the paper, the authors study stable architectures for RNNs. On the theoretical side, the authors present a series of conditions such that a weight matrix of an RNN is contractive. On the modeling side, the authors propose RNN architectures that have contractive weight matrices. The proposed methods are evaluated on benchmark datasets including sequential MNIST, permuted MNIST, and sequential CIFAR-10.


The theoretical results seem interesting, although not very surprising. However, I think there is disconnection between the theoretical results and the proposed model: not all theorems are relevant to the proposed model.

For the proposed model (Section 3.1), I might have missed something, but I fail to fully understand the model; the presentation could be improved to help the readers. For example, the mentioning of "subnetworks" at the beginning of Section 3.1 is not defined/explained. It's unclear to me how the subnetworks are combined. I can only infer from the "recursive construction" in the title and Figure 2 that the resulting weight matrix is a block matrix.

Furthermore, the idea of parametrizing orthogonal weight matrices by exponentiating skew-symmetric matrices is not novel and has been explored in expRNN [1].

The writing of the introduction section could also be improved. Instead of discussing AlphaGo and modules in evolution, the reader might benefit from a more thorough literature review of the RNN trainability and long-term dependence.

[1] Lezcano-Casado, Mario, and David Martınez-Rubio. "Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group." International Conference on Machine Learning. PMLR, 2019.

The theoretical results in the paper seem interesting. However, the presentation of the proposed model is not clear, and the model itself does not seem novel. Overall, I think the paper needs improvement to meet the acceptance threshold.

---


### Paper 2 (paper_id: demdsohU_e)

**Review 1:**

This paper proposes a framework to select the neural networks for downstream tasks. To identify the better generalization model, the authors propose a new metric (Neural Capacitance, NCP) to predict precise learning curves. And the authors provide the theoretical explanations for NCP.

Then the authors have verified the advantages of the proposed method when being applied to different datasets (CIFAR10, CIFAR100, SVHN, Fashion MNIST, Birds) with 17 CNN models.

I am not an expert on this research topic. maybe I cannot give a precise review.

Strengths:

The whole paper is written in high-quality and a clear manner.

To validate the outstanding performance of the proposed method, the authors conduct experiments on many popular CNN models.

Weakness:

I am not 100% sure about the performance, because the author reports the accuracy with figures rather than tables.

The authors conduct the experiments on well-known models, what about applying this framework to select the subnetwork on many NAS benchmarks e.g., [1].


[1]. Hw-nas-bench: Hardware-aware neural architecture search benchmark


NA

---

**Review 2:**

The paper formulates neural network training as a dynamical system and uses existing theory to formulate a metric (beta_eff) to predict how a pre-trained model will perform on a downstream task without requiring to train the model to convergence.

**Strengths:**
1. Application of dynamic systems theory to NNs.

While the theory described in the paper is not novel, the application of it to the task of predicting model performance is novel as far as I can tell.

**Weaknesses:**

1. Clarity

The paper uses multiple prior ideas from dynamical systems and machine learning literature but many of these choices are not well motivated or explained. This makes it difficult for a broad ML audience to understand the paper and its significance.
- The paper draws heavily from theory described in Gao et al 2016 which proposes a method to convert the reslience function on a multi-dimensional system to a single dimensional function x_eff where the critical points B_eff are a one-dimensional value. Gao et al 2016 describes this approach for a non-neural network system and it is unclear why the assumptions introduced in this paper apply to neural networks.

The simplifying assumption used in the paper is that the dynamics of a node in a complex multi-dimensional system can be characterized by the average nearest-neighbor activity. However, in a neural network, perturbing a single weight doesn't only affect the gradients of the immediate neighbors of the node. Thus, it is unclear why this approximation is appropriate for neural networks. This is briefly touched upon in section 4.2 where it is stated that "the others’ contribution as a whole is implicitly encoded in the activation gradient" however the connection to the original theory described ini Gao et al 2016 and why this is an appropriate simplification needs more explanation.

- Next, the motivation for the NCP is not discussed in the paper. By initializing it randomly and freezing it, a random amount of fixed noise is introduced into the process and it is unclear what the purpose of this is. Additionally, is this randomly initialized each time for all the experiments? And if so, are the same experiments repeated with different initializations to control for randomness?

- What is the motivation of using the second order gradient of C to quantify the interaction strength?

2. Correctness
- In Appendix B, section D "PROOF OF THEOREM 1", it is claimed that the gradient vanishes when one of the terms of the product is 0. Then, it is stated that when either goes to 0, the numerator in beta_eff goes to 0. However, it appears that if the gradient (left) term goes to 0, the denominator also goes to 0 since the term is also present in the denominator product. This will result in beta_eff being undefined. Please clarify whether this is accurate.

3. Details
- In the last sentence of section 4.2, the self-dynamics part, f(w_i) is defined as F(w_i*), however, I couldn't find where this was defined. What does this correspond to in the context of neural networks? Additionally, why is g(w_i, w_j) = w_j - w_j*?

4. Figures
- The figures are small and difficult to read. A better presentation of the results such as a table could be added to the main paper while the figures are either added to the appendix or enlarged.

The paper introduced a novel formulation of neural networks based on prior dynamical systems theory for an important machine learning problem. However, there are some parts of the paper that require further explanation and in its current state, I think it is fairly difficult for a broad ML audience to understand the paper and it's significance.

---

**Review 3:**

Accurately predicting the performance at early stage is important for efficient model selection without incurring too much computation. The paper proposes a neural capacitance metric as a predictive measure to capture the performance of a model on the downstream task using only a handful of early training results. The metric is derived from a line graph mapped from a neural network by modeling the dynamical system of the network.

The proposed method is novel. Experiments show that the proposed neural capacitance more accurately predicts the accuracy of the model. Ablation study have been performed to study the effect of parameters such as starting epochs, size of training set, etc.

Overall, I think the paper is potentially a good paper, though I'm not an expert in this topic and does not fully understand the mathematical derivation in Section 4.2 and 4.3. My major concern is

- I'm not sure if I fully understand NCP units. Why the weights are randomly initialized and freezed during finetuning? And in Figure 1(b) does the output layer of right stack do the same thing as output layer of left stack for classfication tasks? I do not find the relationship between the new layers and what is described in section 4.2 and 4.3. Why two dense layers are chosen to be the architecture of NCP units? I think this part is missing from the paper and should be elaborated to build the connection between the previous section.


Overall, I think the paper proposes an interesting idea to model the training NN as a dynamical system and is potentially a good paper, but the some part of the method needs more elaboration.

---


### Paper 3 (paper_id: LGTmlJ10Kes)

**Review 1:**

The paper presents a curriculum learning approach applied to NLP models. Texts are separated into easy, medium and hard subsets based on a difficulty score that is given a standard NLP model. Hyperparameter tuning is used to determining evolving weights for each batch. The approach is evaluated on three benchmark datasets (SNLI, Alcohol, Cancer).

Strengths:
+ The authors conducted a braod set of experiments.
+ The paper is easy to follow.

Weaknesses:
- The main contribution of this work is to add changing weights to easy, medium, and hard training subsets, which an incremental development in my opinion.

- At the beginning of section 2.3, the authors should acknowledge curriculum methods that do not maipulate data samples, such the "model-level" curriculum approaches surveyed in [A].

- The approach divides the training data into 3 parts {easy, medium, hard}, based on a difficulty score. Ablation results with a different number of parts and other difficulty scores should be presented. I think it is particulary interesting to see what happens if th examples are randomly partitioned, i.e. does the difficulty play an important role?

- The introduction leaves the impression that weights for data shards are somehow predicted by the model at each iteration. However, in section 2.4, the reader learns that "curriculum discovery" is based on hyperparameter tuning. The claims from the introduction should be toned down.

- The performance gains shown in Figure 4 are rather small (wintin the standard deviations) compared to competing curriculum methods. The same applies to the supplementary.

Formatting and language mistakes:
- "Without a curriculum learning will be an intractable" => "Without a curriculum, learning will be an intractable";
- "Nie et al. (2020), which also studies" => " Nie et al. (2020), who also study";
- many citations are wrongly formatted, e.g. "logistic functions Richards (1959)" should be "logistic functions (Richards, 1959)".

References:
[A] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: A survey. arXiv preprint arXiv:2101.10382, 2021.

In my opinion, the weaknesses outweigh the strengths of this paper.

---

**Review 2:**

The paper proposes to learn training curricula by considering 3 sigmoids (representing "easy", "mid" and "hard" difficulty levels), that would define instances' weight as a function of time. The sigmoids' parameters are fitted to 3-bucket values of entropy of multi-annotator labels (that need to pre-exist), or of the instances' loss value dynamics (as provided by a baseline, no-curriculum, model that also needs to be trained). The paper emphasizes that the approach can replicate some of existing curriculum heuristics (e.g. easy-to-hard, hard-to-easy etc.), which however so far failed to produce a general recipe of training deep models, -- this diminishes the value of this flexibility. Finally, the approach is illustrated on 3 small to mid-size datasets with largest gaps to no-curriculum achieved on reduced datasets to re-balance classes.



In general, the approach appears to be rather constrained (only 3 difficulty levels; limited to sigmoids; reliance in part on multi-annotator labels), may be onerous to implement (pre-training a baseline; fitting the sigmoids; making manual decisions on entropy vs loss) and was validated on rather small data (balanced datasets with <10k samples) to be useful in practice.

Strengths:
1. Addresses a hard unsolved problem.
2. Modeling simplicity of the curriculum functional class (simple sigmoids that would cover most of the human-hypothesized heuristics; however, it may not be what we need - see below).

Weaknesses:
1. I find the approach conceptually different to classical curriculum learning, since it reweighs instances in the loss rather than schedules them in a particular order (except for the zero-weight case, equivalent to example skipping, where both ways converge); as far as could tell, SGD still samples data uniformly at any time. Besides, since the loss function changes, the baseline and the weighted tasks optimize different objectives and it's not straight-forward to compare them.
2. Regarding sigmoids, I'm not convinced that practical cases of ML curricula can be covered by such simple monotonic schedules; rather, the functional class should be capable of dynamically changing the scheduling weight depending on the learning stage to mitigate forgetting and revisiting learned difficulty levels (and this encompasses monotonic functions too). In fact, such bandit-based curricula increasing/decreasing importance dynamically have been already proposed (Graves et al, 2017, https://arxiv.org/abs/1704.03003) and even evaluated for NLP tasks beyond classification (Kreutzer et al, 2021, https://arxiv.org/abs/2110.06997). The draft could be completed with a discussion of why the approach of Graves et al. is not considered or, better, a comparison experiment could be added. Note that the bandit approach is free from limitations such as number of difficulty levels.
3. The requirement to pre-train the baseline prior to curriculum-enabled training defeats one of the main purposes of curriculum learning -- saving resources in the large data regime and relieving developer from manual training design. Again, dynamic curricula that are trained on the fly would be a more practical approach, as they don't require pre-training.
4. The entropy-based curriculum relies heavy on assumed human-perceived difficulty, which may not be the same for the network's point of view. The fact, that Zhang et al. (2018) and Kocmi & Bojar 2017 both found that reverse curriculum works similarly well, witnesses that human intuitions of what is difficult for the network may be wrong.
5. It looks like data balancing increases the gap to the no-curriculum approach. This again changes the task (in addition to different objectives) and raises the question of practical relevance for tasks where such balancing is not desired or not possible (seq2seq).

Minor:
- i found Fig. 1 redundant, it is just illustrating what is sigmoid function
- fuzzy wording in a few places ("good metric", "fairly distributed", "significant variance")
- change \citet to \citep in some places, Richards (1959), Shannon (2001)
- sec 3.2: "configurations is" -> "configuration is"
- "in Nie et al (2020), which" -> "by Nie et al (2020), who"

In my opinion, the proposed approach is too limited to be of practical usefulness, and its weaknesses outweigh the strengths so that the current draft is below the ICLR bar.

---

**Review 3:**

This paper proposes a new parameterized data partitioning and weighing scheme, that partitions data into three groups {easy, medium, hard} and determines a curriculum based on relative importance of different samples. They evaluate on three datasets (full and balanced versions) and show improvements over other CL approaches. The curriculum also provides interesting insights about the datasets and scale from smaller datasets to larger datasets.

#### Strengths
- Proposed curriculum method encompasses other well known curricula by parameterizing the data partition and weighing schemes.
- Data partitioning is effective, on the three datasets the authors evaluate upon, as is evident from improvements over no curriculum approach.
- Specialized curricula obtained using curriculum discovery improves over no curriculum and other state of the art approaches.
- Sample weight patterns shows the relative importance of samples on different datasets, which is interesting.
- One quite interesting advantage of this approach is that curriculum discovered for smaller, balanced datasets work well on larger datasets.

#### Weaknesses
- The proposed method partitions a dataset into three classes $\textit{easy}$, $\textit{medium}$ and $\textit{hard}$. Will this partitioning scheme work for different types of datasets, where there might be multiple levels of difficulty, multiple sub tasks? How will this method scale?
- Using human agreement as a measure of entropy might not be applicable to datasets where there is less ambiguity between samples. That will cause majority of samples to be in a particular partition. For those datasets using entropy as a measure to partition samples might not be optimal.
-  In Section 2.3, the authors describe how the proposed framework encompasses other well known CL approaches. It will be interesting to add more analysis on how often the TPE algorithm(Section 2.4) discovers curriculums where the parameters fall into pruning or sub-sampling strategy space.
- How does this approach scale to larger datasets in vision like Imagenet, CIFAR etc, that other SOTA methods like SuperLoss evaluate on? Authors evaluate on three datasets, not all of which are well known to the community. I will encourage the authors to also evaluate this approach on more popular datasets.
- How significant are the improvements over other baselines? Showing average over full and balanced datasets(Fig 4) hides some important issues. It seems from Figure 10, that DP method outperforms the proposed CL method on full datasets. We should pay more interest to this number as we would want any approach to work well on full dataset, rather than a subset(balanced) dataset.
- Hard examples are down weighted more aggressively in this approach. It can be seen that the proposed approach often lags behind other approaches like No-CL, MentorNet, DP when it comes to hard examples. This issue is more prominent when it comes to full datasets. Authors should provide some additional analysis on these hard samples from these datasets for more clarity. What % of these datasets are partitioned into $\textit{hard}$ class?

I like the idea of encompassing different curriculum approaches inside a parameterized function. The authors show improvements over other approaches on three datasets with curriculum discovered by this approach. My reservations are mostly around how well this approach will scale to larger datasets and on unbalanced datasets. Some results show that other methods are better when it comes to full datasets. Due to these reasons, I will am recommending the current score. I encourage authors to address these concerns and I will be happy to bump up my score.

---


### Paper 4 (paper_id: gFDFKC4gHL4)

**Review 1:**

A method to estimate the confusion matrix of a black-box classifier, using as few samples as possible. The paper is focused around one use case: tracking changes in ML APIs, considering that such changes may go unannounced.

Strengths
- The use case is one that is not often studied, yet is important both from a business accountability perspective (assessing the trustworthiness of a commercial ML API) and ethics perspective (providing information to society about hidden dangers in commercial ML APIs, which are difficult to track because of the algorithmic intransparency that is a common problem in the tech industry)
- Algorithm is uncomplicated to implement, and provides clear improvement over naive sampling methods
- Collected dataset on API shifts is novel, and is a major contribution of the work if released to the public

Weaknesses
- The idea of difficulty levels (K) is not well-developed. There are a couple of places where I was expecting more details: (1) some experiments use K=2 while others use K=3, what is the justification for doing so?; (2) I was expecting an ablation study comparing MASA with K=1 vs MASA with K>1. As the paper stands, it is unclear just how much of MASA's performance improvement over uniform and stratified sampling is due to K, versus the uncertainty-based sample selection.

Other suggestions for improvement
- Being able to measure the uncertainty in each partition, and use that uncertainty to inform sample selection, is the key idea that makes MASA perform better than uniform or stratified sampling. Given the importance of this idea, it would be better to explain this via an intuitive figure. As a suggestion, the authors could visualize how a partition in which the ML API classifies all data points with the same class (whether right or wrong) will have low uncertainty, while the opposite gives high uncertainty.

The paper makes atypical but important contributions to ML ethics. Although the proposed algorithm is not groundbreaking from a technical perspective, it does contribute significantly towards measuring and tracking changes in black-box APIs, and I think it is of high value to society. I am concerned that the idea of difficulty levels (K) is not fully developed, but I lean towards acceptance.

---

**Review 2:**

Authors show that ML models behind publicly available APIs change and these changes cause result changes for input datasets.
Authors track the changes through confusion matrix differences. They propose an efficient algorithm they call MASA to evaluate the changes in results with reduced number of queries. Their algorithm achieves better estimates given the same budget than uniform sampling.

Strengths:
- Tackles important practical problem of the result differences from ML APIs
- Presents accuracy change results from a number of actual ML APIs from leading providers
- Authors created a novel algorithm (MASA) to efficiently detect and evaluate result differences for ML APIs.
- Authors demonstrate that MASA significantly outperforms baseline algorithms.

Weaknesses:
- Accuracy changes from actual ML APIs is limited in scope. Only few systems were analyzed and only for two dates (spring 2020 and spring 2021). It would be interesting and important to track both more systems and more time points
- Unclear how confusion matrixes were used for speech recognition task, which presumably has a very large number of "classes". I am guessing authors treated speech recognition problem as classification problem for evaluation. However, there are no details on this. It would be good if this was explained and info provided.
- As authors noted, confusion matrix difference is a good measure as a result drift only for certain (classification-like) APIs. It would be good to see how to deal with non-classification APIs.
- Authors suggest that the differences seen in confusion matrix provide useful insights into how API results changed and why they changed. There is little substantiation of usefulness of how the API results changed. I.e. is confusion matrix difference really the best we can do to show how the API results changed? Regarding "why" the results changed, the authors provide guesses, but it seems to me that we cannot really know based on the results. If we cannot determine the "why", this should be stated. If we can, then it would be good to see what can be determined and how.

Minor typo:
Page 7: diffident - should be different.

I think that the problem of ML API result shift is real and important. I believe authors made interesting and useful contribution in evaluating such shifts. Although the paper has some weaknesses, I would recommend accepting it.

---

**Review 3:**

API shifts are common in several deployed machine learning models. This work proposes an efficient way to measure shifts in the confusion matrices of ML models using limited number of API calls.

The problem addressed is important and the method seems to yield good results in practise in comparison to random sampling.

Once concern is its unclear why this problem cannot leverage some of the existing work on stratified sampling (based on explanation at the end of section 1) with the aim to reduce the variance of the estimator? Could you please elaborate on this. After all, the goal is to estimate elements of the confusion matrix.

Based on section 3.3, does MASA yield an optimal allocation of API calls?

API shift is an important problem that needs to be addressed in order to operationalising AI. This work proposes a way to assess these shifts using limited number of API calls and has a lot of practical importance. Most deployed ML models are also priced in a manner wherein each API calls has an associated cost and thus performing API shift assessment using limited calls is quite important even from a pricing stand point.

---

**Review 4:**

This paper considers the problem of estimating the change in the performance of commercial ML APIs (ML as a service) as the models are updated over time (experiments are for 2020 vs 2021). It formalizes the problem as estimating the change in the confusion matrix over time. The main theoretical contribution is an adaptive sampling method to more efficiently estimate this shift. Interesting empirical results on various ML APIs are provided in the paper, showing the relevance of the problem and the effectiveness of the proposed method.

**Originality and significance**: To the best of my knowledge, this is the first work to systematically investigate the shift in the performance of commercial ML APIs. With the popularity of these services on the rise, it is important to study various aspects of the models, including the variations of the performance over time. The proposed sampling method is also novel and can be used in similar settings where model queries are expensive. The paper aims to minimize the Frobenius norm of the error in estimating the confusion matrix of classifiers, while keeping the sample complexity close to the optimal allocation strategy (in hindsight). The proposed algorithm asymptotically approaches the optimal allocation decay rate of 1/N. The empirical studies suggest that the proposed method can be an order of magnitude more sample efficient compared to random sampling.

**Quality and clarity**: The paper is well written and motivates the problem with case studies and examples. The algorithms and theorems are clearly stated. I did not check the proofs in the appendix.

Some limitations of the work:
- The method (as described in the paper) only applies to classification settings.
- Although not a strict requirement, the experimental analysis of the sampling method benefits from a rough estimate of the “difficulty” of each example before it is evaluated by the ML API . This “difficulty” is calculated by using a separate (client-side) cheap model. This goes against one of the main appeals of ML APIs that try to minimize client-side evaluation setup (think installing TF runtime, etc). There are no ablation studies on the quantified role of using such client side models.
- The experiments could be improved by comparison to baselines other than random sampling. Though this might not be possible with updated APIs.

Note: I was a reviewer to an earlier version of this work. Compared to the previous version, (1) the writing of the paper is improved in parts, (2) there is a more clear discussion of related work, and (3) the dataset is being publicly released.

This work opens a discussion around the problem of estimating the performance shift in commercial ML APIs (for classification). The paper defines a metric for the performance shift of such APIs (via the confusion matrix), and presents a method to achieve near optimal sampling rates.

The theoretical contributions of the paper are small but non-trivial. The experimental analysis is detailed and interesting, but could benefit from further ablation studies on the effect of the client-side difficulty gauge model. The problem is of interest to the ML community and the release of the annotated dataset used in this work would be useful to the community.

---


### Paper 5 (paper_id: uxxFrDwrE7Y)

**Review 1:**

In this paper, motivated from the CLS theory in neuroscience that efficient learning requires short-term adaptation and slow learning of structured information, the authors propose a novel dual memory experience replay method for continual learning. The idea is to build  long-term and short-term semantic memories by maintaining another two models: plastic model and stable model. The plastic model is used for fast learning of recent experiences and the stable model is used for slow learning of structural knowledge. The two models are updated with a Mean Teacher fashion during training with different frequencies. The proposed CLS-ER is  task-agnostic and can be applied in different continual learning settings. The experimental results show that the proposed CLS-ER outperforms several baselines for continual learning.

Strengths:

1. The proposed method is well motivated and novel in the context of continual learning. Maintaining multiple models for different purposes is interesting and is shown to be helpful for continual learning.

2. The writing and organization is clear. It is easy to understand the proposed algorithm and the details.

3. Different continual learning settings are considered, the proposed approach shows better results across all the settings. The analysis of different models also make it easy to understand how the method works.


Weaknesses:

1. One of the main drawbacks of the proposed approach is that three models need to be maintained during training which incur a heavy cost.   One question is that if reducing the individual model size while keeping the whole model size the same, what's the performance in this case?

2. The proposed method has a lot of hyperparameters to tune, which may be a problem if the method is applied to a different task.


More questions:

1. Why update the plastic and stable models stochastically?

2. In table 1, why CLS-ER is better than JOINT on S-MNIST? JOINT should be the upper bound performance?

3. The authors make a connection to flat minima, if explicitly enforcing flat minima, would the proposed method can still do better?

In this paper, a novel idea of continual learning is proposed. The authors clearly describe the proposed approach and also give a detailed analysis. Overall, it is a well-written paper with an interesting idea. The results also show the benefits of the proposed approach.

---

**Review 2:**

They propose a dual memory experience based on complementary learning systems (CLS).
The working model is updated using consistency loss with the selected optimal semantic memory from plastic and stable models.
Plastic and stable models are updated with an exponential moving average trick (EMA) and the working model.

The method is very simple to understand and mimicking the CLS theory with the hippocampus and neocortex is interesting.

Compared to existing methods, is it a fair comparison in terms of training cost such as using two models including plastic and stable models and using reply data to train the working model?

I think more analysis about plastic and stable models are needed because these model act like hippocampus and neocortex based on CLS theory. What is optimal semantic? Just select the logit with the highest softmax between the two models?

I am not an expert in this domain and have not much experience with related works. However, the idea is very simple and reasonable and the results outperform previous existing methods. However, more analysis is needed for the claim.

---

**Review 3:**

The paper presents CLS-ER, a dual memory mechanism for the continual learning setting. During training the model receives data from a non-i.i.d. source as well as random samples from a sample buffer (the episodic memory). The stable and plastic models act as teacher models. Their logits are the target of the student model. Which one is used as a target depends on the highest softmax score w.r.t. a ground trough class. The teacher networks are both exponential moving averages of the previous student models. Thus, the only difference between the teacher models "plastic" and "stable" is their EMA rates which in the case of the plastic model is larger such that it focuses on more recent weight updates.  The slower learning of the stable model is the mechanism which ought to mirror the consolidation of structural knowledge. It is used from inference Samples from the datastream are transferred to the sample buffer using reservoir sampling.

The authors experiment on image datasets (variants of MNIST, CIFAR10, Tiny-ImageNet, see the Mammoth framework) with data augmentation (random flips and crops). They compare with several prio works and the evaluation is thorough and convincing.

I enjoyed reading this publication. I'm not very familiar with the latest CL methods but the authors address various related works and evaluate all models thoroughly. I think the setting without task boundaries is harder and more interesting since the real world also doesn't come with clear task boundaries. The paper is following recent work towards robust and extensive evaluation which is more difficult but will help the CL community to make real progress.

The empirical upper bound "JOINT" is very instructive and highlights that there is still room for progress. That said, the JOIN performance seems low given the excellent performance of other models on MNIST, CIFAR, Imagenet datasets. Is this because the models are very small? Does CLS-ER work also with larger models?

The flat mima discussion is less essential but is probably missing a reference to Flat Minima, Hochreiter et al 1997.

Instead of the interesting section 6 I'd have preferred some ablation experiments (though the component analysis is appreciated). E.g. are two teacher networks necessary? How much worse is the model if one teacher network is used (with a tuned rate)? What if there were three networks (each with their own rate)?

I did not understand how samples are removed from the buffer (the episodic memory). Are samples removed eventually? If yes, how and where is that explained? If not, what is the space complexity of algorithm 1?

I think this work is good in general. The writing is great, the evaluation seems thorough, and comparisons look fair. The method is easy to understand and the empirical results are convincing to me. The authors discuss recent and related works and their method seems original but I lack the expertise to judge the novelty of the approach. It builds significantly on previous work (DER) so I'd argue that some aspects of the publication exist in previous work.

---

**Review 4:**

This paper proposes a novel dual memory experience replay method to store the knowledge of previous tasks. In addition, long-term and short-term semantic memories are leveraged to replay the neural activities of the episodic memories and align the decision boundary.


Strengths:
1, Semantic memory is the first leveraged in incremental learning, which is interesting in my opinion. The contribution of this paper is novel.
2, The codes are released, which guarantees the reproduction of the proposed method.
3, The experiment results prove the effectiveness of the proposed method, which obtains impressive performance.

Weaknesses:
1, Can this method be applied to online continual learning task?
2, The experiments for domain-IL are only evaluated in one dataset. Experiment results on more datasets should be shown in this paper.
3, For Class-IL, some experiments with different base classes should be shown in this paper.


In my opinion, the contribution of the proposed method is novel and interesting. In addition, the writing of this paper is clear and easy to understand. The experiments are sufficient to prove the effectiveness of the proposed methods. However, more experiments should be designed to prove the proposed method further.

---



---

## Year 2022

### Paper 1 (paper_id: IQM-3_Tzldw)

**Review 1:**

1. The paper tackles the problem of client drift (locally) in federated learning (FL) due to heterogeneous client data distributions. Besides, they target the period drift (globally), which is the inter-communication heterogeneity of data distributions.
2. They propose a learning based parameterized aggregator called FEDPA, debiasing model aggregation under client drift and period drift in a unified framework. Their key approach is to learn an adaptive calibration parameter to approximate the global objective.
3. The input of the framework includes the intra-communication client parameters.


Strength
1. The 2 levels of drift they tackle is indeed important for FL.
2. They can achieve better training performance on the MovieLens dataset and FEMNIST dataset.
Weaknesses
1. Some small typos, grammar issues, inconsistency issues as stated below.
2. The key idea should be agnostic to the dataset and types of tasks. It would be good to show the performance on other LEAF datasets.
3. It would be good if the overhead on the server can be quantified. If the method can not be applied at scale (equation 7 seems to be iterative but more clarification would be good), it is not a perfect match for FL.


0.Could you explain how the drift issue in FL (stated in Figure 1) is different from similar convergence issues in non-FL training pipelines? Explaining this is important for people to understand that this is indeed a new problem.
1. At the beginning of section 2, “Federated learning with non-iid data Federated Learning with non-iid Data” is duplicated.
2. “Implementation” does not end with “.” in section 4.1.
3. “We compare FEDPA with the baselines that using proxy dataset” should be “We compare FEDPA with the baselines that use proxy datasets” in section 4.2.
4. The idea is inspired by the control theory and dynamic systems. They add a regularization term, which is determined by the client weight updates, weight matrix and the parameters of the learnable aggregator.
5. Could you explain Equation 7 regarding how the dense() operator actually functions?
6. Plan for open source code?


The introduction and related work part is in good shape and I enjoyed reading it. But the quality for writing could be improved. (See the above session for more details)

---

**Review 2:**

This paper proposed a method named FedPA to learn aggregators in federated learning. When aggregating model updates from clients, instead of uniform or weighted by number of examples as in the popular FedAvg, FedPA will feed both the global model and the client model updates to a neural network before “aggregating”/averaging. The aggregator is trained on the server with a proxy dataset. Experiments on EMNIST and MovieLens show the advantage of FedPA.


The general idea of “meta learning” and “learning to aggregate” makes sense.

However, as the authors commented, though interesting, having a proxy dataset is a strong assumption in problem setting.

In addition, the server seems to have access to the model updates of each individual client, which makes it hard to be consistent with FL privacy principles, and other privacy techniques like SecAgg and differential privacy [Federated Learning and Privacy https://queue.acm.org/detail.cfm?id=3501293]

My major concern is that the proposed method seems to be ad-hoc. It is hard for me to connect the motivation of “period shift” to the proposed FedPA method. Instead of learning neural networks for aggregation, I am wondering if there are easier approaches to use the proxy data. For example, we can simply “learn” a scalar weight to do weighted aggregating/averaging of client model updates. I would strongly suggest some more ablation studies of the proposed FedPA method.

A second major concern is the experiment performance. The accuracy on FEMNIST seems to be lower than expected. For example, [Adaptive Federated Optimization https://arxiv.org/abs/2003.00295] reports >80% for natural user non-IID.

I would also appreciate some more comments on hyperparameter tuning. For example, how are 100 communication rounds, 5 epochs, learning rate \eta_l=0.01 chosen? How are training epochs (5 for MovieLens, 30 for FEMNIST) and learning reate \eta_g chosen?



AFAIK, the idea is novel.

Need improvement and clarification: the intra/inter-communication arguments look inaccurate to me. The global model and control variates are shared “inter” rounds, for example, in SCAFFOLD [Karimireddy et al. 2021]. Some previous work also assume all clients can participate in training, and I would strongly encourage the authors to clarify the source of “intra-round heterogeneity”

Could you clarify “since many companies like Google, Facebook remains previous data at the turning point of legislation for privacy” for motivating the proxy dataset?

I may have missed it, is the code open sourced? The authors mention they implement the algorithms in PyTorch. Using a FL framework, or based on previous released code can significantly help reproducibility.

Minor issue:
The citation format does not seem to be consistent. I would suggest the authors carefully consider the usage of `\citep` and `\citet`.
I cannot understand why Kairouz et al 2021 is cited for Figure 1.
Some grammatical errors might need to be corrected.


The idea is interesting, but the draft itself needs improvement. Ablation study and experimental performance are my main concerns.

---

**Review 3:**

This paper proposed a method called FedPA that deals with client and period drift problems. The period drift problem is caused by the asynchronized updates of each client, leading to extra bias in model aggregation. The authors proposed a learning-based aggregation strategy, that parameterizes the aggregation function using neural network models. The models are trained under a meta-learning framework, which treats the global model as a meta-learner and each client as a specific task. Experimental results have shown that FedPA can account for the additional bias induced by both client and period drift and therefore demonstrate superior performance over other FL baselines in various tasks.

Strength:
1. Drifts in FL arise in time and space, while most existing works only address the heterogeneity of client data distributions. This paper has discovered this practically important problem and proposed the notion of period drift that can facilitate further research.
2. The authors have conducted comprehensive experiments in different settings. Results have shown that FedPA has a superior advantage over baselines in different categories.

Weakness:
1. This paper lacks an in-depth discussion on why meta-learning frameworks are particularly suited for the period drift problem. It seems like both client and period drift influence the model performance by introducing extra bias in model aggregation. In that case, why not use a regularization-based approach incorporated with a temporal dimension? Moreover, it seems like this paper [1] have studied a similar problem, the authors could consider comparing FedPA with their work as an additional baseline.
2. The dynamic system analogy seems useless in section 3. The authors are not using significant knowledge from this area. I would recommend adding more discussions or simply removing this part to avoid confusion.
3. From my understanding, FedPA accounts for additional bias via controlling $\Delta w_t^k$ through $u_t^k$, then why do we need two separate neural networks for both $w$ and $\Delta w$? The authors need to be more specific on the choice of NN architectures.

Minor:
1. Please add a discussion on FedDL in section 2.
2. Please move the definition of $n_k$ and $n$ to the beginning of section 3.

[1] Jothimurugesan et al., Federated Learning under Distributed Concept Drift, 2022

This paper is well-motivated and easy to follow. The technical novelty is ok but not much. It is also ambiguous whether the proposed method is the best solution for this problem (please see weakness).

This work can benefit from adding more in-depth discussion on the unique advantages of the proposed method and further polishing the writing. I am leaning toward rejecting this paper at this time.

---

**Review 4:**

The paper presents a learnable aggregation scheme in the context of federated learning. The paper achieves this using meta-learning to generalize the parameters of the aggregator with a proxy dataset. The paper identifies 'period drift' in the current federated learning setup and presents the meta-learning-based aggregator as a way to overcome this issue. The paper follows up with experimental results showing increased accuracy for different methods and heterogeneity rates across two datasets.

Strengths
1. The paper identifies a possible source of client drift
2. The paper proposes a novel aggregation scheme.

Weaknesses
1. The paper does not do enough to discriminate between regular client drift and the so called period drift either theoretically or through experiments.
2. The aggregation strategy uses a proxy dataset which limits use cases. Also, it is very similar to other knowledge distillation-based techniques like FedET[1] and DS-FL[2]. A comparison of performance with these methods should be shown to justify its usefulness.
3. There is no ablation study showing the effect of the data distribution in the proxy data on model performance.
4. The experimental settings are not strong. The datasets and models are too simple. I suggest including results on CIFAR-100 and Stack Overflow datasets.


[1] Cho, Y. J., Manoel, A., Joshi, G., Sim, R., & Dimitriadis, D. (2022). Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning. arXiv preprint arXiv:2204.12703.

[2] Itahara, S., Nishio, T., Koda, Y., Morikura, M., & Yamamoto, K. (2020). Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data. arXiv preprint arXiv:2008.06180.


Clarity: The paper is not very well written and has some grammatical mistakes.
Quality: The paper quality needs to be improved. The axes font in the figures is too small to read and overall the writing needs to be updated.
Novelty: The paper has limited novelty.
Reproducibility: No code was given.

The paper proposes a meta-learning-based aggregation scheme. However, it does not show enough theoretical or experimental justification to highlight the effectiveness of the algorithm. Additionally, the paper lacks enough ablation studies on the different aspects of the algorithm like the data distribution of proxy data, the influence of the size of the aggregator model, etc.  Furthermore, the paper's concept of 'period drift' is not well defined despite being a key motivation of the algorithm.

---


### Paper 2 (paper_id: rUxKM6u8WER)

**Review 1:**

This paper shows that under suitable assumptions, the Bayesian optimal robust estimator requires test-time adaptation, and such adaptation can lead to a significant performance boost over standard adversarial training. It then proposes self-supervised test-time fine-tuning on adversarially-trained models to improve their generalization ability. A MAT strategy is introduced to find a good starting point for the self-supervised fine-tuning process. Extensive experiments on CIFAR-10, STL10, and Tiny ImageNet demonstrate that the method consistently improves the robust accuracy under different attack strategies, including strong adaptive attacks where the attacker is aware of the test-time adaptation technique.

I think this paper has the following strengths:

1. The idea of using a meta adversarial training method to find a good starting point for test-time adaptation is novel. The ablation study experiment shows that the meta adversarial training is effective in improving robust accuracy.

2. It theoretically shows that the estimators should be test-time adapted in order to achieve the Bayesian optimal adversarial robustness, even for simple models like linear models and the test-time adaptation largely improves the robustness compared with optimal restricted estimators. The theoretical results are significant, though I don't check the correctness of the proofs carefully.

3. The experiments show that the approach is valid on diverse attack strategies, including an adaptive attack strategy that is fully aware of the test-time adaptation, in both the white-box and black-box attacks.

4. The paper is well-written. The proposed defense method is described clearly. It provides enough details of the adaptive attacks used.

However, I think this paper has the following weaknesses:

1.  It doesn't evaluate the Greedy Model Space Attack (GMSA) proposed in [1], which is designed for the transductive-learning based defenses (or the test-time adaptation defenses). I think it is important to include GMSA in the attack evaluation of the proposed defense.

2. It doesn't report the point-wise worse-case robust accuracy under all the attacks considered. For each test data point x, if any of the attacks considered could find an adversarial example x' that fails the defense, then the robust accuracy on x is 0; otherwise, the robust accuracy on x is 1. If we aggregate it over the entire test set, we can get the point-wise worse-case robust accuracy under all the attacks considered. It would be good to include this metric in the experimental results.


[1] Chen, Jiefeng, et al. "Towards Evaluating the Robustness of Neural Networks Learned by Transduction." International Conference on Learning Representations. 2021.

I think this paper is well-written and the proposed method is described clearly. The ideas are novel and the results are significant. It provides enough details for the experiments.

Overall, I am positive about this paper. I only have some concerns about the adaptive attack evaluation. It is possible that there are stronger adaptive attacks than the ones considered in the paper. I give a weak rejection for now. If the authors could address my concerns, I am willing to raise my scores.

--------POST REBUTTAL--------

The authors have addressed my concerns. Thus, I raise my score from 5 to 6.

---

**Review 2:**

This paper proposes a test-time fine-tuning to boost the adversarial robustness of the classifier. Namely, the new method updates the parameter of the underlying deep network based on self-training (and potentially assuming the network is trained with a meta-learning algorithm). To evaluate the robustness, the paper places the model under a regular white-box adversary and a smarter one aware of the fine-tuning and shows that even with a smarter adversary the fine-tuned model is still more robust.

### Strength

The paper has a solid motivating theorem that shows why a test-time fine-tuning can improve the robustness. Even though the theorem does not directly help the design of the final algorithm, the theoretical contribution is still important. In the empirical part, the paper uses a standard set of adversaries, e.g. AutoAttack and the improvement of robustness is significant compared to its baselines. Overall, the structure of the paper is easy to follow and I know what to expect when I finish reading one section.


### Weakness

My major concern is the empirical evaluation. I also have some minor concerns on the motivating theorem and the writing. Please see the details below.

**The setup of an empirical adversary may not be strong enough.** Firstly let me restate the setup of the problem. If my read on the paper is correct, given a batch of input $X$, this work proposes to fine-tune the parameter of the model $\theta_0 \rightarrow \theta$ and the update $\Delta \theta = \theta - \theta_0$ is a function of the batch $X$ (and some training batch).

To evaluate the robustness, the paper uses two adversaries: a standard and an adaptive one. I am not able to find the definition of a standard adversary but by reading the description of an adaptive one I think the paper assumes that a standard one targets on a model parameterized with $\theta_0$ while an adaptive one targets $\theta$ (please correct me if I am wrong).

The paper assumes the adaptive one is a strong adversary by fully leveraging the knowledge of the fine-tuning process. My comment for this setup is that:

(1) the adaptive one may be just as strong as a standard adversary who is faced with a model without fine-tuning. This is because the white-box adversary has access to the parameters (i.e. $\theta$) of the model that makes the inference instead of some parameters (i.e.$ \theta_0$) that have nothing to do with the inference;

and (2) an adversary who is smarter should be targeting the fine-tuning process. One example I can think of is that the adversary carefully constructs the test batch sent to your system such that these inputs sit evenly on two sides of the decision boundary and are both less than $\epsilon$ away from the boundary. How would the fine-tuning behave? Will it almost make no update to $\theta_0$ (so the adversary can attack the original model again) or it gives up one half the points so after fine-tuning $\theta$ can be robust on the rest? In practice, the adversary can jointly optimize the noise added to each input. For example, if the adversary only cares about the inference result on $x_i$, it can accompany another input $x_j$, together with $x_i, that targets only on making the fine-tuning doing nothing or worse. I drew a picture [here](https://ibb.co/yXPS71D) for a linear classifier to illustrate the case. Also, the training set used in fine-tuning is also exposed to the attacker and a realistic attacker should take advantage of it. In general, I don't think the current adaptive attacker is adaptive enough to the potentially vulnerable parts in the proposed defense.

Two additional questions regarding the experiment:

1. a strictly stronger attacker should always have lower accuracy than a standard one but in Table 1, some adaptive attacker is even worse than a standard one. For example, on the intersection of the row Probation-OnlineFT and the column Square Attack. Can the authors give some explanation to these results?

2. It seems that there are a lot of hyper-parameters to be tuned. Can the author provide some recommendations of ways to find these parameters in the main body of the paper.

**Soft labels used in Theorem 3.1.**  Will the theorem fail to hold if considering hard labels like [0, 1] instead of soft labels generated from Gaussian noise? I think in a classification setup using a soft label might be okay if well-explained motivation is given.

**Typos.** I found many typos in the paper. I list some examples here:

1. Near Theorem 3.1: “However, for ^ arbitrary ratio c = n/d” (an is missing)

2. Near Theorem 3.1: “we plot the of adversarial risk of three estimators for different adversarial budgets, which clearly shows that adaption can significantly …” (missing “our” or “the” before “adaptation”)

3. Near Theorem 3.2: “The theorem shows that when the given input is the adversarial, the test-time adaptation can still ^ lower the adversarial risk” (“is adversarial”)

4. In Figure 1, it should be $\theta_AB$ not $\theta_BA$ from the caption (or I mis-understand the picture).

5. Near Eq 6, “more efficient for more large amount of data” (larger instead of more large)



### Clarity
Overall the idea of the paper is clearly stated. The clarity can be further improved if (1) The notation becomes simpler and less dense; (2) Some definitions are pretty ad-hoc. For example, the definition of $L_{SS}$ is not presented until the experiment section that discusses what are the self-learning tasks; and (3) Taking several passes to fix the typo.

### Quality
The theoretical part of the work is sound. The empirical evaluation does not convince me that the proposed method is actually more robust (see the Weakness part in the previous review box).


### Novelty
The proposed method is somewhat novel by adapting test-time fine-tuning to improve adversarial robustness.


### Responsibility
The reproducibility can be improved if the author provides a summary paragraph about where to find descriptions that produce the experiments.



In summary, I am inclined to reject it at this moment because I am not sure if the proposed method actually produces a new network $\theta$ that is more robust with more obviously vulnerable parts exposed to the adversary. The writing of the paper can be improved as well.

---

**Review 3:**

The paper introduces a test-time adaptation method to improve the adversarial robustness of a neural network by proposing (a) a test-time objective based on existing self-supervised training methods (e.g., RotNet [Gidaris et al., 2018]) and (b) a pre-training objective based on incorporating gradient-based meta-learning (e.g., MAML [Finn et al., 2017]) and adversarial training (AT). Experimental results on CIFAR-10, STL10, and Tiny ImageNet demonstrate the effectiveness of the proposed method in improving adversarial robustness under several adversarial attacks.

[Gidaris et al., 2018] Unsupervised Representation Learning by Predicting Image Rotations, ICLR 2018

[Finn et al., 2017] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks, ICML 2017


**Strength:**

* The experiments are performed under various datasets to validate the method.
* The paper tackles an under-explored problem of improving adversarial robustness via additional test-time adaptation.
* The method is easy-to-implement, yet shows a considerable improvement in robust accuracy.
* The paper performs a theoretical study that motivates the method

**Weakness:**

* Although I appreciate that the paper includes a discussion (and results) on adaptive attacks (Eq. 12), but I personally doubt that the attack could indeed faithfully find a true (hidden) adversarial examples: (a) the threat model considered in this paper is quite challenging, in a sense that now the attacker should find a sample that minimize the chance of correct adaptation (with a high-dimensional optimization); (b) empirically, the results from adaptive attacks are sometimes worse than the standard attack, which signals that the adaptive attack can be actually worse sometimes.
* The theoretical analysis in Section 3 yet does not fully justify how the proposed method in Section 4 helps to improve the robustness, e.g., to motivate the use of the self-supervision based test-time training objective. Instead, all the explanation in Section 3 deals with a direct AT-like objective; how can such a self-supervised task replace AT as the surrogate objective? Any intuitions/high-level explanations would be welcome. Moreover, at least, for better clarity to readers, I recommend mentioning Section 5.2 in Section 4 that the gradient of the self-supervised objective empirically shows a highly-correlation to a gradient of the AT objective.
* Given that the main claim of the paper is “AT without adaptation cannot be optimal for the best robustness”, the paper should show the superiority of the method by providing a comparison with state-of-the-art AT methods (e.g., TRADES [Zhang et al., 2019]), not limited to regular AT [Madry et al., 2018].
* The technical novelty may be limited: both the pre-training objective (meta adversarial training) and the adaptation objective in the proposed method are an extension of prior works using adversarial examples.
* The paper is generally hard to follow, and the writing could be improved for better clarity. For instance, in the second paragraph in Section 1, “Theoretically, AT does not achieve the optimal robustness. Under suitable assumptions, the Bayesian optimal robust estimator requires test-test adaptation.” appears without detailed explanation or references. It would be much better if the authors can provide several prior relevant works (if it exists) on investigating why the optimal robustness cannot be achieved only with AT and test-time adaptation is a necessary choice for the robust estimator. Otherwise, the paper may include more insights/intuitions on these arguments, as this is the very first part of the paper.

**Questions:**

* The test-time adaptation objectives include the regularization term, which requires the training set: how does the training batch is selected, and is the proposed method robust to the choice of training mini-batches?
* Can the proposed method be combined with other AT objectives (e.g., TRADES), not limited to the vanilla AT objective [Madry et al., 2018]?

**Minor:**

* Typo: test-test adaptation → test-time adaptation (Section 1, second paragraph)
* Typo: Definition 3.2 to 3.2 → Definition 3.2 to 3.4 (Section 4, Theorem 3.1)
* Typo: more large → larger (Section 4.1, last paragraph)


[Madry et al., 2018] Towards Deep Learning Models Resistant to Adversarial Attacks, ICLR 2018

[Zhang et al., 2019] Theoretically Principled Trade-off between Robustness and Accuracy, ICML 2019


**Clarity**

The current manuscript was generally hard to follow. The writing could be improved, in particular for Section 1 and 3.

**Quality**

The experiment is performed on various datasets to verify the effectiveness of their method. I believe the quality can be further improved if the method is compared with other AT methods (e.g., TRADES) and explain “why” the proposed method is beneficial for further improving adversarial robustness.

**Novelty**

While the direction that the paper tackled is relatively under-explored, the technical novelty may be limited; it is generally an extension of existing works in improving the robustness of neural networks against the distributional shift.


The paper tackles a challenging and under-explored problem of test-time adaptation for adversarial robustness. However, I slightly feel a lack of technical novelty, and that the overall clarity could be improved. For the evaluation side, I am still not fully convinced that the adaptive attacks considered here would faithfully assess the proposed model, which would affect the correctness of defense evaluation. In these respects, I am currently on a slight negative side.

---

**Review 4:**

This paper proposes test time training for the task of learning adversarially robust models. The idea is to finetune the model for the test samples using a combination of self-supervised loss on the test samples and a memory-based loss on a small subset of training samples. Since the original adversarially trained model might not provide a good initialization for finetuning, a meta-learning based objective is proposed that learns a model that gives best finetuning robustness. Experimental results are shown on some benchmark datasets.


Strengths:

The idea of using test time training in the context of adversarial robustness is new. The paper advocates for the need for sample-specific decision boundaries as they claim that it is needed to get Bayesian optimal adversarial robustness. To do this, the paper uses a simple self-supervised learning objective on the test samples.

The objective function used is simple and intuitive. The fact that the test time objective also supports a batch size of 1 is nice.

Results clearly show that using test time training improves the accuracy compared to standard adversarial training. The authors also show results on adaptive attacks, which is nice.


Weaknesses:

I think one of the main weaknesses of the paper is that it doesn’t compare with the SOTA approaches such as TRADES, MART, etc. Comparison with SOTA approaches would be nice. The authors can use some benchmarks like RobustBench to compare their models with the best ones out there.

The other thing that is concerning me is the difficulty in training meta-learning based objective. Training models with meta-learning objectives are not straightforward, so it might be hard to scale this to large datasets like Imagenet.

The next issue is the increase in the inference time. From Figure 3, it looks like more adaptation steps are needed to improve the accuracy. This would mean inference time would rise, which could be a concern.

The method proposed by the authors does not depend on what type of adversarial robustness algorithm used. We can simply replace the AT loss with any type of loss used. So, it would be interesting if this method can improve over the best SOTA results. The authors have one experiment in this regard - comparison with Gowal et al.

One of the things I would have liked to seen in this paper is more visualizations. Why is this approach helping? Does the classifier decision boundary change for samples near the decision surface? For the samples far from the decision boundary, do things change? Such visualizations would help improve the understanding of what is happening.

Another interesting experiment would be to see if the approach improves more if the test distribution has more domain shift. I know that the approach was not proposed with this objective in mind, but it might be an additional benefit the authors would get.

The authors use rotation prediction and vertical flip prediction as the self-supervised objective because it doesn’t have dependence on the batch size which makes sense. But these objectives are quite simple ones, and there are other objectives that could be better too. One example is teacher-student objective for self-supervised learning. I am wondering if these approaches would yield better results.


The paper is well written and explained well. I would have liked if the authors showed more visualizations.

Regarding novelty, I think the idea of using sample specific adaptation at test-time is new. I did not go over the theory section, so I can't comment much. Experimental results show improvements over AT, but I feel more expeirments could have been done to really show how effective the approach can get.

The authors provided code for reproducibility.

I think the approach is interesting. But I feel the paper lacks experimental rigor. Also, some visualizations could have really helped. I feel if authors spend a bit more time with more rigorous experiments and visualizations, this paper would get strong.

---

**Review 5:**

This paper is about adversarial training for robust accuracy against adversarial attacks. The authors introduce a new procedure to improve the generalization performance of adversarially-trained networks by using self-supervised test-time ﬁne-tuning. In addition, to determine a good starting point for test-time adaptation, a meta-adversarial training strategy based on the MAML framework is proposed that integrates the test-time adaptation procedure during training. This also strengthens the correlation between the self-supervised and classiﬁcation tasks. This new method is validated for different self-supervised tasks on the CIFAR10, STL10, and Tiny ImageNet datasets tasks, and indicates that their method can improve the accuracy of standard adversarial training under diverse white-box and black-box attack strategies.

Strengths:
+ The key concepts and motivations are described in enough detail to understand the paper.   This approach seems easy to implement and employ in practice.
+ Theoretical explanation for the necessity of the strategy and showing that it improves the robust accuracy of the test data. 1. We show that the estimators should be test-time adapted in order to achieve the Bayesian optimal adversarial robustness, even for simple linear models. And the test-time adaptation largely improves the robustness compared with optimal restricted estimators.
+ Extensive empirical results show the benefit of the method, and ablation studies are provided to characterize the approach.
+ The supplementary material provides additional proofs of theorems, implementation details, adaptive attacks, and experimental results that help support the paper.  Also, since the codes are also provided in supplementary material, there is less concern that the results in this paper would be difficult for a reader to reproduce.
Weaknesses:
- The theoretical analysis in Section 3 is dense and somewhat difficult to read.
- The experimental validation is lacking in some respects. The authors should justify their choice of baselines, datasets, and CNN backbones for experiments. Their proposed method should be also compared with SOA methods in terms of time and memory complexity.  There should be further analysis to assess the impact on the performance of class imbalance and batch size.
- It is unclear why offline FT sometimes yields lower accuracy than online FT with the proposed method.

+ The paper is well organized and their proposed approach is generally well described, although sometimes dense.
+ The approach appears to be novel and well-motivated.
+ The paper includes information on the method that would make it possible to reproduce the experiments.

Overall this is good quality submission.  The proposed method appears to be novel and well-motivated, although the experimental validation could be improved.

---


### Paper 3 (paper_id: vuD2xEtxZcj)

**Review 1:**

The paper aims to accelerate training by pruning the calculated gradients. More specifically, the authors propose 1:2 and 2:4 minimum-variance unbiased estimators and show their effectiveness on a number of tasks, where accuracy is not severely affected by the proposed pruning scheme.

The paper is well written and proposes very attractive improvements to training speed without affecting the accuracy.
Given a relatively general title, I'd be curious to see some results on using the approximate method in setups with different sparsity setups, e.g, 4:8, and also higher sparsity, e.g., 1:4.

The paper is well written and the results are presented in a clear way.
Proposed pruning scheme seems straightforward to implement, but having access to a reference implementation may still be valuable.

A well written paper with interesting and relevant results for practical applications of DL.

---

**Review 2:**

This paper develops a new scheme for pruning the gradients passed through a network during back-propagation. In particular, the proposed method designs a stochastic pruning operator which is unbiased and provides the minimum variance output. Both 1:2 (2 inputs, 1 non-zero output) and 2:4 (4 inputs, 2 non-zero outputs) variants of this operator were developed. In testing, networks pruned with the proposed operator demonstrated little-to-no degradation in performance across a range of network architectures and classification problems.


# Strengths

The paper provides a novel method for pruning the back-propagation gradients.

The proposed method does not hurt network performance, whereas naive greedy baselines hurt performance significantly.

The paper is well-written and reasonably easy to follow.

The paper provides a significant discussion of the implications the paper's results have on hardware acceleration and design

# Weaknesses
The paper doesn't make a strong case for why the purning operator needs to be unbiased and only points vaguely to Chmiel et al.'s finding that "for the neural gradients... it is critical to use unbiased quantization (i.e., Bias[θ(a)] = 0)."

For completely understandable reasons (no hardware support for pruning during training), the paper is unable to demonstrate any actual training time reductions. Hopefully this changes as new hardware is developed.

# Suggestion
The introduction of the MSE of the pruning operation describes $a$ as scalar, rather than a vector. The resulting discussion is then confusing as there's no stochasticity in $a$ and one is left asking why not just let $\theta(a)=a$. I suggest introducing $a$ as a vector from the start, as this will make the resulting discussion easier to follow -- there are many choices for the pruning operator which would produce different MSEs.


Besides the above suggestion, the paper is generally easy to follow.

The results demonstrate the proposed 2x pruning procedure doesn't hurt performance significantly.

The proposed pruning operator is, to my knowledge, novel and seems simple to reproduce from the provided equations.


The paper provides a simple gradient pruning procedure that could accelerate future hardware systems. While the form of the pruning operator could use further motivation, I'm overall in favor of this paper's publication.

---

**Review 3:**

The paper presents a method for a stochastic unbiased masking of the gradients such that half of them are 0 and they can be used to accelerate the matrix multiplications in an accelerator such as a GPU. In particular, the method induces a form of sparsity called N:M sparsity where N out of M consecutive (in memory) elements are 0. The authors propose to use a masking such that the gradients are still unbiased but also such that they have the minimum variance. Thorough experiments show that using their approximate 2:4 sparsity algorithm as well as the exact 1:2 algorithm, allows training a variety of neural network architectures on both images and text with minimal loss in final performance, if any. Moreover, the authors show that their algorithm can be combined with similar algorithms for the forward and backward pass as well as quantization without significant drop in performance.

Strengths
------------

- The theoretical analysis is well written and intuitive
- Accelerating GEMMs is a very important topic of research as it affects all modern neural network architectures
- Thorough experimental evaluations show that the method indeed does not hurt performance and can be used to train a variety of models

Weaknesses
--------------

- The main weakness of the paper is the lack of real world performance improvements due to lack of custom kernels or support from currently available accelerators. The speedup that is mentioned is a best case scenario and makes the method seem more useful than it is probably.
- Another weakness is the lack of comparisons with MSE based sparsity for the gradients. Even though previous work has shown that unbiased estimators are more important for the gradients than the weights, it would strengthen the paper significantly if instead of the greedy estimator the paper also compared with MSE optimal biased estimators.

Typos
-------

- eq. 8 typo it should be $sign(a_2) \left(\|a_1\| + \|a_2\|\right)$
- Fig. 2 typo $a_4 >> \max(a_1, a_2, a_3)$

The paper is well written with clear analyses and provides code for the proposed method.

The method is very intuitive and straightforward. It may be slightly incremental work, however I believe it is a clear contribution.

---


### Paper 4 (paper_id: -SBZ8c356Oc)

**Review 1:**

This work tries to develop an upper bound of the robust risk and design a new algorithm for
adversarial training called ARoW which minimizes a surrogate version of the developed upper bound.

Strength:
1. Well-written
2. Claim both theoretical and empirical contributions

Weakness:
1. **(1)** I guess Equation A.1 should be '$=$' and Throem 1 (and the proof) should be '$=$' (rather than '$\le$') for **binary classification problems**. Hence, Throem 1 is just another expression of 'R_rob = R_nat + R_bdy'. The second term in Theorm 1 is just another expression of boundary error rather than the upper bound of boundary error.
**(2)** For **multi-classification problems**, I guess Equation A.1 is still $=$ rather than $\le$ as $x'$ includes $z(x)$. **Could authors clarify why use $\le$ rather than $=$ here and what's the meaning of Lemma 2?**
**(3)** I also think the last line of last equation in Page 13 is $=$ (rather than $\le$) for binary classification problems, and $\le$ for **multi-classification problem** is meaningless, for me it seems like **loosing the equation and artificially creating a very loose bound** . I.e., use **($Y\ne F(z(x))$ is the sufficient and unnecessary condition of $p(Y|z(x))<0.5$ for multi-classification problem)** to bound the equation. This is the only technical part in this theorem, could we call this a new theorem and claim this a theoretical contribution (theorem 1 is even a simple expansion equation for binary case)?
**(4) In conclusion, for me, Theorem 1 seems like an equation for binary classification problem and an artificially-created loose bound for multi-classification problem. That is,
$R_{bdy}=\mathbb{E}[\mathbb{1}(F(X)\ne F(z(X)))\mathbb{1}(Y\ne F(z(X)))] \le \mathbb{E}[\mathbb{1}(F(X)\ne F(z(X))) \mathbb{1} (p(Y|z(x))<0.5)]$, which uses a loose and simple bound to bound the original equation. In my opinion, the $\le$ here is the only technical part in Theorem 1.
I cannot get the hidden meaning and theoretical contribution of this bound and think it is essentially meaningless. Could authors clarify what is the specific and profound meaning of Theorem 1?**
**Please point out my mistakes if I understand it incorrectly.**
2. In Sec. 3.2, the term 1 {pθ(Y |z(X)) < 1/2} is replaced by its convex upper bound 2(1 − pθ(Y | \hat X pgd)). This upper bound is loose, especially in a multi-classification problem (like CIFAR-10 in the experiments), this bound is meaningless as it is too loose.
**Thus, for me, the theoretical part in this manuscript seems like a created loose bound plus a created loose bound.**
3. Weak experiments and only a little improvement, need more empirical resuts (e.g., CIFAR-100, compare with AWP-TRADES)


The manuscript is clear.

I believe this paper is not good enough to publish in ICLR for the following reasons.
1. (main) Overclaim in theoretical part. Ref weakness 1, 2. I do not think it has sufficient theoretical contribution and its theorem can support the method.
2. (secondary) Weak experiments.

---

**Review 2:**

The paper proposes a modification of TRADES, one of the most popular algorithms to obtain adversarially robust classifiers, to improve its performance: in particular, the regularization term to achieve robustness is weighted to penalize more the training examples which are less robust. In the experimental evaluation on several datasets, the proposed method, ARoW, is shown to achieve better robustness that existing methods, while preserving higher standard accuracy.

Strengths
- The proposed modification of the TRADES loss, while small, is theoretically justified. The paper clearly presents the new scheme and its differences to existing algorithms.

- The experimental results support ARoW in comparison to existing methods. Most of the relevant baselines are included, and several ablation studies are added to analyze the effect of different components of the training algorithm e.g. label smoothing.

- The paper is well written, and it clearly presents the new method, the baselines and the experimental setup. The experiments include different architectures and datasets, and the case of using extra data for training.

Weaknesses
- The main concern is about the results reported for the baselines, especially for the case of additional data. For example, in Table 3, for the case of ResNet-18, HAT attains 56.40% and 55.44% of robust accuracy with the 500k extra images from 80M-TI and DDPM synthetic images respectively, while it is reported to get, for the same setups, 57.67% and 57.09% on [RobustBench](https://robustbench.github.io/index.html). Similarly, for WRN-28-10 with extra data, the model from [A] achieves 62.76%	of robust accuracy, higher than any method in Table 3. Then, it is not clear whether the baselines are optimally tuned.

- In general, the improvements over TRADES and HAT in terms of robustness are quite small, although consistent.

[A] https://arxiv.org/abs/2010.03593

Clarity: the paper is well written and clearly presents the method and the results.

Quality: the proposed method is well justified, and the set of experiments is reasonable.

Novelty: the modification to the TRADES loss is quite small, but relevant.

Reproducibility: sufficient experimental details and code are provided.

The proposed method is reasonable and shows promising results. However, clarifications about the discrepancy of the results for some baselines to the original ones are needed, as well as adding the missing baseline.

---
Update after rebuttal

Given the additional results and clarifications provided during the rebuttal, I increase the initial score to 6.

---

**Review 3:**

This work proposes a simple, effective, and theoretically inspired regularization to enhance the robustness of DNNs agains adversarial attacks.
Extensive experimental results were carried out showing the effectiveness of the proposed approach in providing robustness enhancements.

This appear has several strengths:

- The paper is is well motivated and the added regularizer is theoretically inspired.

- The experimental analysis show the consistent improvement of the proposed method over previous art.

- The paper is well-written. Further, the contributions of this work is placed properly within the literature.

- The wide broad of the empirical results shown in this paper covers many interesting aspects such as combining the proposed approach with AWP, and increasing the fairness.


There are few weaknesses that I hope to be addressed during the discussion:

- While the paper is generally well-written, there few parts that require small adjustments. For example,
In caption of figure 1: “ We exclude MART from the figures because its performance is too bad”

- Generally, the robustness improvements that ARoW provide is marginal. Would the proposed method improve the state-of-the-art model from [A]?

[A] Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples, 2021.

The paper clearly states its motivation, contributions, and places itself within prior art.

There several aspects that I like about this work such as the theoretical motivation, the extensive experimental evaluation, and the writing.
However, there are two concerns that  I hope to be addressed in the discussion period.

---

**Review 4:**

This paper a new adversarial training method to improve the robustness of deep learning classifiers in the field of computer vision. To do so, the authors derived a new loss function, which is the surrogate of an upper bound of the robust risk. Specifically, they point out the differences and connections between the proposed method and previous adversarial training method. Experiments on three datasets (CIFAR10, F-MINIST, SVHN) show that the proposed method outperforms other baselines in terms of clean classification accuracy and robust accuracy. Ablation studies are done to show the effect of different parts of the loss function. They also show that the proposed method can be combined with other adversarial training techniques, such as extra data, to further improve the performance. Finally, experiments on CIFAR10 show that the proposed method is helpful to improve the fairness of the classifier compared to TRADES (which is an important baseline).

Strength:

- Adversarial robustness is an important security issue in the field of deep learning. The proposed method pushes the SOTA performance of adversarial training to a new level.
- The proposed loss function is derived with a theoretical support.
- Extensive experiments show the empirical advantages of the proposed method from different aspects.

Weakness:

- The novelty of the method is ok, but it is similar to previous methods like MART. The author does point out the differences between the proposed method and other methods, so should not be a big problem.
- It's good that for table 1 and table 2 the results are based on 3 runs with standard errors given, but for most results, the improvements seem marginal.
- CIFAR10, F-MNIST, SVHN are all relatively small datasets. Does the method also perform well on larger dataset?


Clarity:

- The paper is clearly written and easy to follow.

Quality:

- The quality is good. The claims are clear and experiments are solid.

Novelty:

- Somewhat novel though similar to some previous methods.

Reproducibility:

- Code and implementation details are provided, so the reproducibility should be good. Though, I did not run the code to test.

Overall it's a good paper with with theoretical justification and experimental support. Though, there are some weakness in terms of novelty and the limitation of the datasets used, it is a paper above the margin.

---


### Paper 5 (paper_id: npwbjVljAEU)

**Review 1:**

Goals and contribution:
* The authors try to draw attention of the community to shallow recurrent neural networks trained by Hebbian learning in the context of cognitive material systems.
* They argue that these models could compete with deep-learning systems in the future  - thanks to upcoming advancement of in-materio technologies.
* They demonstrate this idea on an illustrative example of the N-bit (2-bit) parity function realized by the new-proposed SLIM (Shallow learning in Materio) model.
* The authors also point out several crucial open problems of the proposed approach as topics for future research.



Strengths:
1. I agree with the authors that the shallow recurrent higher-order neural networks are a promising area of future research - due to the progress of in-materio technologies.
2. The illustrative example is well-chosen. It demonstrates well how the proposed SLIM model could overcame the classical problem of n-bit parity (Minsky&Pappert,1988) - thanks to the higher-order units used instead of threshold logic gates.
3. The authors provide several open questions concerning their model as possible subjects of future research.

Limitations:
1. The paper is very brief  and vague in every aspect and its techniqual quality is imited.
2. There is no theoretical justification of the method (and its convergence) and the empirical evaluation is limited to a toy problem.

Discussion of the limitations:
1. The paper is very brief  and vague in every aspect and its techniqual quality is imited.
  * Mainly, the proposed model should be described in more detail, including training algorithm and analysis/discussion of the choice of the F_i functions.
  * The illustrative example also needs a more detailed description. How did you choose F_1,2 and why? Was the model trained by Hebbian learning?
    Can you show the concrete final model (its F-functions and weights)? Figure 3 is not comprehensible - could you describe more clearly what the four images represent?
2. There is no theoretical justification of the method (and its convergence) and the empirical evaluation is limited to a toy problem.
The paper addresses more limitations and problems of the proposed model than its advances.  Many important questions are not addressed:
 * Is it possible to extend the 2-bit parity model to the n-bit one? Are you able to show a solution for n=3?
 * The training process of the model is not described/analyzed.
 * The choice of F-functions for different tasks is not described/analyzed.
 * From the paper it seems like it is difficult to successfully  apply the model to different tasks. Is it right?


Originality:
* The main idea of the proposed approach - to apply the recurrent shallow neural networks in the context of in-materio technologies (enabling to use higher-order units) - is novel and interesting.
However, the realization is not very convincing (due to the lack of clear description, deeper theoretical or experimental evaluation or comparison to alternative approaches).

Related works seem to be cited adequately, except:
- Based on Section 1, the paper closely follows (Lawrence, 2022b). However, (Lawrence, 2022b) seems to be unavailable (the link doesn't work and I was not able to find it elsewhere)
 - Because the authors concentrate on the n-bit parity function realized by shallow networks, they should compare their approach to some alternative/similar models and cite them (e.g., [1], [2]).



Clarity and reproducibility:
* Both the proposed model and the experiment should be described in more detail.
There are many open questions that hinder reproducibility (e.g., missing description of the training process, missing detailed description of the used F-functions,...).
* Figure 3 is not well-described and thus harly comprehensible.

Quality:
* The technical quality of the paper is poor. The paper looks like an incomplete work in progress (see "Strength And Weaknesses" Section for details).

Minor:
* (Rumelhart et al., 1986) is cited as a reference to TPU-based architectures, isn't that an error?

[1] Aizenberg, I. Solving the XOR and parity N problems using a single universal binary neuron. Soft Comput 12, 215–222 (2008). https://doi.org/10.1007/s00500-007-0204-9

[2] Iyoda, E.M., Nobuhara, H. & Hirota, K. A Solution for the N-bit Parity Problem Using a Single Translated Multiplicative Neuron. Neural Processing Letters 18, 233–238 (2003). https://doi.org/10.1023/B:NEPL.0000011147.74207.8c




Although the main idea of the paper is novel and interesting, its realization is poor. The proposed model is not well-described and it is not adequately empirically or theoretically justified.

---

**Review 2:**

The authors introduce a structural variant of single-layer perceptrons with added recurrent connections. The model seems to be mostly a standard RNN, with the different of a more general activation function.
No evaluation is provided, aside from a hand-designed example.

Strengths:
- The paper is short, which makes it easy to read.

Weaknesses:
- The main problem tackled in the paper is not a particular concern since the early 80s and is largely solved.
- The structure of the paper is unorthodox and lacks most of the required content (concerning the theory and the model) and any practical evaluation.
- Figure 3 is difficult to understand, as no explanation is given about the notation in the text nor in the caption.
- The proposed architecture seems to be a standard RNN with a more flexible activation function, which however the authors leave undefined.
- Related literature is severely lacking, both to justify the significance of the problem addressed and all the work done on it since the 80s.


The paper is written poorly and in a non-standard way, and cannot be accepted in its present form.
The work presented does not seem novel, or it is only marginally novel.
Almost no implementation details are given, and the only example is designed by hand.


N/A

---

**Review 3:**

SLIM is intended to be a perceptron that involves a minimally connected recurrent network. The internal state variables have a nearest-neighbor interaction on a chain. The authors conjecture such networks could realize arbitrary N-bit Boolean functions, in contrast to the limitation of Rosenblatt perceptrons, as shown by Minsky and Papert.

Strengths:
The paper explores alternatives to the deep learning architecture and hopes to have self-organized nanomaterial-based learning machines.

Weaknesses:
The key idea is not so novel. From a multitude of recurrent neural networks (including the reservoir computing framework, mentioned by the authors) to predictive coding networks, many approaches utilize the idea of a network with internal dynamics, the equilibrium point of which helps perform the task.

There is very little work done to substantiate the claim for the particular architecture proposed.


The purpose of the paper is clear. Quality, novelty and reproducibility is hard to comment on since there is so little material.

This paper would need much more work to rise to the level of being worthy of consideration for publication at ICLR. At this point, it is just a scheme, and not a particularly novel one.

---

**Review 4:**

The paper suggests an alternative network architecture, but lacks analytical and numerical evaluation.

Strength:
* Out of the box

Weakness:
* Lack of box

Novel, but might profit from "a gifted mathematicians to make new connections or help neuroscientists in unravelling the mysteries of small-scale brains."


Ingenious, but in lack of a theory and of numerics

---



---

## Year 2023

### Paper 1 (paper_id: uXBIfzhu9T)

**Review 1:**

This paper proposes a method for estimating treatment effects in the presence of multiple outcomes and multiple treatments and shared unobserved confounders. The idea extends the setting in treatment effect estimation from multiple treatments, by Wang and Blei 2019. The proposed estimation algorithm involves a two steps paradigm: first identifying the proxy variables using causal structure learning, and then conduct inference with existing estimators. The methodology is further supplemented by identification results, along with synthetic studies and an experiment using the MIMIC III dataset.

1. The motivation of causal effect estimation in presence of multiple treatments and outcomes, and especially utilize the multiple treatments/outcomes as proxies is novel and significant.
2. The identifiability results seems solid and does not require pre-specified proxies. Although I have not checked the technical proof details.

1. The experiment evaluation is weak when compared to other causal inference methods submitted to ICLR or similar AI/ML conferences. None of the benchmark neural network-based treatment effect estimation methods have been included in the baseline, e.g., CFR, DR-CFR, CEVAE, TEDVAE, etc. Most of these mentioned methods have been published in AAAI/ICLR/NeurIPS. They are not designed for multiple outcomes/treatments, but comparing with these methods can give a clear practical implication to the proposed method.
2. This paper is written towards an audience of applied statistics researchers, and is more suited for a statistical journal such as Biometrika. Although the reviewer acknowledge the importance of rigorous discussing identifiability, the methodology may not be very relevant to the broader community of ICLR. For example, it does not discuss neural network or representation learning at all.
3. The method requires an additional step of causal discovery for learning the causal structure.
4. The two-step algorithm design may be suboptimal, especially when the estimation in the second step cruciallly depends on the causal discovery results in the first step.

What if (i) of Assumption 3.1 is not satisfied? Or specifically, does $\mathbf{U}$ has to be the same for all outcomes? Can a subset of $\mathbf{U}$ affect some outcomes, and other subsets affect other outcomes?

---

**Review 2:**

This paper considers a setting with multiple treatments and multiple outcomes. They show that parallel studies of multiple outcomes involved in this setting can assist each other in causal identification, in the sense that one can exploit other treatments and outcomes as proxies for each treatment effect under study. They proceed with a causal discovery method that can effectively identify such proxies for causal estimation.

The work is pretty sound. The idea of using multiple outcomes and treatments as proxies is interesting.

My main concern is about the inference on selected proxies. Have the authors considered post selection problem as the convergence results considered in the Appendix seem to assume a valid Z, W and ignore the selection step?

If there are multiple Z and W satisfy (2) & (3), how shall I pick?

Is Assumption 4.6 a strong Assumption?

---

**Review 3:**

The paper "The Blessings of Multiple Treatments and Outcomes in Treatment Effect Estimation" proposes to extend existing results relying on multiple treatments or multiple outcomes to provide additional adjustment and better enforce the ignorability assumption in the presence of unobserved confounders. The authors propose a theoretical proof to demonstrate the identifiability of the average treatment effect (ATE) and an algorithmic strategy to estimate the ATE in that setting. Finally, experiments on simulated and real data are conducted to illustrate the performances of the new proposed approach.

The article is well written, and technically sound. The proposed approach is interesting, with great potential for practical application. The previous literature is well integrated, and the paper's novelty is well explicited.

1. The proposed extension is novel, and it is very valuable for further practical application to have a sound technical ground, however, the idea is not revolutionary, but rather a natural extension of existing work.

2. The deconfounder approach (Wang and Blei) has faced extensive criticism, with a clear description of settings where this approach is relevant, and of settings where it fails. A similarly clear description of the application field is needed for more applied practitioners.

3. The final "algorithm" is very hard to understand from the sole reading of the article, as many parts of the implemented strategy are just referred to as citations. It would greatly improve the usefulness of the article to provide a clear outline that could be implemented (probably in Supplementary materials). A graphical illustration of the steps (causal discovery, and then estimation) would also really help the readability of the article, especially for applied researchers.

4. The authors should be commended for providing source code for their work, however, it does not seem to be oriented towards being an actual package that could be installed, and used easily by anyone (no mention of the code in the article, maybe in a reproducibility section as suggested in the ICLR author guide, no documentation, such as a README or a tutorial)

main questions
5. The experiments should help clarifying when to use or not use the proposed approach, but the only simulated setting concerns a case where both multiple treatments and outcomes can be leveraged to deconfound. How does the proposed approach perform compared to the deconfounder and POP when only multiple treatments or multiple outcomes can be used?

6. Can you provide the results of all methods to the real MIMIC data application? It would provide insights about the differences in the final results in a real-world application, in addition to simulated data designed to illustrate the benefit of the proposed approach in a favorable case.

7. What would be a way to provide an estimation uncertainty for the ATE in the proposed method? Beyond the estimate, that is an interesting metric to report on real data, for the proposed approach and the baselines.

8. What is the relative required dimensionality of the treatment and outcome compared to the unobserved confounders dimension?


details
9. duplicated bibliography item "Yifan Cui, Hongming Pu, Xu Shi, Wang Miao, and Eric Tchetgen Tchetgen. Semiparametric proximal causal inference."
10. the associated code should be mentioned (maybe in a reproducibility section as suggested in the ICLR author guide)
11. the associated code is not documented. A readme, a tutorial to use etc should be included
12. the proposed method does not have a name
13. PMMR meaning is not defined

---

**Review 4:**

The authors extend earlier work on multiple treatments and single outcomes to the more general setting of multiple treatments and outcomes. They use proximal causal learning to show causal identifiability and use a hypothesis testing approach to find proxies which can then be used to estimate the causal effect. They demonstrate their approach on a synthetic and a clinical dataset.

The authors theoretically motivate their approach and demonstrate it works on two datasets.

The paper is generally easy to follow.

The experiments are modest in size with relatively simple causal relationships. It's not clear if their approach would work with more complex datasets and relationships. For instance, as I understand the authors assume that there is no interaction between treatments and outcomes. To what extent would this approach still work if this assumption was violated?

How would the proposed method scale to larger datasets with more complex causal relationships?

---


### Paper 2 (paper_id: BUDxvMRkc4)

**Review 1:**

This study introduces a framework designed to enhance CLIP in addressing long-tailed visual recognition challenges. This framework integrates a supervised contrastive loss mechanism, grounded on the transport plan, to fortify visual feature extraction. Several evaluations conducted on benchmarks corroborate that this proposed method significantly facilitates discriminative visual feature learning and achieves SOTA performance in long-tailed recognition tasks.

1. The idea of this paper is clear and easy to follow.
1. Experimental results show the effectiveness of the proposed method.

1. Lack of innovation. The approach in this paper provides a more balanced prototype for visual pre-training models to guide the learning of visual feature extractors and designs supervised comparative learning loss. However a similar approach has appeared in previous long-tail methods [1]. The differences in this paper are: 1. A more robust pre-training model, CLIP, is utilized. 2. A text-based prototype design approach is used to replace the target anchor. These innovations are more limited.

2. Some modules are without good motivation.
For example:
- Why do we need a learnable linear classifier? The purpose of its existence seems to be the matching of visual features with textual features. However, the weights of the classifier will change during training, which does not narrow the gap between template label text and image feature distributions.
-  In the unsupervised prototype-guided feature learning part, why choose cos similarity as the distance metric instead of other metric methods such as minimum entropy?
-  For the modules of learnable classifier, unsupervised prototype-guided feature learning, and supervised contrastive loss, it seems to be an incremental improvement and not complementary, so why use them to train models together?

**Reference**
[1] Li T, Cao P, Yuan Y, et al. Targeted supervised contrastive learning for long-tailed recognition[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 6918-6928.

please refer to Weaknesses.

---

**Review 2:**

This study discovers that the fine-tuned CLIP's textual features are more balanced and discriminative compared to its visual counterparts. Building on this, the research proposes utilizing balanced textual features as prototypes to guide the learning of robust representations for biased visual features. The CLIP is further fine-tuned through contrastive learning, followed by the optimization of biased visual representations using linear adapters and the introduction of optimal transport distance to help decouple biased visual features. Additionally, a supervised contrastive learning loss based on the transport plan is designed. Experimental results indicate that the approach excels in leveraging visual-language information for imbalanced visual recognition, achieving state-of-the-art performance.

1. Extensive experiments on ImageNet-LT, Places-LT, and iNaturalist 2018 have demonstrated the effectiveness of the proposed method.
2. Comprehensive visualizations and ablation studies were conducted to validate the impact of the proposed method.

1. The experiment results indicate that the method underperforms for “many” classes in long-tail data.
2. The proposed method employs a two-stage training process and fine-tunes the Full-CLIP, which requires significant computational resources and has a prolonged training duration.
3. The proposed method doesn't seem to have a specific design tailored for long-tail data. The approach of using textual features as guidance for better image features can be applied to situations with limited image feature quality for various reasons, such as long-tail, few-shot, noisy data, generated data, low-resolution data, and so forth.

1. Why does the proposed method underperform in “many” classes of LT dataset? An analysis of the underlying reasons would be appreciated.
2. The experimental results show that the proposed method underperforms in “many” classes of LT dataset. Does this imply that the method is primarily effective for situations with the few-shot scenario (i.e., “medium” and “few” classes in long-tail datasets)?
3. Balanced sampling is a fundamental operation in long-tail methods. Why is random sampling used when fine-tuning the CLIP in the initial stage? Is it to intentionally obtain a CLIP encoder with strong biases caused by imbalance?
4. Reference [1] also leverages CLIP's text features to enhance the discriminative power of image features. In [1], directly using text features and image features in concurrent training a linear classifier can achieve significant improvements in few-shot tasks. However, compared to the method in this paper, the method in [1] is much simpler, with much faster computation speeds and far less computational overhead.
[1] https://arxiv.org/abs/2301.06267

---

**Review 3:**

After the advent of vision-language pre-training, numerous works have adapted the pre-trained vision-language model to various vision tasks, including long-tailed recognition. This paper first presents empirical evidence that textual features remain balanced even after fine-tuning in the context of long-tailed classification. Based on this, the authors propose a framework that leverages balanced textual features as a guide to obtain more robust visual features.

1. The empirical finding that, during the fine-tuning of the entire vision-language pre-trained model on long-tailed data, textual features tend to achieve balance is quite intriguing. This paper goes beyond this observation and contributes to the community by proposing a concrete methodology that leverages balanced textual features to rectify imbalanced visual features.
2. The thorough ablation study conducted on the elements comprising "Phase B," proposed in this work, effectively underscores that the suggested $L_{\text{OT}}$ and $L_{\text{SCT}}$ indeed enhance performance.

1. The overall structure of this paper, which deals with challenges in contrastive learning methods due to class imbalance and suggests remedies, evokes thoughts of Suh and Seo (2023). Nevertheless, the current paper does not include any discourse on this topic.
2. Moreover, while one could mention Kang et al. (2021) as a seminal work on achieving a balanced and discriminative feature space in long-tailed classification scenarios, this is also not discussed.

---
Kang et al., 2021, Exploring Balanced Feature Spaces for Representation Learning.
Suh and Seo, 2023, Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels.

1. Could you offer some informed speculation about why there is a tendency for textual features to be balanced?
2. Since comparing performance between different architectures does not hold much significance, it would be better to provide results for RN50 and ViT-B/16 in separate groups.
3. Does the proposed approach result in any additional training expenses? For instance, what are the costs associated with setting up an optimal transport plan?

---


### Paper 3 (paper_id: n3z5oALWci)

**Review 1:**

The paper proposed a general framework for adapting various explanation techniques to models that process variable-length inputs, expanding explanation coverage to data points of different lengths to address limitation of existing model-agnostic general explanation techniques do not consider the variable lengths of input data points, which limits their effectiveness.

1)	This paper proposed to incorporate temporal information in local explanations to machine learning models that can capture temporal information in the inputs, which makes these explanations more faithful and easier to understand.
2)	This paper proposed a general framework REX to automatically incorporate the above information in popular local explanation techniques.

1)	The experimental part is not very sufficient, and the comparison models used are not many and not the most advanced.
2)	After adding the REX framework, the efficiency of the model has decreased significantly, and I feel that it is not very practical.

1)	After adding the REX framework, the efficiency of the model has decreased significantly. Does this have practical application value?
2)	The reference mentioned in the related work seems to be several years ago. Is there any related work in recent years?
3)	Which models correspond to the methods in Table 4?

---

**Review 2:**

This paper introduces a framework, REX, designed to offer model-agnostic explanation techniques for variable-length input data. The framework utilizes both 1-D and 2-D predicates to address the relative positions of individual features within the input data. REX can be plugged into other explanations directly. Specifically, REX is shown to extend the capabilities of Anchors, LIME, and Kernel-SHAP. The efficacy of the proposed method is evaluated on two datasets using metrics including coverage/precision, accuracy, and AUROC. Notably, coverage/precision is also employed as a measure of human interpretability. The results affirm that REX not only enhances the fidelity of explanations but also improves human understanding without imposing additional computational overhead.

1. This paper addresses an important research question concerning the impact of temporal relationships between features in the input on the quality of explanations for model decisions. The proposed explanation method is versatile enough to handle varying input lengths, an advantage over traditional model-agnostic explanation techniques. The examples provided offer compelling motivation for the algorithm's design.

2. The experimental evaluation of REX is thorough, covering two distinct tasks: sentiment analysis using multiple language models, and anomaly detection using RNNs. The results are analyzed through a variety of metrics, including both automatic measures of fidelity and human-centric metrics for understanding. The performance improvements achieved by REX are promising.

1. Algorithm for "Extending Vocabularies": The paper would benefit from a more detailed explanation of how 1-D and 2-D predicates are extracted from the model inputs, as this is a core component of REX. Additional analyses on the algorithm for extending vocabularies would be beneficial. Specifically, do all 1-D and 2-D predicates positively impact explanations? Including more examples beyond those discussed in Section 3.1 could strengthen the paper's argument for the advantages of REX. Additionally, incorporating another textual dataset could bolster claims regarding REX's generalizability.

2. Details on Human Evaluations: The section on human evaluations lacks some details. For instance, is the experimental design a within-groups setup? If so, biases could arise from the order in which explanations from different methods are presented. What is the precise procedure for the user study? Does each set consist of ten new sentences used specifically for that test block? Furthermore, exploring challenging tasks, such as instances where the model makes incorrect predictions, would offer additional insights into human understanding of model reasoning. If humans can also make “wrong predictions” made by the model, it is very convincing that users understand the model's reasoning.

3. Broader Impact of REX: Could REX be generalized to image inputs, particularly simple image sequences with important temporal information for classification? A discussion on the generalizability of REX’s two core modules—Extending Vocabularies and Perturbation Models—could enhance the paper.

1. Analysis of Table 4: The interpretation of Table 4 in Section 4.3 lacks precision. Specifically, the claimed improvements in precision and coverage, cited as "80.9% and 56.7%" in the text, cannot be observed from the results in Table 4.

2. Question on Input Length: What is the average length of the textual input data? If the text input data consists of long paragraphs, would that impose computational burdens on REX?

---

**Review 3:**

This work extends existing explanation methods (Anchors, lime, Kernel Shap) to augment temporal information in the explanations. Temporal information is the explanations is defined with upto 2 features by highlighting the position/distance between features (e.g. position of feature K - position of feature L >= 3). The explanations are evaluated on text and time series data (which is the target domain of such explanations) which show improvements in metrics as well as a user study involving 19 individuals. In the context of explainability, temporal information has earlier been used in timeseries data related to shaplets, albeit in a different manner.

Overall, the idea of incorporating temporal information using position/distance between features is simple and novel. Temporal information is useful in general and it does help in minimizing the ambiguity in existing explanations as shown by authors for text and time series examples. The fact that existing explanation methods can be extended simply by using an alternate perturbation approach as claimed by authors, is useful.

- Empirical evaluation can include more models and datasets.
- The user study could be extended in size and diversity of users. Although positional/distance between features is useful, in case of timeseries the shape of a curve (e.g. shaplets) provides richer information and more useful information to SMEs. It might be good to present use cases where SMEs value the distance information in specific domains.

- In case of LIME-Text explainer (equivalently for SHAP), words of a sentence are randomly deleted to obtain different binary vectors (1= word present, 0=absent) in order to compute distance between original & perturbed samples and fit a linear model. If the words are shifted, then the binary vectors remain unchanged. It was unclear from text how temporal information is recovered in this setting using the existing ridge regression model used by LIME?

- Compared to the base case of no temporal information, how do (a) number of perturbed samples and (b) number of parameters in the linear model fit by lime/shap, scale when temporal information is requested from the explainer?

---

**Review 4:**

The authors propose REX, a framework that incorporates "temporal information" in local post-hoc explanations of DNNs that can take varying-length sequence data as input.

Specifically, REX provides explanations over a vocabulary of "feature predicates" that specify temporal relationships between features, e.g. "the token at index i is 'never', the token at index j is 'fails', and 'never' occurs immediately before 'fails' (j - i = 1)".  The authors illustrate via examples to calculate existing post-hoc explanation techniques (such as LIME and Anchors) over this new vocabulary of predicates.  The authors demonstrate the value of their new approach by arguing that it results in improvements in fidelity measures and users' performance on a forward simulation task, compared to "naive" application of explanation techniques like LIME and Anchors.

1. The authors' predicate definitions (Def. 3.1 and 3.2) are intuitive to interpret (e.g., the 2D predicates can be used to specify the number of tokens between two particular tokens in a sentence).  The authors illustrate the potential utility of attributing importance to such predicates rather than individual features in Figure 1.
2. The authors' experimental results (Table 2 and Figure 4) demonstrate the potential value of REX. The explanations provided by REX have higher coverage, precision, and are more accurate surrogate models compared to the naive explanations provided over the original feature set.

* **Weakness #1: Clarity.** My primary critique of this work is that I found the present draft difficult to understand.  The notation used was not sufficiently explained, and I found the authors' descriptions of their methodology and experiments to be severely underspecified.  Unless these details are clarified, I do not believe this draft is ready to be published in its present state. Specifically:
  * Section 2 (Notation). The description that you provided in the second paragraph is confusing, and does not clarify exactly how to interpret the notation.  Specifically, what is $d$: is it the order in which the token appears in the sentence? What is the minus sign notation (what is $Pos_g$, and how does it differ from $Pos_f$)? What do the numbers mean (e.g., why is there a 2 in the statement $Pos_{fails} - Pos_{never} >= 2$?)
  * Section 3. Your notation is under-specified. It may be helpful in this section to clarify what the 'features' of the example inputs you presented are (e.g., what are the 'features' of the sentence input "Bob is not a bad boy")? Is $f_j$ in this case the token that appears at index $j$ in the sentence, or something else? Similarly, do your feature predicates $p_j$ compare the value of token at index $j$ to some threshold (e.g. "the token at timestep $t$ = 'fails'", or something else? (EDIT: After reading Section 3.1, it seems like $f_j$ is the value of the token at index $j$?  Can you clarify?)
  * Section 3.2. "Extending vocabularies: I don't understand what is described in this paragraph. What do you mean by 'serve as a feature of an input', and a 'method to evaluate the predicate on a given input'? Do you mean that you construct a new set of indicators for each original datapoint where you evaluate whether the predicate holds for that datapoint, and then learn each "surrogate model" using the set of original features plus the predicates?
  * Section 3.3. I am confused from the explanation provided about how the perturbation model that you've described is used for each of the individual explanation methods.  It is unclear to me how this perturbation method, evaluates the possible importance of all of the possible predicates (from the many possibilities that exist).  Take LIME as an example. I am confused about how being able to generate new datapoints where features are "switched" (for example, say we switch $f_j = c_j$ and $f_i = c_i$, allows us to assign an "importance score" to, for example, the predicate $f_j = c$ AND $f_i = c_i$ AND $i - j >= 1$.  Can you clarify exactly how you calculate the importance scores for each of the 1D and the 2D temporal predicates using such perturbation methods?
  * Section 4.3. Can you clarify how you assigned participant to explanation conditions in the user study? Did a single participant only see explanations from 1 of {Anchor, Anchor*}?  Did you present a single "test", and then ask them to simulate the model on 10 sentences, several different times (so elicited 50 total predictions), or show all 5 "tests" right away and then collect 10 predictions per user?
  * Section 4.3. Can you provide screenshots of your study interface (e.g., how you presented the Anchors explanations to users)? I wonder if simply highlighting the most important tokens identified by Anchors within the existing sentence, like in Figure 4 of [1] (rather than just showing them the rule) may result in similar performance increases as showing them the Anchors* explanation.
* **Weakness #2: No comparison to popular existing post-hoc methods for sequential data.** In your Related Work and in the paper's Introduction, you compare post-hoc feature attribution methods like LIME to DFA methods that explain RNNs.  However, there have been many other post-hoc explanation method techniques that have been proposed to explain sequence models like transformers (most notably, attention [2]). Can you provide additional motivation for the benefits of your proposed method over other existing popular methods?

[1] https://arxiv.org/pdf/2302.08450.pdf

[2] https://arxiv.org/abs/1908.04626

See the listed Weaknesses for my high-priority questions.

Some additional comments/suggestions I had that are less relevant to my score are:
* Introduction: "the explanations can still be complex as RNNs often fail to internalize 'the perfect regular expressions' and contain noise". Why does failure to learn the true regular expression imply that the network is difficult to explain?
* Figure 2: I am confused about this example.  My understanding is that the anomaly data point is 428, and both Anchors and Anchors* identify only datapoints before this anomaly point.  Wouldn't a more suitable explanation include both datapoints that come before, and after point 428?  Is the problem set-up here that you must predict if timestep $x_t$ is an anomaly, given only the observations that came before it?
* Section 2, nit: isn't it more appropriate for the local explanation to be a function of $f$, i.e., $g(x, f)$?
* Section 3. "Anchors is a conjunction which must evaluate to true on $x$".  Can you define what a "conjunction" is inline?  What do you mean by "evaluate to true on $x$"?
* Section 3.1. Can you provide intuition for the explanation with the "I hate that man…" example?  Why does it make sense for "j >= 3" to be the explanation? This seems less important than "but" coming before "love", or after "hate"?
* Section 4.1. Can you provide more detail about the anomaly detection ECG dataset? Is there only a single "feature" being measured at different timesteps? What is the prediction task here (is it given the measurements $x_1, …, x_{(T - 1)}$, to predict if $x_T$ is anomalous)?
* Section 4.2. Your paper says that "REX improves the coverage by 98.2%".  Do you mean that $1.982 x = y$, where $x$ is the average coverage across all of the models and explanation methods before REX, and $y$ is the average coverage across all of the models and explanation methods after REX?
* Section 4.2. I don't understand the intuition for REX's runtime. You state, "REX does not require a larger number of sampled instances".  Why is this the case?
* For your "it's not a bad journey at all" example in 4.2, I think your explanation is incorrect (the token "good" doesn't appear in the original sentence).
* Section 4.3. You state that users "would ignore input length constraints" when "misusing [the original] Anchors' explanations".  But unless I am mistaken, the original Anchors did not include predicates over the input's length? Can you clarify what you mean by this?'

---


### Paper 4 (paper_id: TmcH09s6pT)

**Review 1:**

The paper introduces the concept of "generalized neural collapse", which extends the existing understanding of neural collapse in deep classification models to cases where the number of classes is much larger than the dimension of the feature space. The study reveals that when a generalized neural collapse phenomenon occurs, one-vs-rest margins is maximized. The authors provide theoretical studies for CE loss with diminishing temperate, and under some assumptions show that the optimal solutions satisfy NC1 to NC3. The authors conduct experiments to verify the theory, and propose the method of class-mean features (CMF) classifiers to reduce training memory cost.

1. This paper provides a solid theoretical study of NC for a large number of classes. Specifically, the authors reduce the problem of minimizing CE loss to the HardMax problem, and prove that NC in this setting exhibits a nice combinatoric structure called softmax code. Overall, the theory part is well motivated and clearly presented.
2. The experiments are well aligned with the theory. The proposed CMF classifier seems effective and efficient.

1. I still have concerns on the justifications for using a small temperature. The authors cite two papers [1,2] to support their claim. I am not convinced this is a general practice for the majority of works in this field. Also, I cannot find the exact number of $\tau=1/30$ in [1] claimed by the authors. In [2], the chosen loss function is negative cosine similarity, and I can not see how this is connected with CE with small temperature. Furthermore, I think considering only the diminishing temperature case oversimplifies the problem, since in this case, the problem intrinsically has a max-margin structure which aligns well with the desired NC properties. However, I cannot see why this is also true for general CE loss functions.
2. For GNC3, it is required that the Tammes problem and softmax codes are equivalent. The authors show that this is true for trivial setting, and conjecture it to be true for other settings. Since their separation metrics are quite different, I am not convinced that these two configurations are equivalent. As the experiments tricks in Section 5 are grounded on GNC3, I recommend the authors to make more clarifications on this matter.
3. In the related works part, the authors claim that two of the related paper focuses on networks with weight decay, which is not required in this paper. However, this paper assumes that the weight and feature vectors have unit length, which is stronger than the assumptions of using weight decay. Therefore, I can not see why this makes a big distinction.

[1] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition.

[2] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning.

See the weaknesses part.

---

**Review 2:**

I previously reviewed this paper in NeurIPS 2023. At that time, I thought this work was the standard for acceptance, as it received a score of 76543. However, I noted that two reviewers with lower scores did not provide an objective description of the paper's contribution.  In particular, recent studies in a similar direction tend to focus on Neural Collapse when the number of categories is smaller than the feature dimension.  The contribution of this work undoubtedly broadens the research horizon and is sufficiently significant for acceptance. In the current ICLR submission, while the core content remains largely unchanged, some questions I raised during the NeurIPS 2023 preview process have been addressed, and certain revisions have been implemented in this submission. Consequently, my review remains consistent with my previous assessment.

- This paper provides a clear view of the global optimal conditions for generalized neural collapse, especially for a large number of classes, while the existing work often considers a closed-form case (i.e., a simplex of ETF) in which the number of classes is smaller than the feature dimensionality. Therefore, the contribution of the work undoubtedly broadens the horizon of Neural Collapse.
- The paper's crucial contribution lies in its demonstration that minimizing the asymptotic CE loss is equivalent to achieving within-class variability collapse and aligning with softmax codes, providing valuable insights into the underlying behavior of training neural networks.

- The theoretical results primarily emphasize the asymptotic CE loss rather than the original CE loss, potentially limiting the direct applicability of the findings to real-world scenarios.
- The features and class weights are constrained on the unit sphere.
- There are some minor typos, such as "where the number of classes are ... which occur" should be corrected to "where the number of classes is ... which occurs"
- Incorrect cite command in the paragraph 'Related Work' and others

Please see weakness.

---

**Review 3:**

The paper delves into the mathematical characterization of learned last-layer representations and classifier weights in deep classification models, known as the "Neural Collapse" (NC) phenomenon. While existing studies on NC often focus on scenarios where the number of classes is small compared to the feature space dimension, this paper extends the understanding of NC to situations where the number of classes is much larger than the feature dimension. The authors introduce the concept of "Generalized Neural Collapse" (GNC) and demonstrate that features and classifiers display a GNC phenomenon where the minimum one-vs-rest margins are maximized. The paper provides both empirical and theoretical studies to validate the occurrence of GNC in practical deep neural networks.

+ The paper is well-organized, allowing readers to easily follow the author's ideas. The writer makes sure the content is clear and simple, so readers from different backgrounds can understand the main points.
+ The author expanded the "Neural Collapse" (NC) idea to the "General Neural Collapse" (GNC), considering cases where the number of categories is greater than the feature size.

+ **Too strong assumption in the theoretical analysis**: $\tau \rightarrow 0$ means the norm of the feature goes to infinity, which means that a small angle between every two classes can enjoy the CE closing to $0$. Therefore, practically, GNC can not be achieved when $\tau$ is very small.
+ **Insufficient Discussions**: The Tammes problem is the limit case of Thomson-P problem [1], authors should also consider it and provide more discussions for it, especially Thomson-1 problem.

[1] Numerical Solutions of the Thomson-P Problems

None

---

**Review 4:**

The paper investigates the neural collapse geometry of CE loss with normalized features and classifiers in the case of large number of classes K>d+1. They formalize two key properties (in addition to the vanilla NC1) describing the geometry: (i) Classifiers converge to an arrangement (which they call softmax code) of K vectors on the d-dim sphere that maximizes the minimum distance of one of them to the cvx hull of the rest (ii) Embeddings align with classifiers. Empirically, they demonstrate that property (ii) holds and property (i) holds for small enough temperature parameter on the CE loss. They also show that small temperature leads to good generalization. On theory front, the authors confirm the formalization by studying the corresponding UFM with temperature tending to zero. Finally, motivated by property (ii) they propose methods that avoid training the classifier weights showing that retain good performance while being less heavy on compute.

* important problem / open question: NC geometry for large K
* technical results appear solid although I did not have time to fully check the proofs
* result is to my knowledge novel

* the requirement for feature/weight normalization might seem restrictive. The authors mention that this is standard practice, but looking at the three references, only one of them is on CE loss.
* the description of geometry is elegant, but is in general not explicit. Instead given as solution to a non-convex problem. It nevertheless gives an implicit way of thinking about the classifiers geometry. For d=2 it is nice to have the closed-form, but perhaps uniformity on the sphere is not very surprising (what else could it be?)
* the characterization does not specify uniquely how the classifier weights are assigned to classes. The authors discuss this under the assignment problem in Sec 4, but there does not yet appear to be a concrete answer to that

Can you please elaborate on how your findings compare to the results by Liu et al (2023) and Gao et al (2023)? From the Related work part in Sec. 1, I understand you are claiming that the geometry with spherical constraints is different compared to that with weight-decay. Is this the case? If so, I believe it would be useful/interesting to elaborate on the differences.

Also wondering if you have thoughts on this: Is the use of spherical constraints more of a mathematical convenience to arrive at a more "clean" NC geometry description OR is the claim that this is a practice actually preferred over weight decay?

Thm 3.3 last statement: Is cross-polytope the only solution?

minor: "The following result shows that the requirement in the Theorem 3.5 that k is not a rattler is satisfied
in many cases"  I would say "d=2" and "K<=d+1" is *some* cases

I recommend ploting GCN1 in log scale

Can you elaborate on the numerical optimization of Softmax Code?

Computing \rho_{one-vs-rest} looks computationally expensive for large k (solving k quadratic programs). Can you please comment on that?

In addition to GNC2 metric, is it not possible to track a metric that more accurately captures the geometry? Eg. angles between classifiers. At least for cases that you know the geometry of Ws like K<2d?

The last subfigure in Fig 2 shows that generalization performance improves with decreasing temperature. Could you please comment on whether the best value achieved is also on par with the sota value for CE training without normalization and temperature scaling (which I believe is the "standard" practice?)

PS: In general, I enjoyed reading the paper and the results. I am inclined to increase my score to an 8 after the rebuttal.

---


### Paper 5 (paper_id: OHpvivXrQr)

**Review 1:**

The authors introduce an interesting task of multimer structure prediction in the form of assembly graph where each node is a monomer and each edge represents an assembly action. They propose to first pretrain a model to predict the TM-score between the structure obtained from the given assembly graph and the ground truth, then finetune with prompt and meta-learning to perform link prediction to construct the assembly graph step by step. The prompt is crafted with $l=3$ path to form a 4-node graph so that the link prediction can be implemented as graph-level prediction on small graphs which is well aligned with the pretraining phase.

1. The paper is well written and clear. I really enjoy reading the paper.
2. The paper introduces an interesting task (i.e. multimer structure prediction) to the community with clear formalization (i.e. prediction of the assembly graph given pairwise dimers).
3. The experiments are solid, testing the performance on multimers ranging from 3 chains to 30 chains. The authors also compare the results when given ground-truth dimers or alphafold-predicted dimers as inputs. The results are promising, exhibiting obvious improvement over baselines.

1. The $l=3$ graph prompt is proposed to tackle the distribution shift of chain numbers. However, I notice that in section 4.2 the initial embeddings are obtained from the last layer of the pretrained GIN encoder with the full assembly graph as input. This step may already suffer from the distribution shift and produces out-of-distribution embeddings.
2. The ablation of the pretraining phase is missing. An experiment without pretraining should be conducted to demonstrate the necessity of the proposed pretraining strategy.

1. Can you show the correlation between the number of chains and the node degrees to directly validate the claim "multimers with more chains are more likely to yield assembly graphs with high degrees" in section 3.3?
2. How is the ablation of the C-PPI modelling strategy implemented?

---

**Review 2:**

The paper treats the problem of multimer assembly: given a set of sequences, and the structure of all possible dimers (e.g. from AF2), we wish to assemble the multimer by iteratively selecting the next chain and aligning dimer structures—represented with an assembly graph. The paper proposes a multi-stage solution to this problem. (1) A GNN is pre-trained to predict the multimer TMScore from an assembly graph (2) The “next link” prediction problem is  framed as a TMScore prediction over a fictitious assembly graph, i.e., akin to “prompting” the pretrained GNN. This fictitious assembly graph is created by a “prompting model” and its design is inspired by network-based PPI prediction in bioinformatics. (3) The prompting model–which is specific for each multimer size–is obtained via meta-learning, where the meta-training tasks are small multimer sizes, and the meta-tuning tasks are large multimer sizes.

* The paper proposes a novel solution to the difficult problem of multimer structure prediction. Multiple strategies are employed to make this extremely data-scarce problem tractable for deep learning. These strategies are impressive in their sophistication and the bar for originality / novelty has clearly been surpassed.
* The experimental results are good in terms of both performance and runtime relative to the best existing methods.
* The paper is a nice illustration of the concept of learning on top of foundation models such as AF2, a paradigm which arose in NLP and is becoming increasingly useful in biological ML.

* The paper integrates multiple technical ideas with a complex problem domain, but unfortunately the presentation is very confusing.
   * The paper relies on many ideas that are less familiar to the average reader in protein ML. There should be an extensive background section explaining meta-learning, prompt learning, L=3 PPI prediction, etc.
    * For a procedure with this many moving parts, it is absolutely essential to provide an explicit inference algorithm somewhere.
    * The paper is made even more confusing by certain particular choices of emphasis which serves only to distract the reader on a first pass.
        * It is not clear why it is important to emphasize C-PPI vs I-PPI. Perhaps the authors are trying to draw a distinction with MCTS, but this is really not necessary or within scope. Fully appreciating the difference would require a detailed explanation of the MCTS method, which the paper has no time (or need) to fully explain.
       * The extended discussion in Section 3.3 seems disconnected from the context of the paper and serves only to make it more confusing.
       * The authors repeatedly distinguish between oligomers and multimers based on size, which is very unconventional and should be fixed.
    * L=3 PPI prediction is not obvious and is very confusing when referred to in-passing the first few times it is brought up.

* The pipeline seems unnecessarily complicated and poorly justified. All else held equal, solutions to hard problems should be as simple as possible, and complexity (even if novel) should at least be sensible and easy to justify once understood. Here, it is really not clear why the problem requires such a complex formulation. The so-called source task is a nice way of framing the multimer assembly problem to make it much more data-rich. But then, the most natural solution would seem to be to run the TMScore predictor on all possible next-link additions to the current assembly graph. It seems quite convoluted to instead obtain a prompting model to convert each possible next-link prediction to a fictitious 4-graph when a real (N+1)-graph would also seem to work.

Justification for score. Although I like the wealth of ideas presented in the paper, the presentation is too unclear and the complexity insufficiently justified to recommend acceptance in its current form.

* Can the authors confirm that there is only one pretrained model, despite the discussion in section 3.3?
* Where are the node embeddings H is used in prompt model? Are only $H_u, H_d$ used?
* Is there precedent for learning a prompt _model_ that generates a different prompt for each input, as opposed to simply learning a _prompt_?
* Are the encoder parameters $\theta$ and task head parameters $\phi$ ever separated? If not, then denoting them separately only makes the paper more confusing.
* How is runtime calculated? I assume the dimer structures are completed "lazily." What explains the large gap in runtime relative to MCTS? It would be nice to report the total number of dimer structures "required" by MCTS vs the proposed method.

---

**Review 3:**

This paper proposes a sequential protein complex assembly method called PromptMSP. In each assembly step, PromptMSP predicts where a protein should be assembled to the current complex. During training, PromptMSP learns a continuous score for a given protein assembly graph and during testing, it uses the learned score model to find the most likely assembly graph. To avoid training and testing distributional mismatch, PromptMSP employs prompt learning to reduce the gap of input formats. PromptMSP is compared with existing multimer prediction baselines and outperforms AlphaFold-multimer baseline.

* The proposed method outperforms AlphaFold-Multimer (AFM), which is impressive.
* The evaluation setting is comprehensive. It includes both ground truth dimer setting and predicted dimer setting, which ensures a fair comparison with AFM.
* Ablation studies show that each proposed component is effective.
* Incorporation of L=3 PPI rule into the inference procedure is an interesting contribution.

* The method description is very confusing. Figure 5 is very crowded and rather uninformative.
* It is very hard to understand what meta-learning part (section 4.3) is actually doing. A visual step-by-step illustration of prompt fine-tuning can be helpful.
* The introduction of prompt fine-tuning seems an overkill. A simpler approach should work equally well. For example, we can adopt a standard autoregressive link prediction algorithm to this problem. In each step, you predict the link between a pair of proteins and train the model to predict the right link given different prefix graphs.
* Analysis in section 3.3 is unclear. How did you compute Centered Kernel Alignment between two models?
* It's unclear how a new protein is docked to the current assembly in each step. Did you use EquiDock? If so, how do you ensure that EquiDock is not trained on any of your test set instances?

* At test time, what prompt do you provide to the model? It seems that the prompt is basically the assembly graph that model predicted. I don't see why prompt engineering is useful during training.
* It would be helpful to report model performance for each number of chain (from 3 to 30).

---

**Review 4:**

This paper introduces a new algorithm to predict multimer structure with multiple chains via a pre-training and prompt tuning framework. The overall idea is novel and interesting. Different from MoLPC, where proteins docking are independent without the consideration of other protein, this method considers the influence of third-party proteins when performing docking. This paper compared several baselines on N chains datasets (N>=3). The experimental results show improvement on AlphaFold-Multimer and MoLPC. Although this paper introduces some new idea, many details are unclear. Also, the baselines are so weak and the experimental setting is not realistic.
I vote to reject this paper.

- Solving multimer structure prediction via pre-training and prompt tuning is interesting.

- It's reasonable to consider conditional docking for multiple protein.

- defintion 1 is problematic. Because in real-world setting for docking, monomer's ground-truth structures could not be provided. So that the correctness could never be 1 in real-world setting.

- the baselines are so weak. when taking ground-truth structure as input, HDock[2] and xTrimoDock[3] are strong baselines.

- it could be interesting if you can compare different baselines over different the number of chains. The performance could be reduced when increasing the number of chains.

- missing some related references: [1], [3], [4] [5]

[1] Ghani, Usman, et.al. Improved docking of protein models by a combination of alphafold2 and cluspro.

[2] Yumeng Yan, et.al. The HDOCK server for integrated protein–protein docking.

[3] Yujie Luo, et.al. xTrimoDock: Rigid Protein Docking via Cross-Modal Representation Learning and Spectral Algorithm.

[4] Mohamed Amine Ketata et.al. DIFFDOCK-PP: RIGID PROTEIN-PROTEIN DOCKING WITH DIFFUSION MODELS. use torsional diffusions to solve rigid protein docking, and the source code is released.

[5] Lee-Shin Chu et.al. Flexible Protein-Protein Docking with a Multi-Track Iterative Transformer.

- when comparing with AlphaFold-Multimer, do you input monomer's ground-truth structure as the template?

- how does your method perform when using predicted monomer structure? is the method robust?

---



---

## Year 2024

### Paper 1 (paper_id: OOqvY9yvVG)

**Review 1:**

This paper proposes combining NTP with knowledge embedding methods to remedy NTP training difficulties. Specifically, the authors verified four combinations, and all of them show good performance across the symbolic reasoning datasets (Kinship, Nations, and UMLS). In some tasks, their methods give SOTA results. They also conducted some ablations experiments for configurations of KGEs.

1. This paper tries to solve a real problem of NTP: sparse proofs lead to training difficulties. The authors propose to use KGE to remedy this problem. By combining KGE and NTP, the embeddings become easier to train, because KGE is computationally dense and efficient.
2. The structure of the paper is clear, with four combinations to investigate: it proposes using KGE in various modules of NTP, including step, unification, and loss computation. The experimental results show the effectiveness of them.
3.  The experiments are extensive.

1. It claims that "We provide the first study for integrating KGEs into NTPs", however, it is not very appropriate. Although some integration methods proposed in this paper modify the NTP modules, the method that combines losses between NTP and KGE was already proposed in the first NTP paper [1]. In Sec. 4.2, it uses neural link prediction (ComplexE) as an auxiliary loss.
2. I believe that the equation in Line 108 is incorrect: the second term should be (1 - NTPxxx).
3. The proposed methods are trivial to some extent. Actually, except for the CTP1, the others seem to be irrelevant in easing training difficulties. Why does changing the unification equation improve training?
4. There are more systemic methods to convert the sparse computation into dense ones: [2] converts the grounding into dense matrix computation, [3] uses an Einsum operation for fast parallel groundings. They seem to be more promising in this direction.

[1]. End-to-End Differentiable Proving

[2]. Logic Tensor Network

[3]. LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints

Please see the weaknesses above.

---

**Review 2:**

The paper proposes integrating Knowledge Graph Embedding (KGE) methods with Neural Theorem Provers (NTPs) to enhance neuro-symbolic reasoning systems, addressing the optimization challenges of NTPs. It presents four specific integration strategies (CTP1 to CTP4) and evaluates their performance across various datasets. The results demonstrate substantial improvements over baselines, especially in terms of Mean Reciprocal Rank (MRR) and inference speed, with CTP2 standing out as the most effective integration approach. Ablation studies and detailed analysis add depth to the findings, although limitations in scaling and efficiency are noted.

The integration of KGE methods with NTPs is well-motivated, bridging the optimization smoothness of KGE with the interpretability of symbolic logic inherent in NTPs. The systematic exploration of different integration strategies adds scientific rigor.

The paper evaluates the proposed methods across multiple benchmarks, showing a clear improvement over baselines. The use of detailed ablation studies, t-SNE visualizations, and analyses of embedding space provides strong empirical support for the proposed approach. Also, the authors applied 4 integration strategies (CTP1 to CTP4) and evaluates their performance across various datasets. The integration strategies are diverse and comprehensive.

The paper is well-organized and well-written. All 4 strategies are described with enough details.

While the proposed integration strategies are explained, some technical details, such as the choice of parameters for λ and specific KGEs for each variant, could be clarified further to assist replication and general understanding.

I am not familiar with the FB122 dataset. I know FB15K-237 is a commonly used KG reasoning database. The authors may want to explain why not using FB15K-237 but use FB122 instead.

Adding related work at the end is okay in general. But in this paper, the related work is entangled with the ablation study figures and tables, which makes the end of this paper hard to read. Please add related work after introduction.

The authors may want to explain why not using FB15K-237 but use FB122 instead.

---

**Review 3:**

This work proposes to integrate a knowledge-graph embedding (KGE) loss function into the optimization of a neural theorem prover (NTP) model, a type of neurosymbolic search architecture that is particularly hard to optimize due to subgradient sparsity in its backward pass. The goal of this integration is to alleviate the optimization challenges faced by the standard NTP objective, which contains many operations with sparse gradients and is plagued by local minima. The authors explore several alternative formulations of this integration,
all based on the Conditional Theorem Prover (CTP; Minervini et al., 2020):
- CTP$_1$: a straightforward linear mixture of the original NTP loss and the KGE loss
- CTP$_2$: adding the KGE scoring function to the NTP stepwise unification scores
- CTP$_3$: changing the goal-reformulation module of the CTP to use the KGE's tuple completion geometry, viable for path-based KGE methods that support calculating the tail entity embedding in a triple directly from the head entity and relation embeddings
- CTP$_4$: replacing the final unification computation in a CTP proof path (normally a very large kernel evaluation) with a KGE lookup.

The authors evaluate their approaches on several link prediction tasks against three prior NTP variants, various KGE methods, Minerva, and NeuralLP. On the Kinship, Nations, UMLS, and FB122 (Test-II and Test-ALL splits), one of CTP$_1$ or CTP$_2$ performs best. On the FB122 Test-I split and WN18RR, KGE baselines outperform the proposed systems.

The authors additionally include analysis of the learned embedding spaces and training dynamics, including the observation that on large datasets like WN18RR, the embedding spaces learned by their hybrid methods do not appear to recover the structure seen in the spaces learned by the baseline KGE methods.

- I feel that the authors successfully demonstrate that hybridizing the two approaches (KGE and NTP) can yield complementary benefits.
- The analysis and ablations are very thorough: I found that they improved my understanding of the differences between the proposed systems and fairly characterized the remaining shortcomings of the best-performing options.
- The paper does a good job of describing the NTP background clearly. While I'm familiar with the method, it doesn't seem like it would be hard to follow for someone who was less so.

- While the proposed methods are successful on certain datasets, they fail to match baselines in certain cases (as the authors note). The paper presents clear diagnostic evidence of this issue in section 4.3 and some speculation on why this occurs, but the fact remains that the proposed methods are clearly situational and not straightforwardly scalable to large/complex domains.
- The specifics of CTP$_2$, CTP$_3$ and CTP$_4$ could use more elaboration; it's unclear how the KGE negative samples are included in these cases.

Q1: In CTP$_2$, CTP$_3$, and CTP$_4$, how are contrastive negatives included? Is the original KGE objective still optimized (it seems like it's not)?

---

**Review 4:**

This paper handles the link prediction by combining Neural Theorem Proving (NTP) and Knowledge Graph Embeddings (KGE). Specifically, the KGE is injected into the CTP framework (a successor of NTP) in four ways. Empirical studies on large-scale datasets justified the empirical performances of those four ways of integration. Notably, CTP 1 and 2 perform way better than but significant slower than CTP 3 and 4.

- The empirical study is systematic and details the effectiveness of four variants and the impact of some crucial hyperparameters, including the weight and number of negative samples.
- the visualization also helped people understand the role of KGEs, which provides better global embedding structures.

- One of the key weaknesses is that all findings developed in this paper are solely for neural theorem provers. This makes the paper less impactful, given the existence of other technical solutions, such as GNN-based link predictors like NBFNet, rule miners like LERP, and a large language model for knowledge harvesting.
- In light of the weakness above, it would be much better if the authors could also analyze the proof produced when conducting link predictions as the key feature of neural link predictors.

- In terms of performance, what is the performance and speed of using KGE to make link predictions?

---


### Paper 2 (paper_id: DoB8DmrsSS)

**Review 1:**

This paper proposes a novel non-$\ell_p$ attack algorihtm for image-observation reinforcement learning, based on a history-conditioned diffusion model. The generated attacks are semantically meaningful while misleading to the agent. Experiments show that the proposed SHIFT attack can significantly break existing robust RL learning algorithms that are mainly designed for $\ell_p$ attacks.

- The paper points out the limitation of mostly-studied $\ell_p$ attack model for image-observation reinforcement learning environments. By utilizing a denoising diffusion probabilities model (DDPM), the paper achieves stealthy and realistic attack by altering the semantic meaning of the original state.
- The paper clearly defines the concept of valid and realistic states and adopts an autoencoder to enhance the realism of the generated attacks.
- Comparison with existing methods show that SHIFT can lower the reward of agent while having low state reconstruction error.
- The paper also proposes a possible defense method agains the new attack model.

- Although the proposed attack uses methods such as autoencoder guidance to enhance the realism of the perturbed states, it is not guaranted or bounded like $\ell_p$ attacks, making it hard to compare and evaluate the stealthiness of the perturbations. It is not clear to me whether the reconstruction error can effectively represent the realism of the state perturbation. It would be better if the authors can provide a gif or video showing the full perturbed trajectory.
- The experiments are not very informative. It is not surprising that RL agents learned via $\ell_p$ attack assumptions will break under the proposed attack. But more empirical study can be done to verify the effectiveness of the proposed design. For example, how does the varying attack strengh influence the attack effects?

As the author mentioned, the proposed method uses a myopic target action manipulation objective which can be sub-optimal. Is there a way to improve it? For example, how can it be combined with RL-based attack methods such as PA-AD?

---

**Review 2:**

The submission claims to find that the effectiveness of the current defenses is due to a fundamental weakness of the existing $\ell_p$-norm constrained attacks. Furthermore, the submission proposes a method to go beyond the $\ell_p$-norm bounded adversarial attacks in deep reinforcement learning. The submission evaluates its proposed attacks in Atari games and argues that the proposed attack method of the submission lowers the cumulative rewards of the agent by 50%.

AI safety and robustness is an important research area.

The major claimed contributions of the submission have been previously both mentioned and analyzed in previous work [1]. However, the submission does not refer to these studies, and furthermore, within the existing prior work the main claimed contributions of the submission are rather misplaced and inaccurate. The paper [1] already extensively studies and demonstrates that both deep reinforcement learning policies and current defenses, i.e. robust deep reinforcement learning, are not robust against semantically meaningful adversarial attacks and this study further reveals the need to have robustness beyond $\ell_p$-norm bounded attacks.

Not only has the necessity of considering beyond  $\ell_p$-norm bounded attacks already been discussed in previous work, furthermore the approach proposed in this paper [1] achieves higher degradation on the policy performance without even having access to the training details, the policy network (i.e. black-box adversarial attacks), and further without even training any additional network to produce such adversarial examples.

The submission substantially lacks appropriate references, and further positioning itself within the existing prior work and clarifying its main contributions within these studies. The claimed contributions of the submission are misplaced and incorrect.

[1] Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness. AAAI Conference on Artificial Intelligence, AAAI 2023.

Furthermore, the submission lacks main technical details to interpret their experimental results. Not a single experimental detail is provided regarding deep reinforcement learning. These details are essential for reproducibility and further to interpret and analyze the experimental results provided in the submission. However, the submission does not provide any information on this.

The submission only tests their algorithm in 3 games from Atari. However, in adversarial deep reinforcement learning it is usually tested in more games [1,2,3,4]. In particular, RoadRunner is missing from the baseline comparison.

[1] Robust deep reinforcement learning against adversarial perturbations on state observations, NeurIPS 2020.

[2] Robust Deep Reinforcement Learning through Adversarial Loss, NeurIPS 2021.

[3] Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness, AAAI 2023.

[4] Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions, ICML 2023.

The submission also refers to main concepts in the adversarial machine learning literature with inaccurate wording. For instance, in the introduction the submission writes:

*“by poisoning its observation (Huang et al., 2017; Zhang et al., 2020a)”*

However, poisoning attacks in adversarial machine learning literature refer to completely something else and these papers are not poisoning attacks. These papers are test time attacks. Thus, it is misleading to use the word poisoning here.

One thing I find ineffective is that the submission refers to a long list of papers such as these [1,2,3,4,5], however, somehow still misses the prior work that substantially coincides with the main claimed contributions of the submission and even further these prior studies already demonstrate the claimed contributions of this submission.

[1] Kangjie Chen, Shangwei Guo, Tianwei Zhang, Xiaofei Xie, and Yang Liu. Stealing deep reinforcement learning models for fun and profit. ACM Asia Conference on Computer and Communications Security, 2021.

[2] Mengdi Huai, Jianhui Sun, Renqin Cai, Liuyi Yao, and Aidong Zhang. Malicious attacks against deep reinforcement learning interpretations. ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020.

[3] Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipulations on cost signals. Decision and Game Theory for Security (GameSec), 2019.

[4] Zikang Xiong, Joe Eappen, He Zhu, and Suresh Jagannathan. Defending observation attacks in deep reinforcement learning via detection and denoising. Machine Learning and Knowledge Discovery in Databases: European Conference 2023.

[5] Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Janjua, Ala I. Al-Fuqaha, Dinh Thai Hoang, and Dusit Niyato. Challenges and countermeasures for adversarial attacks on deep reinforcement learning. ArXiv 2020.

See above

---

**Review 3:**

This paper introduces SHIFT, a diffusion-based adversarial attack that targets RL agents in vision-based environments by creating realistic, history-aligned state perturbations that go beyond traditional lp-norm attacks. Unlike existing methods, SHIFT generates semantic changes that significantly impair the agent's performance, bypassing even the most advanced defenses. Results demonstrate the attack's efficacy, reducing cumulative rewards by over 50% in Atari games, underscoring the need for more robust defenses in RL.

1. This work proposes semantic-level RL attacks using conditional diffusion models that balance semantic changes, realism, and historical consistency. The insight is novel.
2. Identifies a fundamental weakness in lp-norm attacks - their inability to meaningfully alter state semantics despite large perturbation budgets.
3. Employs EDM to enhance generation efficiency, making the approach more feasible

1. Despite using EDM and weighting joint, the paper lacks any systematic analysis of attack efficiency and computational costs.
2. While reporting larger attack budgets, results are limited to PGD and MinBest baselines, missing broader comparative analysis.
3. Experiments are restricted to only three Atari environments, providing insufficient evidence for the method's generalizability.
4. Overall Soundness: While the core idea is interesting, the paper falls short in rigor - lacking ablation studies, methodology analysis, and comprehensive experiments. The current evaluation scope is not convincing enough to support the claims.

1. The Manipulation Rate and Deviation Rate metrics appear exclusive to SHIFT's diffusion-based approach, raising questions about fair comparison with non-diffusion methods. The necessity of diffusion models needs stronger justification.
2. The paper lacks crucial comparison between DDPM and EDM in terms of both effectiveness and efficiency. This missing analysis weakens the justification for the chosen architecture.
3. Overlooks recent related work [1] about temporally-coupled perturbations

[1]Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations, Liang et al, ICLR 2024

---

**Review 4:**

The paper studies how to generate state perturbations for reinforcement learning, especially perturbation in an unconstrained way, instead of traditional L_p perturbations. The methods are based on diffusion models to generate states with different semantics. The experiments outperforms some existing baselines.

1. The paper studies an interesting question of non-L_p attacks, which is largely neglected by existing literature.
2. The methods can scale to image-input domain

1. One concern/question is that the goal of the paper is not very concrete. The paper said existing methods cannot change the semantics of the image input while this paper can. However, it is not very clear why the attacker has the motivation to change the semantics? In other words, isn't being stealthy beneficial for the attacker?
2. Some newest/recent defense baselines to my best knowledge [1, 2] are not discussed or compared in experiments. These game-theoretical based defense methods are significantly different from the defense mechanisms discussed in the paper by nature, and more importantly agnostic to the attacker model (which means one only need to change the attacker model to non-L_p accordingly to extend the defense to non-L_p). Therefore, it will be important to evaluate how the attacks performs under such kind of defense strategies.

[1] Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations ICLR 2024
[2] Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies ICLR. 2024

see weakness

---


### Paper 3 (paper_id: Iz75SDbRmm)

**Review 1:**

This paper introduces Deep Schema Grounding (DSG), a framework that extracts structured representations from LLMs to grounding and reasoning over visual abstractions. DSG leverages the inner knowledge of LLMs to generate schema descripting the abstraction concept. The content within abstract schema will be later grounding into corresponding compoents in the image by Large Vision Model via a hierarchically process. To evaluate the effectiveness of DSG, a new benchmark, Visual Abstraction Benchmark (VAB), is purposed in paper to evaluate model's understanding in visual abstractions.  VAB consists of 540 test examples consists of 180 images presenting 12 abstraction concepts spanning 4 categoris (15 examples each).

The paper do comprehenisve experiment comparing DSG with 5 current VLMs. The result shows DSG can boost 6.6 percent accuracy oveall compared to GPT-4o while current VLMs shows a big gap between human performance. Further abalation study pinpoint that each modules in DSG is necessary and contribute to its success. DSG can boost model performance on both open-source and close-source model but authors notes the challenge still exist in visual abstraction field.

1.  Schema-based framework: The paper start from cognitive science and theory-theory to purpose a schema-based concept representation framework. The process of representing high-level abstract concept and later grounding in actual image bring boost in model's ability of abstract understanding. The pre-generetad abstract concept schema seems helpful on mitigate halluciations (16.6% relative improvement in counting-based questions) and is beneficial for both open and closed source models.
2. Curately designed Visual Abstraction Benchmark (VAB): The author provide detail explanation the reason behind VAR design choice. Detailed description of data collection and curation process is provided. The dataset is released for public use. As abstraction can play important role in understanding concept and ease the learning process, I believe this benchmark is a good step for solving the abstraction problem.
3. A comprehensive experiment: the experiment extensively test current LVMs and point out their weakness in visual abstraction understanding.  Further experiment demonstrate the effectiveness of DSG and the ablation studies prove the design choice of each module.
4. Writing. The paper is well-written overall, with clear and well-designed figures and tables. The comprehensive appendix offers additional details beyond those covered in the main text. I like the way authors start their paper with an illustration on ''what makes a maze look like a maze'', which provide a easy-understanding approach for readers, especially for the concept of visual abstraction.

1. Size of Benchmark: one concern I have for the benchmark is relatively small size of the data. VAB only comprised 12 abstract concepts acroos 4 categoires, I know the data collection can be challenging but the limited size can decrease the significance of the benchmark evaluation. The author mentions the benchmark is divertse in terms of subject, environments and domains (Line 377). A dataset statistic over these metadata can be really helpful to support the argument.
2. Error grounding: Perception is also a challenge issue in the whole framework. It is possible that VLM have the understanding of the abstraction concept but fails to percept the image well and ground the concept within actual images (especially for images containing multiple subjects, image of cell in Figure 3).  That may also be the reasoning the schema-based framework become most helpful on couting problem. A further experiment showing VLMs can percept the image correctly is necessary.

Overall the limited size of benchmark is my primary concern. I like how paper design the framework and benchmark. The benchmark provide a  good practice on handle and evaluate abstract concept, which is helpful in current literature.

Questions:
1. In llnes 399 to 400, it seems that there are multitypes of question and also each question type has different baseline accuracy (0.5 for binary in ideal). Do each categorie has a same distribution of different questions types? Otherwise the comparison over each category might be misleading.
2. Is it possible the schema generated from LLMs doesn't fit well with image? For example, the first maze image (candy maze) in Figure 5, it is unclear for human what is the layout and where is the entry point.  How does the framework handle this scenario where the generated schema bring additional concept and may cause hallucinations.



Suggestion:
1. To make reader understanding the significance of abstraction, author can enrich the introduction with some practical application of abstraction elaborate more on how human use abstraction, such abstraction enable human mind to learning from small, obtain different levels of abstraction which can later be used to trim the learning space for new concepts.

---

**Review 2:**

The paper proposes the Deep Schema Grounding (DSG) framework, aiming to enhance VLM’s capability in understanding abstract visual concepts like “the maze”. The framework integrates schema representations with hierarchical grounding to guarantee the VLM’s to better capture human-aligned visual abstractions. DSG operates by extracting schemas of abstract concepts, then grounding schemas on images following a hierarchical manner, then augmenting the VQA problem with resolved schemas, resulting in improved visual reasoning. The authors also introduce the Visual Abstractions Benchmark (VAB) to systematically assess abstract concept reasoning. Experimental results indicate that DSG consistently outperforms baseline models across diverse abstract concept categories and question types.

1) The paper is well-written, it effectively conveys its proposed framwork and the motivation behinds it with clarity.
2) The experiments are well-made and the experimental analysis is comprehensive. Extensive experiments validate the effectiveness and efficiency of the DSG, which serves as a plug-and-play trick to enhance the performance of existing VLMs.
3) The pipeline that utilize schema representations with hierarchical grounding for visual concept understanding seems reasonable to the reviewer, as it emulates the human cognitive process of understanding abstract concepts.
4) The proposed method is simple yet effective, and it is easy to follow.

1) The gerealibility is somehow resricted. The method has been tested on only 12 types of abstract concepts and it has not been tested on the problems which invovles multiple abstract concept, which raises questions about its generalizability to a broader range of abstractions.
2) The techinical contribution of DSG is limited. The main technical contribution of this paper lies in the combination of neural-symbolic and VLM by the development of specifically designed prompts. (Prompt for LLM to generate schema, prompt for VLM to generate concept grounding). The improvements made to the prompts are relatively straightforward, and the utilization of CoT is a standard practice.
3) The risk of the hallucination of LLM/VLM. During schema construction and hierarchical prompting, the LLM may produce hallucinated errors. If errors occur in hierarchical content, individual instances may fail; however, if schema generation is flawed, it could lead to failures across an entire category of problems. The paper lacks a thorough analysis of failure cases. The authors provide some failure cases demonstration in Appendix A.3. But it seems that these results are not enough. See question 5) for further suggestion.

1) Is the proposed DSG method applicable to other general visual reasoning benchmarks, such as RAVEN, Bongard, CVR, and SVRT?
2) Can DSG apply to the scenario where there are multiple abstract concepts in an image? I expect the authors to provide some insight for this, but I don't expect the authors to scale up to more experiments in the rebuttal though.
3) In table 3, the performance of schema plus grounding surpluses the performance of DSG in counting problem. Why would this happen?
4) I also have some questions regarding the length control of the schema. In the prompt provided in the appendix, it is limited to 'Keep the program simple with four or fewer components.' Why is this limitation imposed? Can the authors provide experimental results for schemas with different numbers of components? Or should the depth of the schema increase for more abstract concepts, while simpler schemas can suffice for more general concepts?
5) How robust is the method in general to failures: How often does the LLMs fail to provide a valid schema and how often does VLMs fail to provide a correct grounding? Does an invalid schema or grounding result in the failure when generate the answer?

---

**Review 3:**

This paper introduces a VQA benchmark and method for visual abstract reasoning.

The "Visual Abstractions Benchmark" consists of 180 images, annotated with 3 questions each (and each question has 5 answer annotations from Prolific workers). These questions ask about abstract concepts in the image: for example, "Q: Imagine that the image represents a maze. Are there clear entrances or exits in the maze? [Yes or No]" (Figure 1; an image of an ice skating rink partitioned like a maze).

The method, "Deep Schema Grounding (DSG)", is a prompting framework that (1) uses an LLM to decompose an abstract concept into a schema, (2) uses a VLM to ground the "primitive" concepts from the schema to concrete concepts in the image, and (3) uses a VLM to answer the question given the schema and grounding.
1. The schema is a DAG: a "maze" would decompose into "layout" and "walls", and "layout" further decomposes into "entry-exit".
2. A VQA question is used for grounding: e.g. given the image, "..., what is the layout?". This graph is hierarchically grounded: e.g. "entry-exit" is grounded using the question "..., the layout is [], what is the entry and exit?". The ice rink example above entails the following grounding: "{layout: grid-like, walls: ice, entry-exit: gates}".
3. The image and grounded schema are joined as a VQA question for the VLM to produce a final answer.

Applying this framework improves performance of GPT-4o by 10% (relative) on the benchmark.

This paper is extremely clear and straightforward.

In general, I believe the comparisons are fair and sufficient ablations are provided, so it is methodologically sound. For example, they show a 10% relative improvement when applying their method with GPT-4o and 7% with (the open-source and weaker) LLaVA model. They also compare against other methods (ViperGPT and VisProg) that also use GPT models (GPT-3) for visual question decomposition, but don't perform nearly as well on this benchmark. Finally, they provide sufficient ablations of their own method (separating the schema/grounding/hierarchy/context components and showing incremental gains of each).

I believe this is a creative benchmark and that existing vision–language benchmarks under-explore abstract visual concepts. The method is task-specific, but I think that is fine, because it is prompting-based. I don't think the approach is entirely original but I think it sufficiently rounds out the paper: they show how explicit structure improves performance of existing models on their benchmark.

Altogether: this paper presents a benchmark, shows that existing vision–language models underperform, and introduces an explicit prompting method that can be applied with any LLM/VLM to improve performance. The best performance with this method is still just 73% (with the strongest, closed models) or 44% (with open-source models), leaving sufficient challenge for future work.

I believe the construction of the dataset is under-specified (and I also checked the appendix). I understand that answer annotations were provided by crowd workers. However, how were the questions written? How were the images selected (and where do they come from: L376 just says "the Internet")? Likewise for the 12 abstract concepts and their 4 categories. Right now, I am imagining that the authors curated these themselves. That could be fine, but I would like to know more details, so that we can determine whether there may be any biases: e.g. does the introduced method perform better on this particular selection of abstract concepts?

I notice that answers in the open-ended setting are evaluated using BERTScore. However, (Kamalloo 2023) says

> unsupervised semantic similarity-based evaluation metrics such as BERTScore (Zhang et al., 2020) that rely on token-level matching of contextualized representations exhibit poor correlation with human judgment in QA evaluation (Chen et al., 2019) and lack the ability to capture attributability (Maynez et al., 2020)

I also find this to be true in general. Could you show examples, so we can see if the metric is indeed suitable for this distribution of data? Otherwise, could you consider using a method that is better aligned with human judgments for this evaluation: possibly (Kamalloo 2023)?

Reference: [Evaluating Open-Domain Question Answering in the Era of Large Language Models](https://aclanthology.org/2023.acl-long.307) (Kamalloo 2023)

The word "lifted" is used in several places (e.g. "lifted symbols" or "lifted rules"). I think this terminology isn't entirely obvious. If this has a specific meaning, could the authors please elaborate in text (or just use a different word if not)?

It could be informative to clarify the distribution of schemas based on their graph depth in the text. E.g. I see that "cell" has depth 2 and "atom" has depth 3 (L875-881). The single in-context prompt for schema generation is "academia" (depth=3; Sec. C.1). Could abstract concepts exist with depths larger than 3? Would the LLM limit their decompositions to depth=3 (at most) because that is the only prompt? I think the authors could elaborate further on their prompt engineering and exploration, especially since their method is a prompting framework.

---

**Review 4:**

The paper proposes Deep Schema Grounding (DSG), a method to break down a visual concept into smaller concepts with dependencies on other concepts. The authors show the capability of DSG in solving complex visual question answering task. A visual abstraction benchmark is proposed with 12 abstract concepts and 180 images.

The paper provides a good solution for VLMs to better understand abstract concepts in an image. The presentation of the paper is clear with many figures for demonstration. The experiments are comprehensive, making the main message convincing. The benchmark created is novel and interesting.

In terms of the idea behind DSG, it seems to be close to chain of thought with specific instructions. For example, the maze example in the paper can be integrated with just one prompt: "Imagine that the image represents a maze. <the question> Think step by step by recognizing the layout of the maze, the walls of the maze, then the entry and exit of the maze one by one." Maybe gpt-4o can automatically do this even without the instructions. This is probably why in Table 7 if the generation is free form, DSG does not outperform gpt-4o much despite using more API calls.

Therefore, I am a bit concerned whether these types of schemas are necessary. After all, the dependency graph is generated by gpt-4, so at least gpt-4o should know how to do it if prompted well. But I agree that for smaller models, DSG can be very useful.

Another concern I have is about the diversity of the benchmark because the number of categories is quite limited.

Could you try something like chain of thought prompting as I mentioned above?

Is there any way to generalize the idea to create a more general benchmark?

---

**Review 5:**

The paper introduces a framework called Deep Schema Grounding (DSG) and a VQA benchmark for evaluating abstract visual reasoning of VLMs. For a given image and a question about the concept in the image, DSG first generates a schema of the concept using LLM. Then it grounds the components of the schema onto images using VLMs. Once the schema is grounded, the grounding information is given as context in text from to the VML to answer the question about the image. They show that giving grounded schema context to the VLMs improves their ability to correctly answer questions about abstract concepts in the image.

- Innovative use of pre-trained LLMs and VLMs to give additional context (in terms of grounded schemas) for VQA. In principle, this additional information should help answer pre-trained VLMs to better answer the questions related to the abstract concepts in the image. (However I think that the generated schemas shown in the appendix of the paper are not detailed enough to achieve this - please check weakness section)

- Hierarchical Grounding: I like how they have used a hierarchical method of grounding components of the schemas. Grounding independent concepts first and then the dependent concepts of the schema should be the correct way for grounding which is used in the paper.

- Schemas are not detailed enough to give information about the concept. For example, tic-tac-toe schema include {board, symbols, strategy}. Although a tic-tac-toe game has  {board, symbols, strategy} it is not complete. Many board games have {board, symbols, strategy}. This schema does not tell that the board is a 3x3 grid. Similarly, for “negotiating” the schema is {participants, setting, object}. This schema could be of many different settings. (p.s. I do understand that the capabilities of current VLMs are not enough to handle detailed concepts)

- It is shown that giving grounded schema as additional context to VLM to answer VQA question is improving the performance. But it is not shown or discussed why?/how? is the additional context improving the performance. For instance, in the example given in figure 1 of the paper where the question asks “What is the player in the maze?”, how is giving information about the layout, walls and entry-exit helping a VLM answer question about the ‘player’.

- Many recent works (https://arxiv.org/pdf/2305.10355, https://arxiv.org/pdf/2401.06209, others) have shown that VLMs  are ‘blind’ and don’t actually look at the visual information in images. VLMs mostly rely on language and questions in VQA benchmarks can be answered by having a good language prior. I was wondering how much of the questions in the proposed benchmark can be correctly answered by “language” only? It would be great to show a “language-only” baseline for the proposed VQA benchmark. Especially after adding grounded schema as context- it can show the impact of providing grounded schema too.

- A qualitative analysis of how/why is the grounded schema is improving VQA capabilities of the VLM would be great. Especially for the cases where the question does not ask anything about the concepts of the schema (like the example of “What is the player in the maze?” mentioned in the weakness section)

- Are the generated schemas verified by some expert? How are we sure that the schemas provided by the LLMs are correct for the provided concepts?

---


### Paper 4 (paper_id: TZa84ZkOLM)

**Review 1:**

This work builds on Genie 1 by adding motif scaffolding, enabling the generation of protein scaffolds that support specified motifs - an important step toward the ultimate goal of designing functional proteins.

The authors developed a simple yet effective method to encode motif information as node and pair conditions, enabling the generation of protein structures that closely adhere to targeted motif cores. They also extend the single-motif problem to multi-motif generation, exploring whether the model can generate proteins with independent functions, pushing the boundaries of creating more complex and versatile protein machines. Additionally, they found that augmenting training with AFDB clusters and applying conditional training benefits both motif scaffolding and unconditional structure generation, with benchmarks against strong SOTA models, including RFDiffusion, FrameFlow, and Chroma.

In summary, this work provides an interesting perspective on protein motif design and demonstrates the improvements of Genie 2 in protein structure generation.

**[Clarity and Quality]**

- The manuscript is clearly structured and easy to follow, with a comprehensive background review covering sequence, structure, and co-design, as well as motif scaffolding. This allows readers to understand the main concepts without needing to refer to external sources.
- The authors conducted sufficient experiments to evaluate empirical performance of Genie 2 on both the unconditional generation and scaffolding, comparing it with strong SOTA baselines such as RFDiffusion and FrameFlow.

**[Originality and Significance]**

- The authors extend motif scaffolding to design multiple independent motifs, supported by a benchmark problem set, which could benefit protein generation in more complex protein machinery.
- By encoding motif conditions as node and pair embeddings, the authors provide an SE3-invariant and flexible approach for representing motif information.

**[Quality and Clarity]**

- The current version lacks in-depth studies on key tuning parameters (e.g., sampling temperature) and critical design factors (e.g., conditional training) (see Q1/Q3).
- Some important baselines are missing (see Q8/Q9).
- Some analyses and statements need further evaluation (see Q5/Q6/Q7).
- Certain statements are vague and would benefit from clarification (see Q2/Q12).

**[Significance and Originality]**

While the independent multi-motif design presents an interesting task, the current work faces limitations that affect its impact:

- The primary improvement of Genie 2 over current SOTA appears to be in diversity (including diverse solutions in motif scaffolding). However, it remains unclear whether this improvement is due to training with AFDB or low-temperature sampling (see Q3).
- Training on AFDB is a logical approach that has already been applied in other protein design models (e.g., Bilingual Language Model for Protein Sequence and Structure, Michael Heinzinger et al., bioRxiv, https://doi.org/10.1101/2023.07.23.550085).
- Encoding protein structure as SE3-invariant node and pair features is also an established approach (e.g., AlphaFold 2 encodes template information as node and pair). Although this work shows that SE3 invariance is effective for motifs encoding, it does not demonstrate broader impact on this design choice.

1. Table 3: The authors demonstrate the effect of conditional training ratio on small-scale, “unconverged” models. While this limitation is understandable due to time constraints, the conclusion that conditional training universally enhances performance remains speculative. More comprehensive results during the rebuttal period would strengthen the findings.
2. Low-temperature sampling (line 323): The authors state that “low temperature sampling yields better results.” Could they specify which metric(s) this “better” refers to, as it’s unclear from the context?
3. Temperature sensitivity: The authors used different temperatures (γ) for unconditional generation and motif-scaffolding, highlighting the importance of tuning this parameter. Could they include the default temperatures used for each model in the main text and clarify the impact of varying temperatures on each task? Does this adjustment impose trade-offs, and how sensitive is the model to this factor?
4. Training data balance: Is it reasonable to balance experimental structure data (PDB) and synthetic data (AFDB) for training? Are the results significantly biased towards synthetic data?
5. Secondary structure distribution (section 4.2): When evaluating secondary structure distribution, comparing to AFDB might be problematic as 1) not all baselines were trained on AFDB, and 2) AFDB may differ in structural distribution from real-world PDB data.
6. Secondary structure distribution (section 4.2): Contrary to the authors' suggestion in line 318, it seems Chroma, Proteus, and RFDiffusion appear to generate more beta strands (top left) than AFDB. Could the authors clarify their statement about underrepresented beta strands and loop elements?
7. Low-temperature sampling for AFDB (line 352): The lower designability in AFDB might explain low designability at normal temperatures for Genie 2, but it doesn’t directly explain why low-temperature sampling would mitigate this. Could the authors provide additional insights?
8. Baseline model: The authors mention the Twisted Diffusion Sampler in related work but do not include it as a baseline. Could they clarify this choice?
9. Baseline model Genie 1: Could the authors include baseline performance results for Genie 1 in unconditional generation?
10. Multi-motif scaffolding benchmark curation: This benchmark includes only proteins with <200 amino acids. Have the authors explored more complex cases with longer sequences, or are there specific challenges in curating such datasets?
11. Multi-chain generation: Although Genie 2 accepts “chain index” as an input feature, it does not support multi-chain generation with motif-scaffolding. What limits this implementation?
12. Clarifications on some claims:
    - The authors claim Genie 2 “better captures protein structure space” and covers a “larger and more diverse protein structure space.” However, this aspect has not been directly evaluated. Current results only show higher diversity, not necessarily “broader structure space coverage” compared to Genie 1, especially given the smaller sample size examined (1,035 structures, line 309).
    - In Equation (5), the authors claim that “motifs are enforced as a soft constraint, ensuring responsiveness to motif specifications while designing the protein as a whole.” However, the current loss does not distinguish explicitly between motifs and scaffolds (e.g., no separate regularization for motif losses). The authors might want to suggest the benefits of jointly learning the motif and scaffolds coordinates and further clarification could strengthen the interpretation.

---

**Review 2:**

This paper presents Genie 2, an improved protein diffusion model that extends the capabilities of its predecessor Genie. The key innovation is the addition of multi-motif scaffolding capabilities that allow designing proteins with multiple functional sites without specifying their relative positions and orientations. The model uses SE(3)-equivariant attention for the reverse diffusion process and introduces a novel motif representation approach. The authors demonstrate state-of-the-art performance in both unconditional and conditional protein generation tasks, particularly in terms of designability, diversity, and novelty.

1. Technical Innovation: The paper introduces a clever representation for multi-motif scaffolding that uses distance matrices to specify intra-motif but not inter-motif distances, allowing flexible placement of functional sites.
2. Comprehensive Evaluation: The authors provide thorough comparisons with existing methods like RFDiffusion, FrameFlow, and Chroma across multiple metrics on unconditional protein generation and single-motif scaffolding.
3. Data Innovation: The use of AlphaFold database for training data augmentation is a novel approach that expands the structural space beyond experimentally determined structures.

1. Training Constraints: The model is limited to training on sequences up to 256 residues in length, though it can generate longer sequences. The implications of this limitation could be better explored.
2. Multi-motif Benchmark: The benchmark set of only 6 multi-motif scaffolding problems is too small to draw statistically significant conclusions about the model's capabilities. A robust benchmark should include at least dozens of diverse cases to properly evaluate performance.
3. Multi-motif Baselines: Although previous works like RFDiffusion were not specifically designed for multi-motif scaffolding, they could still address such problems by randomly sampling relative positions and orientations between motifs. The authors should have included comparisons with these existing methods to demonstrate the advantages of their approach over such a baseline strategy.

Similar to what was mentioned in the weaknesses section.

---

**Review 3:**

The paper describes Genie 2, an extension of the protein structure diffusion model Genie for motif-conditioned generation. The method is extensively benchmarked against alternative methods for unconditional and motif-conditioned diffusion models, and seems perform comparatively well.

The authors detail an extension of the Genie model for motif-conditioned protein structure generation. The incorporation of motif information is clearly explained and quite simple, and importantly seems to work quite well empirically.

Comparisons to existing methods for unconditional and motif-conditioned generation are clearly presented and fairly thorough. There are many protein structure diffusion models and the authors have done a nice job presenting results for some of the most prominent.

The authors highlight functional protein design as a motivation for this work, and assert that functional protein design tasks can often be reduced to scaffolding of known motifs. While this may be true for simple enzymes, many functional proteins of therapeutic and industrial interest are considerably more complex (e.g., involving conformational changes or inter-domain coordination). Without experimental validation of the results presented here, it is difficult to know whether the proposed method would meaningfully improve functional design outcomes.

Given the relationship between this work and the original Genie paper, it is surprising to not see any direct comparisons between Genie 2 and the original model. For instance, how would the original Genie architecture trained on the PDB compare with the results presented in Table 1? It seems that the largest improvements came from expanding the training dataset to include AFDB, but it would be useful to see more detailed ablations of the architectural, data, and training objective changes, and the interplay between these (if there exists any).

The analysis of low-temperature sampling is quite hard to follow as currently presented. It is not clear that the authors have used low-temperature sampling for the results presented in Table 1 until the following section analyzing secondary structure distributions. Further, in this section (line 323), the authors cite a finding that low-temperature sampling yielded better results, but do not provide the results of this analysis.

The result about “designability” of AFDB structures is interesting, but not very convincing in its current form. Rather than summarizing the results of changing from scRMSD to scTM, the authors should present the full distribution of values for both of these metrics. The authors note that ProteinMPNN may be biased towards the PDB; this hypothesis could be tested by using ESM-IF1, which has been trained on AlphaFold2-predicted structures.

Much of the evaluation presented in this paper depends on ESMFold as an oracle for structure prediction. While this has become a standard practice, as noted by the authors, it would be useful for readers to include some discussion of the shortcomings and potential biases introduced by this approach.

At a couple places in the paper (line 806, line 872), the authors note that certain (earlier) model checkpoints were selected for evaluation due to better performance. How were these checkpoints selected?

It seems that the Chroma conditioner framework should admit single- and multi-motif scaffolding as described in this paper. Was there a reason this model was not benchmarked in these scenarios?

How important is the sequence information for motif conditioning? In the absence of this information, would the model still produce viable solutions to the motif scaffolding problems?

---

**Review 4:**

The authors introduce Genie 2, an extension of Genie that incorporates architectural innovations and extensive data augmentation. Genie 2 supports both unconditional generation and multi-motif scaffolding. The authors claim that Genie 2 achieves state-of-the-art performance, outperforming all known methods on key design metrics.

1. For unconditional generation, Genie 2 performs better than Genie and other methods when generating short proteins.
2.  Genie 2 also supports multi-motif scaffolding.

1. The authors overstate the performance of Genie 2 in the Abstract. While they claim that Genie 2 achieves state-of-the-art performance and outperforms all known methods on key design metrics, Figure 3 Panel B shows that Genie 2 performs significantly worse than Proteus on long proteins.

2. The comparison benchmarking is insufficient.

    a. Unconditional generation: The comparison lacks the inclusion of recent methods such as MultiFlow [1], CarbonNovo [2], and FoldFlow2 [3], which have demonstrated significant performance improvements.

    b. Single-Motif Scaffolding: Methods like Chroma and FoldFlow2 should be included for comparison. While Chroma is included in the benchmarking for unconditional generation, it is excluded in the scaffolding generation comparisons. Clarification on this omission is needed.
3. The methodological contributions appear incremental. The authors extend Genie's architecture and training procedure to enable motif scaffolding, with most modifications focusing on adapting input features. Additionally, Genie 2 incorporates triangular multiplicative updates, which have already been shown to be effective in previous works such as Proteus [4], CarbonNovo [2], and FoldFlow2 [3].
4. Insufficient Analysis of Multi-Motif Scaffolding: Despite claiming multi-motif scaffolding as a key contribution, the paper provides limited analysis in this area. Interesting analysis would be :

    a. In failed cases, is the failure due to incorrect single motif design or inaccuracies in the relative positioning of motifs?

    b. Does the success rate of scaffolding depend on the number of motifs being used?

 5.  The method requires significantly more inference steps than previous methods, indicating low sampling efficiency.


[1] Campbell et al. “Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design”. ICML 2024. https://proceedings.mlr.press/v235/campbell24a.html

[2] Ren et al. “CarbonNovo: Joint Design of Protein Structure and Sequence Using a Unified Energy-based Model”. ICML 2024. https://proceedings.mlr.press/v235/ren24e.html

[3] Huguet et al. “Sequence-Augmented SE(3)-Flow Matching for Conditional Protein Backbone Generation”. https://arxiv.org/abs/2405.20313

[4] Wang et al. “Proteus: Exploring Protein Structure Generation for Enhanced Designability and Efficiency”. ICML 2024. https://proceedings.mlr.press/v235/wang24bi.html

1. Previous methods tend to generate more alpha-helices than beta-strands. In Genie 2, what is the distribution of generated proteins among the structural classes of all-alpha, all-beta, alpha+beta, and alpha/beta? Does the designability of the generated proteins depend on the ratio of alpha-helices?

2. Why was a batch size of 256 chosen for training Genie 2? Both Proteus and Chroma use batch sizes larger than 500, yet they exhibit significantly different performance levels. Therefore, without additional experimental results, it is unclear whether batch size is the main reason for Genie 2's significantly worse performance compared to Proteus on long proteins.

2. Please address the concerns and questions raised in the Weaknesses section.

---


### Paper 5 (paper_id: 9DSUwiYJP3)

**Review 1:**

The paper **TinyMem: Condensing Multimodal Memory for Long-Form Video Action Detection** proposes TinyMem, a model that efficiently condenses video frames into a compact embedding for online action detection. It introduces a novel multimodal memory design that combines visual and text embeddings to reduce the memory footprint while maintaining or improving performance. The key contribution is using language descriptions (captions) and condensed visual regions as memory tokens, which significantly reduces the number of tokens required. The model is evaluated on long-form video benchmarks such as AVA v2.2, Epic-Kitchens-100, and Breakfast, showing superior performance compared to prior models like MeMViT.

**Novelty in Memory Design**: Introducing text-based memory tokens alongside visual tokens is a novel approach. By using compact textual representations, the model reduces memory usage while preserving critical semantic information. The results are intriguing as just using compressed textual information leads to such as an improvement on the evaluated tasks.

**Memory Efficiency**: TinyMem achieves competitive or superior performance with fewer memory tokens. The model maintains high accuracy on benchmarks such as AVA, Epic-Kitchens and Breakfast while reducing the computational overhead, which is crucial for long-form video understanding understanding.

Overall, the proposed method is simple, with limited novelty, yet the results are intriguing. The discussion here covers key aspects of the approach, along with potential limitations and questions that arise from the study’s findings.
Here is a list of key limitations/questions.

#### 1. Captioner Dependence
The model demonstrates efficiency, but its training is dependent on captions generated by models such as BLIP-2. This reliance could introduce external dependencies and may lead to significant computational costs during training. The implications of these dependencies are worth examining, especially concerning scalability and robustness.

#### 2. Captioner-Free Dynamic Inference
The process of Captioner-Free Dynamic Inference remains unclear, specifically how it avoids leaking label information. During inference, captions are generated heuristically using predicted action labels, which simplifies computation. However, this approach raises questions about robustness. Incorrectly predicted actions could lead to flawed captions, potentially compounding errors throughout the inference process.

#### 3. Temporal Relationships in Captioning
A notable flaw in the captioning method is its disregard for temporal relationships between frames, which might limit the model’s ability to capture nuanced temporal dynamics within videos.

#### 4. Text Memory Compression Technique
The technique used for text memory compression (Section 3.2) is highly aggressive, condensing an entire caption into a single token. The authors should clarify why they chose the `[EOT]` token for this purpose and discuss potential outcomes if tokens were sparsely sampled or averaged. Would these approaches lead to improved performance? Alternatively, why not consider taking the average of all token embeddings after mapping to a joint vision-language feature space?

#### 5. Token Merging Techniques in Table 2
In Table 2, would results differ if visual tokens were merged using techniques like average pooling or cosine similarity? A comparison of these methods might provide insights into token merging strategies and their impact on model accuracy.

#### 6. Effect of Additional Textual Data
The results in Table 1 indicate that additional textual data improves model performance over other information types, such as ROI visual tokens. Is this due to the auxiliary data source, or are there other contributing factors? The authors could provide an analysis of this observation.

#### 7. Scalability of Memory for Long Videos
Further discussion is needed on how the model’s memory mechanism scales with longer videos. This is particularly relevant in cases where extended temporal context might affect performance or computational feasibility.

#### 8. Choice of Comparison Model
The study uses VideoMAE as a comparison model in Table 11. However, clarification is needed on why VideoMAE was selected. Is this model chosen due to its distinct architecture, such as vanilla ViT or MViT, or because it features self-supervised pretraining? Additionally, was TinyMem initialized from a similarly pretrained model?

#### 9. Example Captions and Ground Truth Comparisons
It would be beneficial to examine some captions generated by BLIP-2 alongside corresponding video frames and the ground truth labels. Such comparisons could offer insights into caption accuracy and alignment with true actions in the video content.

#### 10. Limited Task Exploration
The evaluation is limited to long-form video benchmarks, though the study briefly mentions the possibility of exploring other tasks, like video question-answering and temporal action localization. Expanding the scope of tasks could provide a more comprehensive evaluation of the model’s versatility and robustness across diverse video-based applications.

### Missing Related Work on Memory

The approach closely resembles the Just Caption Every Frame (JCEF) baseline, yet a direct comparison is absent. Including this would help in evaluating the model’s novelty and performance against established baselines. Additionally, references to related work, such as [1] and [2], are missing.

---

### References

1. Min et al., *MoReVQA: Exploring Modular Reasoning Models for Video Question Answering*, CVPR 2024.
2. Kahatapitiya et al., *VicTR: Video-conditioned Text Representations for Activity Recognition*, CVPR 2024.

See weakness.

---

**Review 2:**

TinyMem addresses a significant challenge in video understanding: the ability to process and analyze lengthy videos that span minutes or hours. While current deep learning models excel at analyzing short video clips, they struggle with longer content typical in real-world applications like streaming services.

**Key Innovation**: The paper introduces a novel memory system that dramatically improves efficiency by condensing video content into two compact forms:
1. Semantic Memory: Converting video content into concise text descriptions using BLIP-2, a vision-language model
2. Region-Based Memory: Summarizing important visual elements through ROI (Region of Interest) tokens

**Technical Architecture**:
- Built upon MViTv2 (Multiscale Vision Transformer)
- Uses a FIFO (First In, First Out) system to manage memory
- Projects each caption into a single token, significantly reducing dimensionality
- Maintains just 16 memory tokens per video clip
- Implements a captioner-free dynamic strategy for improved inference efficiency

**Performance Advantages**:

*State-of-the-art results on multiple benchmarks*:
- AVA v2.2 action detection
- EpicKitchens-100 action classification
- Breakfast long-term activity detection

*Efficiency Improvements*:

- Uses up to 5x fewer memory tokens compared to baseline models with dense visual memory
- Lower GFLOPs that scale more efficiently with text and region tokens

**Originality**:
- The paper introduces the concept of using text as a compression mechanism for video content, which, as far as I know remained unexplored thus far.
- Also introduced is a hybrid memory architecture combining semantic text tokens and ROI visual tokens.

**Rigorous Evaluation**: Results on multiple datasets as well as clear ablations validating design choices.

**Typos and Language Issues**
- There are typos in Figure 3, where “captioenr” should be “captioner,” and on lines 77 and 78 ("illustarted" should be "illustrated").
- Review the use of adverbs and certain descriptors; for instance, “Nevertheless” in line 539 is redundant as it follows a sentence already commending the model. Similarly, "notably" and "more importantly" are used excessively or inappropriately (e.g., lines 83, 245, 295). Words like "fuels," "outweighs," and "overwhelms" are not appropriate in the context they are used in the paper. Consider revising for clearer emphasis.

**Claims vs. Evidence**
- The paper claims that current methods fail in real-world settings but lacks proof that TinyMem overcomes these challenges in real-world scenarios. Strengthen this by providing benchmarks or examples of such cases.

**Clarity and Structure**
- The introduction is too detailed, detracting from the paper's focus. Consider summarizing and moving background information to a separate section.
- Figure captions (Figures 2 and 3) should summarize each figure’s purpose and insight rather than simply stating what it is.
- The *Methods* section could benefit from a more cohesive structure. Consider how each consecutive subsection follows from the previous one instead of only detailing the concept. (For instance, What is the input of your MULTIMODAL MEMORY EMBEDDING? Where does its output goes next? ...)
- The *Experiments* section is hard to follow. AVA results appear in both Section 4.1 and 4.3, making the results difficult to trace, please consolidate AVA results into a single section.
- Also, the authors are presenting ablations in both sections 4.1 and 4.3 making it hard to know which section is presenting what. Consider a clear separation between "Results" and "Ablations".
- Table 5’s “Vid/s” is ambiguous; specify its meaning for clarity.

**Comparative Analysis**
- Comparison on Epic-Kitchens-100 is only against two methods. Expanding this to include a broader range of baselines would make the result more convincing.

**Technical Details**
- TinyMem is described as a “lightweight alternative,” but it relies on an off-the-shelf captioner and has more trainable parameters than others. Provide more details on FLOPs and throughput relative to baselines for a clearer comparison of its efficiency.
- Novelty appears limited as the main innovation is the use of an off-the-shelf captioner (BLIP) for marginal performance gains. Each frame requires BLIP-2 captioning, creating potential bottleneck. Please add captioning time analysis and address the novelty issue.
- It's not clear the idea behind captioning using a VLM model (BLIP) and then reverse the captioning by using yet another VLM model (CLIP). If BLIP-2 is already pre-trained on text-image pairs, using intermediate representations could improve efficiency rather than captioning each frame and then re-encoding. Could the authors explain the rationale behind that?
- Unclear how the model will scale with other captioners. Consider experimenting with a video-captioning model or exploring alternatives that could enhance the model’s scalability and efficiency.
- In lines 205-207, there are these sentences: *We employ RoI features as compact region representation and feed them into the classification head to gain the final prediction. In consequence, we obtain resulting N_region region memory tokens.* which means that the final prediction is the same as the region memory tokens. Could you clarify this?
- The choice of using only FIFO memory structure should be justified or explained.

See weaknesses

---

**Review 3:**

This paper introduces TinyMem, a novel approach to long-form video action detection that addresses the limitations of existing video transformer models that rely on dense visual memory embedding. Rather than using hundreds of memory tokens to capture long-range dependencies, TinyMem employs a more efficient multimodal memory system that combines condensed visual region embedding with abstract text semantics derived from video content. By leveraging vision-language models to generate framewise captions and utilizing ROI features or global tokens for region embedding, TinyMem achieves state-of-the-art performance while using significantly fewer memory tokens than previous approaches. Results are reported on AVA-v2.2, Epic-Kitchens-100 and Breakfast datasets.

Strengths:

1. The idea is simple but innovative and well motivated.
2. The paper is well presented and easy to follow.
3. The ablations are detailed and informative.
4. The method achieves strong performance on multiple benchmarks.

I am concerned regarding the sensitivity of the method on the type of text captioning model/language model being used. As the paper mentions, *"Text embedding overwhelms other formats of embedding on AVA by a large margin"*. I wonder how this varies with different pretrained language models and vision-language models. Additionally, it can be seen that improvements on other benchmarks such as Epic-Kitchens-100 and Breakfast are much less compared to those on AVA. Is that because on Epic-Kitchens-100 and Breakfast the text embedding is not as useful as on AVA? If that is the case, then it would imply the main performance improvements, especially in AVA, is dependent on the quality of the text embedding. Which in turn means that performance is dependent more on the type of pretrained model used and not the actual method being proposed in the paper.

Please consider the weaknesses section and consider the points regarding impact of the choice of pretrained model. I will be revising my rating after discussing further with the other reviewers.

---



---

## Year 2025

### Paper 1 (paper_id: 9juihpPDXQ)

**Review 1:**

This paper presents a new approach called Multihead Differential Gated Self-Attention (M-DGSA), designed to enhance the robustness of Transformers against noisy inputs. By introducing an input-dependent gating mechanism for each attention head, M-DGSA dynamically combines excitatory and inhibitory branches, allowing it to more effectively suppress noise. Experimental results demonstrate that M-DGSA consistently boosts accuracy and robustness, particularly under noisy conditions, across both vision and language tasks.

The paper introduces M-DGSA, a method that learns per-head, input-dependent gating to dynamically suppress attention noise. The results demonstrate consistent improvements in noisy environments, showcasing the method's effectiveness. Additionally, the paper is well-written and accessible.

The paper presents a relatively straightforward idea and lacks significant originality. Compared to the approach in Ye et al. (2024), there are no major innovations or departures in the proposed method.

The experiments have some notable limitations. The CIFAR and MNIST datasets are relatively small and simple, which may not fully showcase the model's capabilities. Additionally, the reported ImageNet accuracy (Table 2) is low. Due to these factors, the claims regarding the algorithm's effectiveness remain somewhat unconvincing.

N/A

---

**Review 2:**

The authors propose multihead differential gated self-attention (M-DGSA), a modification of the differential transformer (DT). M-DGSA replaces the scalar \lambda in DT with an input-dependent sigmoid gate. The method is evaluated in both vision and language tasks, showing modest accuracy gains over the DT baseline on (clean) datasets.

- Replacing DT's static $\lambda$ with an input-dependent gate is a reasonable modification
- Evaluations are averaged over 5-seeds with sd reported, which is excellent
- Empirical evaluation is fairly sound, though shows relatively modest results (ImageNet is convincing, mod concerns about memory/compute being held equal)
- Something only mentioned in the appendix: they got rid of DT's $\lambda$ schedule, using a fixed value of 0.8. This seems like a potentially useful contribution, especially if it applies only to the new gated version. Tuning the schedule sounds like a hassle
- Qualitative results for images seem convincing

- More discussion of compute/memory requirements would be appreciated; it's unclear if the relatively small gains on ImageNet are worth potential additional training/inference time. The appendix mentions it's roughly equal, but seems somewhat offhand.
- Relatively small gains compared to DT itself, except the Newsgroup dataset where DT performs suspiciously badly.
- Undertrained CIFAR-10 baselines -- 75% accuracy on CIFAR-10 is super low, makes it hard to trust the comparisons. You could use something like mimetic initialization to compensate for small datasets (I know it's hard to train ViTs on small datasets)

- Are the comparisons to DT and vanilla ViT fair wrt the use of SwiGLU? Should there be additional ablations for this?
- Do you have any more analyses of memory/compute requirements compared to baselines?
- The method is motivated as improving noise robustness to some extent -- have you done experiments on this in particular?

I'd increase my score pretty easily if some of these things were cleared up.

---

**Review 3:**

This paper introduces Multihead Differential Gated Self-Attention (M-DGSA), a novel self-attention mechanism for Transformers inspired by lateral inhibition in biological neural circuits. M-DGSA splits each attention head into excitatory and inhibitory branches, fusing their outputs via a learned, input-dependent gating mechanism to dynamically suppress attention noise. The approach is designed to enhance robustness to corrupted or noisy inputs and integrates seamlessly into existing Transformer and Vision Transformer (ViT) architectures with minimal computational overhead. Experiments on both vision (e.g., CIFAR-10/100, ImageNet) and language (e.g., IMDB, MNLI) benchmarks show that M-DGSA improves accuracy and noise resilience over standard and Differential Transformer baselines.

+ The motivation of the propopsed method is resonable introducing an interpretable gating mechanism based on lateral inhibition.

+  M-DGSA shows improved accuracy and noise resilience across several vision and language tasks, outperforming baselines. It also produces sharper, more focused attention maps.

+ M-DGSA can be incorporated into existing Transformer architectures with negligible computational or memory cost.

- The gating mechanism, while lightweight, adds more complexity to the attention computation and may require careful tuning. It is not clear how the proposed method can effectively and efficiently scale up: effects on training stability, convergence speed, or performance on very large-scale or long-sequence tasks.

- The evaluations are limited to synthetic noise. Most robustness experiments use synthetic corruptions, while real-world noise and other modalities e.g., cross-attention, multimodal or diffusion tasks are not explored.

- While gains are consistent obtained on several benchmarks, the margin over Differential Transformer and ViT baselines is marginal, especially on saturated or simple benchmarks.

Please refer to the detailed questions raised in Weakness section above.

---


### Paper 2 (paper_id: dYaIotpCiK)

**Review 1:**

The authors introduce SuperIgor, a framework designed for instruction-following tasks. Prior research has addressed complex instructions by predefining subtasks that agents can execute and then decomposing language instructions at the subtask level to solve them. In contrast, SuperIgor employs iterative co-training, where the RL agent follows generated plans, and the LLM adapts and refines those plans based on feedback from the RL agent. Experiments demonstrate superior performance relative to baselines.

Unlike the majority of studies that treat LLMs as APIs detached from action-executing agents, SuperIgor's approach of optimizing the LLM through feedback from the RL agent represents a key differentiator from existing work.

While the proposed method appears innovative, the experiments fall short in substantiating its novelty. The authors did not incorporate baselines [1] and [2], which require a predefined "set of possible subtasks," as comparisons. Instead, the baselines seem to rely on raw instructions or plans generated by GPT-4. This setup suggests that the primary distinction from baselines may lie not in the claimed benefits of modifying the LLM via RL feedback, but rather in the use of predefined possible subtasks. Although the appendix illustrates how plans evolve during LLM finetuning, the marginal difference in success rates between SI-DPO and SI-SFT raises questions about the true impact of LLM finetuning.

[1] Zhang, Jingwei, et al. "Game On: Towards Language Models as RL Experimenters." *arXiv preprint arXiv:2409.03402* (2024).

[2] Ahn, Michael, et al. "Do as i can, not as i say: Grounding language in robotic affordances." *arXiv preprint arXiv:2204.01691* (2022).

1. The choice of the PPO algorithm for the RL agent is intriguing. What motivated this selection? Additionally, unlike SayCan, which trains individual policies for each skill, the framework appears to enable a single policy to handle multiple skills. Were there any limitations encountered when training with PPO to perform diverse skills?
2. Success rate was used to assess 'skill mastery' and trigger 'LLM finetuning.' Relying solely on success rate might result in suboptimal skills being learned. Is there a specific reason for not incorporating metrics like reward or value?
3. As highlighted in the Weakness section, including baselines similar to [1] and [2] would more robustly support the paper's claims.
4. The paper specifies the use of Qwen2.5-14B-Instruct for the LLM. What was the rationale behind this choice, and does the selection of different LLMs influence the results?

[1] Zhang, Jingwei, et al. "Game On: Towards Language Models as RL Experimenters." *arXiv preprint arXiv:2409.03402* (2024).

[2] Ahn, Michael, et al. "Do as i can, not as i say: Grounding language in robotic affordances." *arXiv preprint arXiv:2204.01691* (2022).

---

**Review 2:**

This paper addresses instruction-following tasks by integrating plan generation with instruction decomposition. The proposed framework enables iterative plan refinement through co-evolution between plan generation and execution modules without manual annotation. Experimental results demonstrate the effectiveness and generalizability of the method.

Originality

The paper introduces a self-supervised learning paradigm for instruction-following tasks that reduces dependency on manually annotated plan datasets. While LLM-RL integration is prevalent in the field, the paper makes a contribution by articulating the plan generation process with sufficient technical depth and providing an analysis of the iterative refinement between language models and RL agents.

Clarity

The method and experiment setups are well-structured. The research questions are explicitly stated, and the experimental design addresses distinct aspects, including effectiveness, generalization, training dynamics under sparse feedback, and performance evolution across iterative cycles. The appendices provide algorithmic specifications and implementation details that enhance reproducibility.

Significance

The experimental results are convincing, demonstrating quantifiable improvements over baselines and meaningful ablation studies that substantiate the necessity of key components.

Despite the paper's contributions, several aspects require clarification to strengthen the scientific rigor and reproducibility.

- The definition of the research contains vague descriptions and lacks operational definitions in the paper. For RQ1, the paper evaluates "generalization" by testing on compositionally novel instructions (New Objects) and paraphrased formulations, while "effectiveness" is not explicitly operationalized. For RQ2, is "well" pertains to final performance or learning efficiency? Provide concrete metrics and align the terminology in RQs to establish coherent connections between questions and experimental protocols.

- The paper describes building a "subtask base by extracting and canonicalizing possible subtasks from the instruction dataset" to create "a unified vocabulary." Your method extracts subtasks from instructions, while prior work defines them directly. Is there any difference between previous works and yours?

- The model settings and data representation in this paper are somewhat confusing. How do you parse the LLM-generated plan in natural language into PPO? What is the format of the feedback used for fine-tuning? What are the actual inputs and outputs of the policy? These technical details could be elaborated further.

- The paper only demonstrates solving EASY dataset. Have you considered solving MEDIUM and HARD datasets? And how is the result?

- The paper mentions in the introduction that “a language model first decomposes an instruction into a structured sequence of actions,” but I did not find any further discussion of this.

---

**Review 3:**

This paper presents a hierarchical framework for language-guided agents. A planning module (a VLM)  first produces a high-level plan, which is then executed by an RL agent. The key idea is that the system does not require annotated plans or a predefined skill library: instead, it generates multiple candidate plans zero-shot at the start of training and then evaluates and refines them during training. The agent’s success provides a preference signal for refining the plan generator using DPO.  They also propose a curriculum learning method for skill learning, only training with plans that contain at most one skill that has not already been mastered.

This paper is clearly written and compares against strong baselines (e.g., goal-conditioned PPO, plan-conditioned PPO). They demonstrate that this hierarchical approach allows their method to generalize combinatorially to unseen goals. I found the experiments detailing the benefits of the skill curriculum very clear (Figure 4).

My main concerns are as follows:
* The paper argues that requiring a predefined set of skills is restrictive. However, the proposed approach still fixes a set of skills at the start of training, derived via prompting, and does not modify this set during training. I think a comparison with previous work mentioned in the paper, like SayCan, which has fixed sets of skills, could therefore be apt (by using the same skills derived via prompting).
* The paper claims robustness to stochastic environments, but it is not clearly demonstrated in the experimental section how CrafText is stochastic, or how the effects of stochasticity manifest.
* Figure 3 lacks confidence intervals or the number of seeds, which makes it difficult to assess statistical significance.
* The prompt for plan generation seems to contain an in-context example that has 3 distinct skills that would be enough to accomplish most of the Craftex tasks. What happens if, instead of defining 3 skills for your task, you use an example from a different environment with skills that are not directly applicable to your environment?

* How sensitive during policy extraction is the hyperparameter for the success rate to count a skill as learned? Would this bias the reward for DPO to have as few skills as possible, as the more skills in a plan, the longer it takes during training to be fully trained on?
* What is the difference between SI-DPO and SI-SFT? There doesn’t seem to be that large a gap within the same cycle
* What do you think is the main reason that you are not able to match the performance of the Oracle plans? The oracle performance increases from cycle 1 to cycle 2. How many steps would it take for it to converge?
* Can you provide examples of the paraphrasing for the OOD evaluations?

---

**Review 4:**

The paper introduces "SuperIgor," a framework for instruction-following in complex, partially observable environments. The method proposes an iterative co-training loop between a LLM planner and a RL agent. The LLM generates high-level plans , which the RL agent, trained with PPO, attempts to execute. The agent's execution success rate is then used to create a preference dataset , which fine-tunes the LLM planner via DPO. The authors claim this self-guided mechanism reduces the need for manual annotation. To handle sparse rewards, the paper also introduces the Skill Curriculum Learning method. Experiments on the CrafText benchmark show the method outperforms baselines and generalizes to unseen instructions.

The paper tackles the challenging and highly relevant problem of instruction following in dynamic, sparse-reward environments where agents must execute long, complex plans. To solve this problem, the paper clearly identifies the sparse reward problem as a critical bottleneck. The proposed SCL is well-motivated. The ablation study in Figure 4 provides compelling evidence that this curriculum is not just helpful but essential for learning, as even an agent with Oracle plans fails to master more than a few basic skills without it.

The paper's central contribution claim is critically undermined by its own methodology. The abstract and introduction explicitly frame the contribution "in contrast to prior methods that depend on a fixed set of predefined subtasks." However, Sec. 4.1 describes a process that does exactly this. The method starts by "build a subtask base by extracting and canonicalizing possible subtasks from the instruction dataset" to create a "unified vocabulary" in a "strict normalized format". The LLM then generates plans "in terms of the established subtask base". This is a fixed set of predefined subtasks. The fact that it is generated from the training dataset rather than manually specified is a minor implementation detail, not the fundamental shift in approach that the paper claims. This contradiction is a major misrepresentation of the work's core contribution.

Besides, the Core DPO Contribution Shows No Empirical Benefit. The paper's primary thesis is that the iterative alignment of the LLM planner via DPO (i.e., the "self-guided" feedback loop) is "highly effective". This claim is directly and conclusively contradicted by the paper's own results in Figure 3.
- To isolate the effect of the DPO loop, one must compare SI-SFT (agent trained on SFT-tuned LLM plans) against SI-DPO (agent trained on DPO-tuned LLM plans) in the final "Cycle 2."
- On Combo CrafText Tasks (Fig 3b): SI-DPO achieves a 0.21 Success Rate. SI-SFT also achieves a 0.21 Success Rate. The DPO loop provides zero benefit.
- On New Object CrafText Tasks (Fig 3c): SI-DPO achieves a ~0.17 Success Rate. SI-SFT also achieves a ~0.17 Success Rate.

Given the above points, the paper's strong performance over baselines is almost entirely explained by (a) using plan-based supervision and (b) the Skill Curriculum Learning. The ablation in Figure 4 is the strongest result in the paper, showing SCL is the key enabler. The paper should have been framed around this curriculum, which is critical, rather than the DPO loop, which is empirically useless.

The method uses the RL agent's overall success rate as a preference signal for DPO. This is an exceptionally noisy and unreliable signal. The paper even admits this in its own limitations (Section I), stating, "it is difficult to determine whether the failure stems from a flawed plan... or from inadequately trained policy". This is not a minor limitation; it is the central research challenge of this paradigm, and the paper offers no solution. Using DPO on such a high-variance, ambiguous signal is unsound. The fact that it didn't work (per Weakness #2) is therefore unsurprising.

Please refer to the weakness part above.

---


### Paper 3 (paper_id: ABdgMoJhlO)

**Review 1:**

This paper aims to reduce LLM hallucinations in long-context settings, especially in the context of RAG.
The key limitation of current RAG approaches is that, either the long context makes it hard for the model to accurately identify key info, or model fails to retrieve intermediate results from its own reasoning chains.
To address this limitation, the paper introduces **micro-macro retrieval (M$^2$R)**, a retrieve-while-generate method.
M$^2$R hinges on the positive correlation between key information proximity to model outputs and factual accuracy, which was reported previous works.
M$^2$R directly enforces this proximity mechanism into the LLM through curriculum-learning with GRPO, such that *macro retrieval* (also `<think>` phase) implements traditional RAG and maintains a key info repo while *micro retrieval* (also `<answer>` phase) extracts key info from the established repo to ground model outputs.

Experiment evaluation on multi-hop QA benchmarks show that M$^2$R generally outperforms most baselines and its comparative advantage is more evident in challenging scenarios (HotpotQA-2/3Q).

1. Originality

   This paper is novel in that it alleviates hallucinations in RAG based directly on empirical insights from [1-2] that key information position methods in long-form generation.
   The application of GRPO + curriculum learning to realize the insight above also makes sense.
2. Quality

   The experiment design is sound and directly supports central claims of this paper.
   The authors have also done an excellent job by releasing the source code with clear documentation.
3. Clarity

   The paper clearly explains its motivation, key insights (lines 53-55, Appendix B), as well as discussions on the rationale of key designs (lines 120-122, 136-137, 161, 249-256, etc).
   The case study is useful for readers to understand the method intuitively.
4. Significance

   This paper could be of significance to the field by grounding in empirical findings of previous works, combining GRPO and curriculum learning and achieve a compelling performance boost in challenging multi-hop QA tasks.

References:

[1] Lost in the middle: How language models use long contexts. (2023)
[2] Found in the middle: How language models use long contexts better via plug-and-play positional encoding. (2024)

(Authors do **not** need to refer to points raised in this section since the main points are already mentioned in *"Questions" section*.)
1. W1: Limited model family

   The paper only involves experiments on Qwen-2.5-3B/7B models; results on more diverse model families could strengthen the paper's arguments.
2. W2: Missing discussions on costs

   Since the core method introduces additional storage requirements (key info repo) and requires retrieving key info during answer phase, there are concerns regarding whether these components induce heavy storage/time costs.

**Major questions (that could affect rating)**
1. **Question 1**: Limited model family

   M$^2$R is only tested on two models of different sizes (3B, 7B) but the same model family (Qwen2.5). This limitation does *not* directly undermine the central claims of the paper, but results on more diverse model families could greatly enhance the general utility of the proposed method.
2. **Question 2**: Cost analysis

   Inference efficiency is a critical concern in RAG applications. The proposed method requires maintaining a key info repo during macro retrieval and retrieving key infos during micro retrieval. Therefore, a natural concern arises as to whether the performance boost is worth the cost:

   *How does this micro-macro framework affect inference latency, and what are the storage costs of the key info repo?*

   A detailed analysis (either theoretical or empirical) could be useful in deciding whether M$^2$R is usable in practice.

**Minor questions and suggestions (that are not considered to affect rating)**
1. **Minor question 1**: Additional details for reproducibility

   Although the authors have provided source code and some experiment details in the paper, additional details such as hardware requirements and seeds could be useful for reproductions of results.
2. **Suggestion 1**: Notations

   The method name, M$^2$R, is not consistently presented: it is usually written in normal font but sometimes written in italics (lines 206-261, 267, etc).
3. **Suggestion 2**: Paper organization

   Related Work section could help readers set the context, but currently it is placed in the Appendix. Therefore I recommend move Table 1 (M$^2$R prompt template) to the Appendix and move Related Work section to the main body instead.
4. **Suggestion 3**: Details on training stability

   The paper mentions at lines 242-243 that, directly optimizing macro/micro retrieval leads to poor convergence. Detailed results (preferably placed in the Appendix) could help future researchers gain in-depth understandings regarding the significance of curriculum learning.

---

**Review 2:**

This paper proposes Micro–Macro Retrieval (M2R), a two-level retrieve-while-generate framework designed to reduce hallucination in long-form generation. By combining macro retrieval of coarse evidence during reasoning with micro retrieval of key information during answer generation, M2R ensures that essential evidence remains proximal to output tokens. Trained with a curriculum-based reinforcement learning strategy using rule-based rewards, the method achieves significant improvements in factual consistency and robustness across multi-hop QA and long-context benchmarks compared to strong RAG baselines.

1. The idea of Micro–Macro Retrieval surprisingly natural and well-motivated — it addresses one of the most persistent issues in RAG systems (long-form hallucination) with a solution that feels both principled and minimal. The “retrieve-while-generate” framing elegantly captures how reasoning and retrieval should co-evolve.

2. The paper is very carefully written. I particularly like how the authors formalize the two retrieval levels and the transition between \<macro_tool_call>, \<key_info_save>, and \<micro_tool_call>. It feels like reading a well-designed system that could actually be implemented in production without hidden tricks.

3. I really appreciate that the system is interpretable by design: it shows that we can literally see the reasoning flow: what it retrieved, what it saved, what it reused. That’s a refreshing contrast to the black-box nature of most retrieval-augmented models. It also feels cognitively aligned with how humans solve tasks: note things down, then recall them precisely.

4. Compared with Self-RAG[1] this work feels like a thoughtful continuation rather than simple imitation. Self-RAG let the model decide when to retrieve; M2R turns that spark into a full reasoning routine. It not only detects when retrieval is needed but also explicitly manages what to keep and reuse, maintaining a long-term internal memory grounded in already verified facts. I find this progression deeply satisfying—the model isn’t merely “asking for help” anymore; it’s learning to remember what it already knows to be true. That evolution from reactive retrieval to proactive self-memory feels like a genuine step forward.

[1] Asai, Akari, et al. "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection." The Twelfth International Conference on Learning Representations.

I think this paper has no particularly obvious weaknesses. Unlike the naïve combination of RAG with RL or GRPO, this work takes a much more principled approach—from memory to retrieval to the overall training strategy, making it a significant step forward in improving RAG performance. However, I do have a few questions that I’d like to raise briefly.

1. In your gradient computation implementation, how did you mask out the information from the retrieval part? Is the positional information handled relative to the retrieval step, or do you directly remove the masked portion?
2. When invoking retrieval, are macro-retrieval and micro-retrieval mutually exclusive, or can they be used jointly?
3. When is **key_info_save** called? In your experiments, is it triggered only when the macro retrieval is considered contextually relevant, and thus the macro information is saved? If there are identical or similar **micro_tool_calls**, does the model jointly retrieve them?
4. If the model chooses to invoke **micro**, but there is no stored memory or relevant content, does that lead to hallucination? How do you handle cases where micro-retrieval fails or retrieves incorrect results?
5. Are there situations where neither **micro** nor **macro** is used?
6. What are the overall training costs and latency characteristics? If the database is large or the query is long, does it lead to bottlenecks?
7. If provides a main figure, it will help more readers to fastly grasp your methodology.

See above, I am warmly welcome to discuss further detailed on this paper.

---

**Review 3:**

This paper tackles hallucination in large language models (LLMs), especially in long-form generation where redundant contexts and extended reasoning amplify factual errors. The authors observe that factual accuracy improves when key information appears closer to the generated output, yet existing retrieval-augmented LMs (RALMs) lack mechanisms to ensure this proximity.
To address this, they propose Micro–Macro Retrieval (M2R), a retrieve-while-generate framework combining macro-level external retrieval with micro-level key information reuse from an internal repository built during reasoning. M2R directly maintains evidence proximity to outputs and reduces hallucination in long-context tasks.
Trained via curriculum-based reinforcement learning with rule-based rewards, M2R achieves consistent gains in factual accuracy and grounding across multiple benchmarks, showing good effectiveness in lengthy-context scenarios.

1. The topic is valuable, especially for mitigating hallucinations in long-form reasoning models.

2. The method is straightforward and conceptually sound.

3. The paper is well written and easy to follow.

1. The core methodological details are underspecified. If I understand correctly, the approach hinges on constructing and maintaining a key-information repository, and then:

- (1) For macro retrieval: per Lines 060–061, how is “the reasoning process yields answer-aligned evidence” detected, and how exactly is it inserted into the repository(i.e., what constitutes the key and the value)?

- (2) For micro retrieval: what query is used to retrieve answer-related information from the repository?

- (3) Does GRPO merely teach the LLM when/how to invoke the macro- and micro-retrieval tools, rather than optimizing the retrieved content?

2. Empirical coverage is limited. Training is conducted only on Qwen2.5-3B/7B and evaluated on four relatively simple QA datasets. How does the method perform on other model sizes/families and on more challenging reasoning benchmarks? Moreover, Figure 2 shows substantial reward oscillation and a low mean (~0.4), suggesting training stability or sufficiency may be a concern.

3. FlashRAG details and ablations require clarification, including the knowledge base size, and the effects of chunk size and retrieve number on performance. Reporting token statistics of input/output during inference would further clarify whether the approach practically alleviates long-form reasoning constraints.

Please see the weaknesses,

---

**Review 4:**

A "retrieve-while-generate" framework that performs macro retrieval (external sources during reasoning) and micro retrieval (from key-information repository during answer generation) to ensure evidence proximity and reduce hallucination.

1. Strong Empirical MotivationLost in the Middle Phenomenon (Liu et al. 2023):

GPT-3.5 accuracy drops from 75% → 55% when answer-bearing evidence moves from context start to middle (Figure 3)
Empirically validated across multiple models and tasks
Theoretical Grounding (Appendix B):

Analyzes RoPE positional encoding: attention ∝ q^T R_{θ,m-n} k
High-frequency components cancel at large distances Δ
Formal claim: "Evidence contribution decreases monotonically with distance"
This foundation is solid — problem is real and well-documented.

2. Explicit Key Information ManagementKey-Value Repository Design:

Advantages:

- Explicitly separates "what to remember" from "how to answer"
- Forces model to extract atomic facts rather than rely on context attention
- Reduces cognitive load during answer generation
This is cleaner than implicit reasoning traces (e.g., ReSearch where key info is buried in <think> text).

1. "Retrieve-While-Generate" might be Misleading

What the Paper Claims:

"M²R is the first framework to introduce a retrieve-while-generate paradigm during the answer phase."
This suggests a novel generation mechanism — e.g., retrieval happening during the forward pass.What Actually Happens:Multi-Turn Generation with Tool Calls:

This is identical to:

OpenAI's function calling
Anthropic's tool use
ReAct (Yao et al. 2022)
Self-RAG (Asai et al. 2023) — which also retrieves during answer generation!
M²R requires 5-10 sequential model invocations per query.

2. Cost Analysis Completely Absent

1. How many model invocations does M²R require per query on average?

Please report separately for: (a) think phase, (b) answer phase, (c) total
Break down by dataset (HotpotQA, MuSiQue, etc.)
What is the range (min/max)?



2. What is the end-to-end latency in realistic deployment?

Table 2 shows batch inference time (0.67s), but what about non-batched API calls?
Assuming 100ms per forward pass: how long does a typical query take?
How does this scale with question complexity?

---


### Paper 4 (paper_id: bRVJcc89Em)

**Review 1:**

Text-to-video models are quickly improving and creating excitement both among researchers and users of AI.  However, like LLMs, these models are prone to hallucinate details of their output, especially when the input prompt is underspecified or underrepresented in training data.  To address this challenge, this work presents the first (to their knowledge) study of uncertainty quantification in text-to-video models.  They propose a black-box UQ method based on the epistemic/aleatoric decomposition to help identify when a text-to-video model is likely to hallucinate, and also plan to release a dataset of 40K videos for benchmarking UQ.

Effective uncertainty quantification is a central pillar in creating trustworthy AI systems.  While most focus on UQ in deep learning has been in image classification and more recently LLMs, it is important that these tools are extended to other fields and application areas, for example robotics or other generative media besides text.  This paper aims to take the first step towards developing a framework and tools for UQ in text-to-video systems.  This is a very solid motivation, and creates the potential for a significant contribution.

The main weakness I find is that this paper does not carefully treat the concepts of epistemic and aleatoric uncertainty, in particular by treating them primarily in terms of the input prompt rather than as properties that depend on the interaction between the model, its capacity, and the data distribution. Aleatoric uncertainty is described as randomness from prompt vagueness, while epistemic uncertainty is tied to a lack of model knowledge. This framing assumes these uncertainties are intrinsic to the prompt, but in practice, they are model- and data-dependent. For instance, if the entire training set consists of videos of cats napping on purple beds in the backs of pickup trucks, then the prompt “a cat napping on a purple bed in the back of a pickup truck” would still display high aleatoric uncertainty, not because the prompt lacks specificity, but because the data distribution itself is highly variable in that region. By focusing almost entirely on prompt semantics, the paper overlooks the fact that the distinction between epistemic and aleatoric uncertainty depends fundamentally on the model and the data it has seen.

This conceptual problem extends directly into the method. The decomposition in Equation (3) is presented as a principled separation between epistemic and aleatoric uncertainty, but in practice both quantities depend on the behavior and biases of the specific models used to estimate them. The authors estimate aleatoric uncertainty by prompting an LLM to generate refined textual variants and epistemic by sampling multiple videos from the same generative model. Both steps produce variability that arises from model architectures and training data of the various models, not from isolated intrinsic uncertainty types. What they call aleatoric uncertainty reflects the LLM’s own distribution, while their epistemic uncertainty reflects the video embedding model’s representation space, making the split depend on implementation choices rather than underlying epistemic principles. As a result, the decomposition is not theoretically or empirically meaningful.

Beyond these conceptual issues, the method relies on untested and implausible assumptions. The independence assumption discards dependence between the text prompt and the generated video, which is unlikely to hold in text-to-video generation. The estimation of entropy in embedding spaces further introduces arbitrary geometric distortions, since the embedding dimensions and projection have a major effect on the computed entropies. The authors provide no sensitivity analysis or justification for these choices, leaving the reported uncertainty values largely uninterpretable.

The experimental evaluation generally lacks rigor. The decision to use CLIPScore as the primary accuracy metric is based on a small 10 sample correlation study, an inadequate basis for methodological justification. The subsequent experiments that claim to disentangle aleatoric and epistemic uncertainty depend on opaque subsetting of data where one component is deemed zero according to the authors’ own estimators, introducing circular reasoning. These experimental protocols make the reported calibration and correlation results difficult to trust.

Overall, the proposed decomposition lacks solid conceptual grounding, the implementation does not meaningfully separate uncertainty types, and the empirical evaluation does not convincingly support the claims.

See weaknesses.

---

**Review 2:**

- The authors introduced a framework to measure the uncertainty of video generative models.
- The framework consists of a metric for evaluating the calibration of video models based on robust rank correlation estimation.
- They also introduce S-QUBED, a black-box UQ method for video models. S-QUBED effectively distinguishes between uncertainty arising from ambiguous prompts and uncertainty stemming from the model's lack of knowledge.
- They will also release a dataset of 40K videos across diverse tasks to help benchmark calibration in video models.
- The authors used their method to disentangle and understand aleatoric and epistemic misunderstandings of the video generation models. For example, to assess epistemic misunderstanding, they generated multiple videos for the same prompt and embedded them. Then, they measured the embeddings' spread, with wider spread indicating higher epistemic uncertainty.
- For the main result of their work, they further study the correlation between accuracy and the different uncertainties. They find that when uncertainty is higher, accuracy tends to be lower. This holds for both overall uncertainty and aleatoric/epistemic misunderstanding.

- Uncertainty quantification of LLMs is well studied, but not studied at all for video generation models. This work was novel in that it studied uncertainty quantification of video generation models.
- The black box approach makes it accessible to evaluate any video generation model.
- The authors presented the material well, providing the necessary background to understand the motivation and importance of this work, which is especially important given its novelty.

- I would like to see empirical results and to validate S-QUBED on other open (non-API) video models, given that it is a black-box approach. The authors mentioned that different models were considered but not evaluated due to access and compute constraints. However, I believe there should be multiple open text-to-video models to evaluate S-QUBED on (e.g., OpenSora).
- Typical metrics (e.g., CLIP, PSNR) for evaluating text-to-image and text-to-video models often do not align with human judgment. Would like to see the correlation of uncertainty with human judgment metrics.

No questions as the background, motivation, and results were presented well.

---

**Review 3:**

This paper is (to the authors’ knowledge) the **first study of uncertainty quantification (UQ) for text-to-video models**, proposing a three-part framework: (i) a **calibration metric** based on robust rank correlation between uncertainty and task accuracy, (ii) a black-box UQ method, **S-QUBED**, that uses a **latent-space factorization** to **decompose total predictive uncertainty** into **aleatoric** (prompt vagueness) and **epistemic** (model ignorance) components, and (iii) a ~**40K-video UQ dataset** for benchmarking. Experiments on VidGen-1M and Panda-70M show that S-QUBED’s total uncertainty is **significantly negatively correlated** with semantic accuracy (CLIP score), and its decomposition yields calibrated aleatoric/epistemic trends on subsets where the other source of uncertainty is minimal.

* Positions UQ for video generation as a first-class problem; formal **entropy decomposition** (h(V|\ell)=h(V|Z)+h(Z|\ell)) cleanly maps to epistemic vs. aleatoric sources.
* **S-QUBED** operates without model internals, aligning with many **closed-source video models**.
* Uses **Kendall’s τ** and demonstrates **significant negative correlation** between S-QUBED uncertainty and **CLIP accuracy**, with visuals that match the trend.
* Empirical **disentangling** of aleatoric vs. epistemic uncertainty shows expected behavior on curated subsets.
* Plans to release a **~40K-video UQ dataset** covering diverse tasks.

* Calibration hinges primarily on **CLIP similarity**; other perceptual metrics (SSIM/PSNR/LPIPS) show weak or insignificant correlations, raising concerns about **metric sensitivity** and potential semantic-evaluator bias.
* Estimating **epistemic uncertainty** requires **multiple generations per latent prompt**, which the authors acknowledge as a limitation.
* Main experiments use **Cosmos-Predict2** and two datasets; broader **model diversity** and real-world perturbations (codecs, length, audio conditions) are not deeply explored.

1. Beyond CLIP, what **additional accuracy signals** (e.g., human semantic judgments, video-text retrieval scores, physics consistency probes) are necessary to **validate calibration** and mitigate evaluator bias?
2. What **sampling schedules** (fewer latent prompts/videos, adaptive stopping) or **latent-space proxies** would you require to deem S-QUBED **computationally practical** without sacrificing epistemic resolution?
3. Which **additional models/datasets** or **deployment artifacts** (compression, prompt styles, audio/no-audio) would most convincingly demonstrate that the **aleatoric/epistemic decomposition** remains **stable and calibrated** in the wild?

---

**Review 4:**

The paper proposes a black-box framework that lets text-to-video models express uncertainty by decomposing predictive uncertainty into aleatoric (prompt vagueness) and epistemic (model ignorance) components. The framework is evaluated with a rank-correlation-based calibration metric, and a 40K-video UQ benchmark is released.

1. The paper is well-presented, well-written, and the motivation is justified.
2. The research topic of principled evaluation of synthetic videos is very timely and important.
3. The proposed dataset will be valuable.

1. The method’s evidence of **general** video-model UQ almost entirely depends on one text-to-image-to-video pipeline (Cosmos-Predict2). While I appreciate that authors state the API/compute constraints, it will be more convincing if the paper proposes potential solutions or fixes to overcome the challenge. That being said, the practicality and calibration of stronger video models shall be evaluated.
2. Please fix salient typos such as "video modes" (Page 3) and "peak signal-to-noise ration" (Page 13).

While there are several weaknesses stated above, I believe this paper will be contributive and will provide new insights to the community. I therefore have the initial rating of 6 for this paper. Please note that my final rating will be conditioned on the soundness of the rebuttal.

---


### Paper 5 (paper_id: ZqiCAEQ4Sx)

**Review 1:**

The proposed sequential inference-time scaling (iterative reasoning) is superior to parallel self-consistency (independent reasoning paths) under matched compute. The presentation is clear, technically rigorous, and supported by extensive experiments across multiple models and benchmarks. However, a few aspects could be improved to enhance its credibility and accessibility for reviewers.

1. Introduces a training-free, information-theoretic voting method based on Shannon entropy. Improves accuracy in nearly all configurations (97% sequential, 100% parallel).

2. Careful matched compute design ensures fair comparison (identical token budgets). Extensive reproducibility details (prompts, hyperparameters, seeds, API configs).

1. The paper lacks a formal explanation for why sequential refinement works better (e.g., in terms of uncertainty reduction or information accumulation).

2. Builds on prior “self-refine” and “reflexion” frameworks; reviewers may see it as an empirical extension rather than a fundamentally new paradigm.

3. Uses small sample sizes (≈30 problems per benchmark), so some reported differences may not be statistically robust. Needs clearer reporting of p-values or confidence intervals in the main text.

4. Sequential reasoning is slower in wall-clock time since it runs steps serially. No quantitative latency analysis or discussion of mitigation (e.g., parallelized refinement).

5. The creative-writing ablation feels loosely connected to the paper’s core reasoning claim and could be trimmed or moved to the appendix.

N/A

---

**Review 2:**

This paper revisits inference-time scaling for large language model (LLM) reasoning and challenges the long-standing assumption that parallel self-consistency decoding (Wang et al., 2022) is the optimal test-time scaling method. The authors propose a systematic comparison between parallel and sequential reasoning paradigms under matched compute budgets across five open-source LLMs (GPT-OSS, Qwen3, Kimi-K2) and three reasoning benchmarks (AIME-2024/2025 and GPQA-Diamond).

They find that sequential reasoning, where each reasoning chain iteratively refines previous attempts, outperforms parallel reasoning in 95.6% of settings, achieving up to 46.7% accuracy gains. Furthermore, the paper introduces Inverse-Entropy Weighted (IEW) Voting, an information-theoretic aggregation method that weights answers by inverse Shannon entropy of token-level log probabilities. This method yields consistent improvements over majority voting in both sequential and parallel setups, achieving optimality in 97% of configurations.

1, This paper challenges a widely accepted inference-time scaling orthodoxy (parallel self-consistency) with compelling evidence favoring sequential reasoning.

2, Controlled matched-compute setup and multi-model, multi-domain evaluation ensure fairness and reproducibility.

3, Inverse-entropy voting introduces a principled, information-theoretic mechanism that improves upon heuristic majority voting.

4, This paper demonstrates generality across reasoning, scientific, and creative tasks, reinforcing the universality of sequential refinement.

1, More related works should be discussed. e.g. https://aclanthology.org/2024.findings-emnlp.135.pdf, https://arxiv.org/abs/2401.02009, https://arxiv.org/abs/2308.00436. For example, at the same cost, does the proposed method perform better than mirror-consistency, self-contrast & self-check?

2, The main benchmarks (AIME, GPQA) focus on mathematical and scientific reasoning; inclusion of commonsense or real-world tasks (e.g., MMLU, GSM8K) would further support generality.

3, Self-refinement is somehow a doubtful pathway (http://arxiv.org/abs/2310.01798). The paper could better explain its qualitative decision process and failure modes under extreme conditions.

1, What's the performance comparison between the consistency-based methods and other inference-time methods? e.g. multi-agent systems or other prompting methods like step-back https://arxiv.org/abs/2310.06117. Or let me ask in another way, why should we keep optimizing consistency-based methods, given all other prompting strategies?

2, The method is mainly a prompting engineering work. Can the llm be trained to be better at self-refinement?

---

**Review 3:**

This paper challenges the prevalent parallel reasoning paradigm (Self-Consistency) in large language model (LLM) inference scaling. Through rigorous experimentation across five state-of-the-art open-source models and multiple benchmarks (AIME, GPQA-Diamond), the authors demonstrate that sequential refinement (where LLM reasoning iteratively builds upon and corrects prior outputs) outperforms parallel approaches in 95.6% of configurations under a crucial condition: matched token budget/compute. This superiority is achieved without additional fine-tuning, leveraging the inherent mechanisms of iterative error correction and progressive context accumulation unique to the sequential process.

One  contribution is the introduction of Inverse-Entropy Weighted (IEW) Voting, a training-free method that uses token-level log-probabilities to quantify model confidence (lower Shannon entropy equals higher confidence) and assign higher weight to more confident chains. IEW Voting proves to be the optimal aggregation strategy across both sequential and parallel paradigms, achieving optimal performance in 97% of sequential configurations. The paper advocates for a paradigm shift, positioning sequential refinement as the robust default for LLM reasoning, with the 6-chain configuration emerging as the optimal balance of compute and performance gains.

* The paper offers near-universal evidence (95.6% win rate) that sequential reasoning outperforms the parallel method (Self-Consistency) across diverse LLMs and complex reasoning tasks

* The technical contribution of Inverse-Entropy Weighted (IEW) Voting is elegant and training-free, providing a principled way to leverage the LLM's inherent uncertainty (via logprobs) to aggregate results.

* The paper is in an important area, and we definitely need more analysis and interesting studies about detailed aspects of reasoning and test-time scaling. The paper is also well written, and well organized, and relatively easy to follow (despite being fairly technical). Good work!

* The comparison is fair and scientific, strictly matching the total token budget between sequential and parallel configurations (e.g., $N \times 4096$ tokens). The paper also studies multiple benchmarks (math and creative) and studies multiple base models (GPT, Qwen, Kimi).

* The paper acknowledges that sequential, serial execution has a substantial wall-clock time overhead compared to parallel methods, making it challenging for real-time applications

* The core advantage is hypothesized to come from Error Correction and Context Accumulation, but the experiments do not empirically decouple and quantify the contribution of these two distinct mechanisms

* The Creative Tasks ablation shows a divergent trade-off (Sequential: high lexical diversity; Parallel: high semantic diversity), suggesting the "universal superiority" may only hold for correctness-focused, convergent reasoning tasks

* In my opinion, the paper is somewhat borderline because the new method has a practical limitation of high latency. Specifically, it seems difficult to parallelize the method, and so the wall clock time is high. I am curious if there are ways to mitigate this (see questions later).

* I am open to raising my score, but I have a handful of technical questions that I would like some clarity on.

* Another big issue, which you should just fix (we don't need to talk about it). The related work is super limited. I don't seem to find a related work section. Please add this. Also fix the parentheses and the citations for the related work -- there is often no space between the text and the starting parens, which is sloppy.

* To mitigate the latency, have the authors explored an ablation where the token limit of each individual refinement step is aggressively constrained (e.g., to 512 tokens instead of 4096) to reduce the time-per-step while still utilizing the same total budget? This would demonstrate if the iterative refinement loop itself, rather than the length of each attempt, is the true source of the sequential edge, making the method more practically viable.

* The paper attributes the sequential advantage to three mechanisms: (1) Iterative Error Correction, (2) Progressive Context Accumulation, and (3) Answer Verification. The current experiments combine all three. Can the authors conduct an ablation to decouple the effects of the explicit error correction instruction versus the passive context accumulation? Compare Sequential Refinement (prompt: "Review your previous reasoning, identify any gaps or errors...") against a new baseline Sequential Re-Prompting, which uses the history as context but with a neutral continuation prompt (e.g., "Continue the analysis with the next step.") This would isolate the gain derived from the LLM's ability to respond to an explicit error-correction instruction, strengthening the claim about the mechanism.

* The Inverse-Entropy Weighted (IEW) Voting metric uses the mean entropy across all tokens in the reasoning chain (Equation 1). However, the Appendix F ablation showed **surprisingly identical results** whether using the mean, median, maximum, or minimum entropy of the full chain. This suggests that important localized confidence signals might be diluted by the mean or are entirely sufficient using minimal tokens. Could the authors present an ablation comparing the current mean-chain entropy weighting to a Localized Answer-Token Entropy weight? This weight would be based solely on the log-probabilities of a small window of tokens (e.g., the final $N=100$ tokens) immediately preceding the extracted answer tag. This would test the hypothesis that the confidence signal is localized to the concluding segments of the chain, rather than being distributed across the entire, potentially noisy, reasoning sequence.

* The Creative Task Ablation (Figure 3) reveals a crucial trade-off: parallel generation achieves higher Semantic Diversity, while sequential refinement achieves higher Lexical Diversity. Sequential is superior for refinement and depth, while parallel is better for exploration and breadth. Does the claim that Sequential is the "robust default" still hold for tasks requiring high divergent exploration (e.g., novel code generation, brainstorming)? Could the authors propose a Hybrid Gated Scaling approach? This approach would execute a small initial set of parallel chains (for breadth) and then use the IEW metric to select the top-performing parallel result, which is then fed into a subsequent, short sequential refinement chain (for depth and error correction). This framework would leverage the strengths of both paradigms and provide a more nuanced "optimal" strategy.

* For the limited related work, one question. I don't know this area very well -- are there baselines to compare against from recent papers?

---

**Review 4:**

This paper compares sequential test-time scaling (iterative self-refinement where each chain conditions on earlier reasoning) to the dominant parallel self-consistency approach at matched token budgets. Across five OSS models (GPT-OSS-20B/120B, Qwen3-30B/235B, Kimi-K2) and three benchmarks (AIME-2024/2025, GPQA-Diamond), the authors report that sequential reasoning wins in 95.6% of configurations with gains up to 46.7 pp; they also introduce inverse-entropy weighted (IEW) voting, which weights each chain’s answer by the inverse of the chain’s mean token-level Shannon entropy computed from logprobs, and claim it is near optimal among tested aggregators. Key experimental choices include strict token-budget matching (e.g., 6×4096 tokens for both paradigms) and fixed system/refinement prompts.

1.	Claim clearly presented with wide variety of evidence. The author claims that sequential self-refinement beats parallel self-consistency at matched token budgets. The claim is supported by supported by results across 5 models, 3 benchmarks, and multiple chain counts (3/6/9), and indeed show the higher accuracy in almost all the settings. The wide range of configurations ensures the generalizability of the claim.
2.	Training-free and cross-model. The author avoid additional fine-tuning and show the effect across different families (GPT-OSS, Qwen3, Kimi-K2), which strengthens generality beyond a single architecture or a special-trained model.
3.	Attempted fairness via matched token budgets and fixed decoding. In the experiments, author ensures the fairness of comparison by keep the temperature fixed and top k disabled. Most critically, the authors matched the total tokens generated between sequential and parallel. Ablation studies were conducted to also analyze how the token budge affect the performance

1.	Hypothesis on token-level entropy and model confidence as a metric to weigh the chains’ quality is not verified. The author proposed to use token-wise entropy as a weighing factor for generated chains. The critical assumption here is model confidence is positively correlated with quality or correctness of the response. It has been a common phenomenon that model tends to generated confidently the wrong answer under certain given prompt. The test to verify the effectiveness of token level entropy as a metric is missing.
2.	Matching tokens budget does not equate match compute. Equal tokens do not equal compute because sequential steps repeatedly read longer contexts (quadratic attention cost), while parallel chains don’t. Token budgets therefore may understate sequential compute and the comparison for CoT quality is unfair since more computes are spent on generating a new token in sequential than parallel generation.

1.	The first is whether the sequential generation benefits more from iterative refining or the voting process. If only the final answer is taken, do you almost always get the same results? Is voting more central in the sequential generation than in previous generations? While the author conducted a variety of experiments on voting methods, it might be best for comparison if an ablation experiment without any voting is conducted.
2.	Could you justify the correlation between token-level entropy and the correctness of the response?
3.	Are questions drawn randomly from GPQA-Diamond? Would a small sample size of 30 bias towards a specific domain of knowledge?
4.	Refinement Prompts: How sensitive are the sequential results to the specific refinement prompts used? Did the authors experiment with other self-correction or refinement prompting strategies, and if so, how did their performance compare?

---



---


